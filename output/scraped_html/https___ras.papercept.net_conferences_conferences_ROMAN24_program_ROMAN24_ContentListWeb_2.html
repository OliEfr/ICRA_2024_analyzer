<!DOCTYPE HTML>
<html>
 <head>
  <meta content="en-us" http-equiv="Content-Language"/>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="width=device-width" name="viewport"/>
  <script src="https://ras.papercept.net/conferences/scripts/dom-drag.js" type="text/javascript">
  </script>
  <script src="jquery-1.11.1.min.js">
  </script>
  <title>
   RO-MAN 2024 Program | Tuesday August 27, 2024
  </title>
  <style type="text/css">
   body, table, td, th{
	Font-Family : sans-serif;
	Font-Size : 10pt;
}
.r {text-align: right}
.blue {color: #0000FF;}
td {vertical-align: top; text-align: left}
.c {text-align: center}
table.s {
	border-collapse:collapse;
	border-width: 1px;
}
table.s td{
	border-width: 1px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
table.t {
	border-collapse: collapse;
	border-width: 0px;
}
table.t td{
	border-width: 0px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
.dots {
    background:url('./images/dot.gif') repeat-x center;
}
.field {
    background-color: #FFFFFF;
}
#pTitle { /* Page title */
   font-size: 14pt;
   line-height: 1.5em;
}
#pSubTitle { /* Page subtitle */
   color: #909090;
   font-size: 10pt; 
   line-height: 1.5em;
}
#container {
	position: absolute;
	width: 100%;
	margin-top: 2px;
/*	overflow: hidden; */
}

.sHdr {   /* Session header Content list */
   background-color: #F0E68C
}
      
.sSHdr {   /* Subsession header Content list */
   background-color: #f8f3c6 
}
      
table.trk { /* Track table Content list */
   border-collapse: collapse;
   border-width: 0px;
   margin: auto;
/**   width: 640px; **/
   width: 720px;
}
table.trk td{
   border-width: 0px;
   padding: 4px;
   border-style: solid;
   border-color: gray;
 }
      
.pHdr {  /* Paper header Content list */
   background-color: #E6E6FA;
   color: black;
}
hr.thin { /* Horizontal rule content list */
   border: 0px; 
   height: .8px; 
   background-color: #8888FF;
}
      
.pTtl {  /* Paper title Content list */
   font-size: 11pt;
   font-style: italic;
}
      
.ssHdr {  /* Subsession header container session Content list */
   background-color: #DDDDDD;
   color: black;
}
      
.ssTtl {  /* Subsession title container session Content list */
   font-size: 10pt;
   font-style: normal;
   font-weight: bold;
}
  </style>
  <script language="JavaScript">
   function initXMLHttp(){
   var oRequest = false;
   try {
      oRequest = new XMLHttpRequest();
   }  catch (trymicrosoft) {
      try {
         oRequest = new ActiveXObject("Msxml2.XMLHTTP");
      }  catch (othermicrosoft) {
         try {
            oRequest = new ActiveXObject("Microsoft.XMLHTTP");
         }  catch (failed) {
            oRequest = false;
         }
      }
   }
   if (!oRequest){
      alert("Error initializing XMLHttpRequest! Your browser does not support AJAX");
   }
   return oRequest;
}
function modify(number,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'Add';
   }
   else{
      action = 'Delete';
   }
   
//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=431&' + action + number;
//   window.open(url,'myprogrampage');

   modifyItem("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","431",action,number)

}


function modifyItem(url,ConfID,action,number){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&Number=' + number;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

var iIntervalId;  // Global variable
function modsession(id,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'AddSession';
   }
   else{
      action = 'DelSession';
   }

//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=431&' + action + id;
//   window.open(url,'myprogrampage');

   modifySession("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","431",action,id)

}

function modifySession(url,ConfID,action,id){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&ID=' + id;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

function getCookie(sName){
   var sRE = "(?:; )?" + sName + "=([^;]*);?";   
   var oRE = new RegExp(sRE);
   if (oRE.test(document.cookie)){
      return decodeURIComponent(RegExp["$1"]);}
   else{
      return null;
   }
}
function loadprogram(){
   var list = getCookie("ROMAN24");
   if (list){
      var List = list.split(",");
      for (var i=0; i<List.length; i++){
         var names = document.getElementsByName('modify' + List[i]);
         if (names.length){
            for (var j=0; j<names.length; j++){
               names[j].checked = true;
            }
         }
      }
   }
}
function reset(){

   // Uncheck all modify and addsession checkboxes

   var ins = document.getElementsByTagName('input');
   for (var i=0; i<ins.length; i++){
      if (ins[i].type == 'checkbox' && ins[i].id && ins[i].id.substring(0,3) == 'mod'){
         ins[i].checked = false;
      }
   }
   
   // Reload the program
   
   loadprogram();
}
function startreset(){
   iIntervalId = setInterval(reset,2000);
}
function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
var uhash;
var pColor;
$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){
   
      // Mark the session
   
      pColor = $('#' + uhash).parent().css('backgroundColor');
      $('#' + uhash).parent().css('backgroundColor','#FF8888');
   }
});


$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){

      // Set the widths
      
      setwidth();
      
      // If claasical view is required then return

      if (!ghit){return;} 
      
      // Reset left margin for FF

      document.getElementById('container').scrollLeft = 0;;

      // Discover the table and the block and determine the block Id
   
      var rt = $('#' + uhash);
      var done = false;
      while (!done){
         rt = rt.parent();    
         var etype = rt.get(0).tagName;  
         if (rt.is("table")){      
            done = true;
         }
      }
      rt = rt.parent().parent().parent();
      var iid = rt.attr('id')

      // Show the block

      initialize();
      $('#' + iid).show();
      $( '#A' + iid ).focus();
      var ypos = $('#' + iid).offset().top;      
      window.scrollTo(0,ypos);

      // Cancel the scroll to uhash

      var url = location.href;
      url += '_';
      location.href = url;
      
      // Scroll into view

      var leftPosition = $('#' + uhash).parent().position().left;
      var topOffset = $('#' + uhash).parent().offset().top;
      var divOffset = $('#' + iid).find('div').offset().top;
      var topPosition = topOffset-divOffset;
      $('#' + iid).find('div').scrollLeft(leftPosition);
      $('#' + iid).find('div').scrollTop(topPosition);
   }
   else{
      setwidth();
      initialize();
   }
});

var ghit = false;
function setwidth(){
   var viewportwidth = $( window ).width();
   var viewportheight = $( window ).height();
   var sdiv = $( ".sdiv" );
   for (var i=0; i<sdiv.length; i++){
      $(sdiv[i]).css({width: .98*viewportwidth + 'px'});
      $(sdiv[i]).css("height", .9*viewportheight-50 + 'px');      
   }

   // Detect horizontal overflow on any of the divs
   
   var divs = document.getElementsByTagName('div');
   for (var i=0; i<divs.length; i++){
      if (divs[i].id && divs[i].id.substring(0,3) == 'div'){
         if (divs[i].scrollWidth > divs[i].clientWidth){
            ghit = true;
            break;
         }
      }
   }
   if (!ghit){
      for (var i=0; i<divs.length; i++){
         divs[i].style.height = 'auto';
      }
   }
}

function selfollowing(hsh){
   $('#' + uhash).parent().css('backgroundColor',pColor);
   setwidth();
   initialize();
   if (hsh == 'TheTop'){
      var ypos = $('#container').offset().top;
      window.scrollTo(0,ypos)
   }
   else{
      $('#' + hsh).show();
      $( '#A' + hsh ).focus();
      var ypos = $('#' + hsh).offset().top;
      window.scrollTo(0,ypos)
   }
}

function initialize(){

   // Show all day blocks
   
   var blcks = $('.blck');
   for (var i=0; i<blcks.length; i++){
      blcks[i].style.display = 'block';
   }

   // Detect horizontal overflow on any of the divs
   
   var hit = false;
   var divs = document.getElementsByTagName('div');
   for (var i=0; i<divs.length; i++){
      if (divs[i].id && divs[i].id.substring(0,3) == 'div'){
         if (divs[i].scrollWidth > divs[i].clientWidth || divs[i].scrollHeight > divs[i].clientHeight){
            hit = true;
            break;
         }
      }
   }
   if (hit){
   
      // Set overflow hidden on body. This will prevent it from scrolling
      
      $("body").css("overflow", "hidden");
      document.getElementById('start').style.display = 'inline';
      
      // Hide all day blocks
   
      var blcks = $('.blck');
      for (var i=0; i<blcks.length; i++){
         blcks[i].style.display = 'none';
      }
      var scrlis = $('.scrlis');
      for (var i=0; i<scrlis.length; i++){
         scrlis.show();
      }
   }
   else{
      $("body").css("overflow", "auto");
      document.getElementById('start').style.display = 'none';
      var blcks = $('.sdiv');
      for (var i=0; i<blcks.length; i++){
        blcks[i].style.height = 'auto';
      }
      var scrlis = $('.scrlis');
      for (var i=0; i<scrlis.length; i++){
         scrlis.hide();
      }
   }
   return;
}
  </script>
 </head>
 <body onresize="setwidth(); initialize()">
  <form action="https://ras.papercept.net/conferences/scripts/myprogram.pl" name="myprogram">
   <div id="container">
    <body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0">
     <link href="https://fonts.googleapis.com" rel="preconnect"/>
     <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
     <link href="https://fonts.googleapis.com/css2?family=Antonio:wght@500;700;900&amp;display=swap" rel="stylesheet"/>
     <table border="0" cellpadding="0" cellspacing="0" width="100%">
      <tr>
       <td alt="" height="140;" nowrap="" style="background-color:#000;" width="100%">
        <img alt="" border="0" height="140" src="/images/roman/roman24.png" style="position:absolute;top:0px;left:0px;"/>
        <span style="font-size: 34px; font-family: Antonio, sans-serif; font-weight: 900; color: #fff;line-height:40px;transform:scale(1.25,1);
text-align: left; position: absolute; top: 4px; left: 238px; text-shadow: 0px 0px 0px #ffffff, 0px 0px 0px #ffffff;">
         IEEE RO-MAN 2024
         <br/>
        </span>
        <span style="font-size: 18px; font-family: Antonio, sans-serif; font-weight: 900; color: #fff;line-height:40px;
text-align: left; position: absolute; top: 40px; left: 212px; text-shadow: 0px 0px 0px #ffffff, 0px 0px 0px #ffffff;">
         33RD IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION
        </span>
        <span style="font-size: 16px; font-family: Antonio, sans-serif; font-weight: 900; color: #F5BB77;line-height:40px;
text-align: left; position: absolute; top: 80px; left: 212px; text-shadow: 0px 0px 0px #ffffff, 0px 0px 0px #ffffff;">
         EMBRACING HUMAN-CENTERED HRI
        </span>
        <span style="font-size: 12px; font-family: Antonio, sans-serif; font-weight: 900; color: #fff;line-height:40px;
text-align: left; position: absolute; top: 100px; left: 212px; text-shadow: 0px 0px 0px #ffffff, 0px 0px 0px #ffffff;">
         AUGUST 26-30, 2024 - PASADENA - CALIFORNIA, USA
        </span>
       </td>
      </tr>
      <tr>
       <td alt="" border="0" height="1" style="background-color:#000;" width="100%">
       </td>
      </tr>
     </table>
     <table border="0" cellpadding="0" cellspacing="0" height="80%" width="100%">
      <tr>
       <td height="100%" style="background-color:#000;" width="5">
       </td>
       <td width="5">
       </td>
       <td height="100%" valign="top" width="100%">
        <br/>
        <div class="c" id="TheTop">
         <span id="pTitle">
          <a href="http://www.ro-man2024.org" target="_blank">
           <b>
            2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)
           </b>
          </a>
          <br/>
         </span>
         <span id="pSubTitle">
          <b>
           August 26-30, 2024, Pasadena, CA, USA
          </b>
         </span>
         <br/>
         <br/>
        </div>
        <div class="c" style="position: relative">
         <a href="ROMAN24_ProgramAtAGlanceWeb.html">
          Program at a Glance
         </a>
         <a href="ROMAN24_ContentListWeb_1.html">
          Monday
         </a>
         <a href="ROMAN24_ContentListWeb_2.html">
          Tuesday
         </a>
         <a href="ROMAN24_ContentListWeb_3.html">
          Wednesday
         </a>
         <a href="ROMAN24_ContentListWeb_4.html">
          Thursday
         </a>
         <a href="ROMAN24_ContentListWeb_5.html">
          Friday
         </a>
         <a href="ROMAN24_AuthorIndexWeb.html">
          Author Index
         </a>
         <a href="ROMAN24_KeywordIndexWeb.html">
          Keyword Index
         </a>
        </div>
        <div class="c">
         <p style="color: gray">
          Last updated on July 31, 2024. This conference program is tentative and subject to change
         </p>
        </div>
        <div class="c">
         <h3>
          Technical Program for Tuesday August 27, 2024
         </h3>
        </div>
        <p class="c">
        </p>
        <div class="c">
         <span style="color:gray ">
          To show or hide the keywords and abstract (text summary) of a paper (if available), click on the paper title
         </span>
         <br/>
         <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">
          Open all abstracts
         </a>
         <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">
          Close all abstracts
         </a>
        </div>
        <div class="c">
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuoc">
             <b>
              TuOC
             </b>
            </a>
           </td>
           <td class="r">
            Room T9 - Ballroom
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuoc" title="Click to go to the Program at a Glance">
             <b>
              Opening Ceremony
             </b>
            </a>
           </td>
           <td class="r">
            Interactive Session
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tupl">
             <b>
              TuPL
             </b>
            </a>
           </td>
           <td class="r">
            Room T9 - Ballroom
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tupl" title="Click to go to the Program at a Glance">
             <b>
              Plenary Talk - Vincent Vanhoucke
             </b>
            </a>
           </td>
           <td class="r">
            Plenary Session
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuam1_br">
             <b>
              TuAM1_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuam1_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuat1">
             <b>
              TuAT1
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuat1" title="Click to go to the Program at a Glance">
             <b>
              Personalising Robots Behaviour in Social Human-Robot Interactions I
             </b>
            </a>
           </td>
           <td class="r">
            Special Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat1_01">
             10:15-10:30, Paper TuAT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('171'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Human-Like Social Learning for Social Robots: A Systematic Review (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399793" title="Click to go to the Author Index">
             Burkart, Diana
            </a>
            (Karlsruhe Institute of Technology (KIT)),
            <a href="ROMAN24_AuthorIndexWeb.html#154831" title="Click to go to the Author Index">
             Bruno, Barbara
            </a>
            (Karlsruhe Institute of Technology (KIT))
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab171" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_learning_and_skill_acquisition_via_teaching_and_imitation" title="Click to go to the Keyword Index">
               Social Learning and Skill Acquisition Via Teaching and Imitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Social learning is a learning paradigm aiming to make the process of teaching new skills to an artificial intelligent agent (such as a robot) as close as possible to the process we employ when teaching skills to other humans. Equipping robot companions with the ability to learn from social interactions with their users would enable naive users to effortlessly teach skills to robots and support personalization and long-term use. Aiming to support research on the field, in this article we present a systematic literature review of human-like social learning for social robots, with a specific focus on works employing real robots and real human-robot social interactions. The review examines relevant papers from perspectives including the type of robot used, the task to be learned, and the method and assessment metrics employed, and allows for the identification of open research avenues.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat1_02">
             10:30-10:45, Paper TuAT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('152'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Social Attributes in a Handshake Robot Adaptive to Human Shaking Motion Using a CPG Controller (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#294772" title="Click to go to the Author Index">
             Yamasaki, Kakeru
            </a>
            (Kyushu Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#106955" title="Click to go to the Author Index">
             Shibata, Tomohiro
            </a>
            (Kyushu Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#105872" title="Click to go to the Author Index">
             Henaff, Patrick
            </a>
            (Université De Lorraine, CNRS, INRIA, LORIA, F-54000 Nancy, Fra)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab152" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the field of human-robot interaction(HRI), there are two types of human-robot coordination, one is physical coordination, which refers to the direct coordination between the robot and the human who interacts, and the other is social coordination, which refers to the human/robot coordination through emotional and cognitive states. In particular, social coordination can be an important factor in considering the social position of robots for the social implementation of technology. A handshake is usually the first interaction between humans that involves physical contact, and it is the first opportunity for social exchanges. In this paper, a handshaking robot that physically synchronizes with human handshaking motions is controlled with a Rowat-Selverston CPG (Central Pattern Generator) neuron oscillator. We compared a handshake robot adapted to a passive control method using simple impedance control and a cooperative and active control method using the developed CPG controller, and investigated how the control method affects the robot's sociodemographic attributes as perceived by humans. Changes in social attributes were evaluated in a 14-subject experiment using the Robotic Social Attributes Scale (RoSAS), which measures the robot's social attributes. The experimental results showed that CPG-based control methods improved the social attribute of being competent and comfortable with the robot. Based on the results of these studies, we propose guidelines for future research in the field of handshake robotics, which will contribute to the development of social robots in the future.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat1_03">
             10:45-11:00, Paper TuAT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('356'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Identifying Socio-Emotional Features with a Mediator Robot (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#281543" title="Click to go to the Author Index">
             Cooper, Sara
            </a>
            (Honda Research Institute Japan),
            <a href="ROMAN24_AuthorIndexWeb.html#116318" title="Click to go to the Author Index">
             Gomez, Randy
            </a>
            (Honda Research Institute Japan Co., Ltd),
            <a href="ROMAN24_AuthorIndexWeb.html#219226" title="Click to go to the Author Index">
             Szapiro, Deborah
            </a>
            (University of Technology Sydney),
            <a href="ROMAN24_AuthorIndexWeb.html#103530" title="Click to go to the Author Index">
             Merino, Luis
            </a>
            (Universidad Pablo De Olavide)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab356" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we identify a set of socio-emotional cues and signals that are promoted by a tabletop social mediator robot in the context of a school setting. The robot adopts different roles to enhance such socio-emotional features, consequently aiding their identification by a structured annotation system. Various socio-emotional signals were observed for different robot roles, as well as different cues (gaze, speech). Future work will analyze cultural nuances as it expands the pilot to more schools worldwide.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat1_04">
             11:00-11:15, Paper TuAT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('459'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Evaluation of Preference on Context-Aware Utterances Based on Personality Traits Using a Conversational Android Robot System (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399716" title="Click to go to the Author Index">
             Fukui, Ryota
            </a>
            (Tokyo University of Science),
            <a href="ROMAN24_AuthorIndexWeb.html#213633" title="Click to go to the Author Index">
             Yuguchi, Akishige
            </a>
            (Tokyo University of Science),
            <a href="ROMAN24_AuthorIndexWeb.html#101578" title="Click to go to the Author Index">
             Matsumoto, Yoshio
            </a>
            (AIST),
            <a href="ROMAN24_AuthorIndexWeb.html#147255" title="Click to go to the Author Index">
             Okadome, Yuya
            </a>
            (Tokyo University of Science)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab459" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#linguistic_communication_and_dialogue" title="Click to go to the Keyword Index">
               Linguistic Communication and Dialogue
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#androids" title="Click to go to the Keyword Index">
               Androids
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With recent technological improvements, the performance of conversational robots such as context-aware dialogue is expected to be dramatically enhanced. While context-aware utterance is considered one of the crucial functions of conversational robots, only a few studies investigate the relationship between context awareness and user preference.
             <p>
              As is the case with human social relationships, human-robot interactions are under the effect of user's attributes such as personality, and thus considering these factors of users is essential to continue interacting with robots. In this paper, we evaluate the preference for context awareness based on personality trait effects in an open-domain dialogue using a conversational android robot system. Two types of utterances, context-aware and context-free, are implemented in the robot system, and the participants talk with and evaluate our conversational system. The preference for two systems and the personality traits of each participant are collected. We analyze the relationship between preferences and personalities. The results suggest that the participants' preference for whether an utterance is context-aware or context-free is distinguished by some personality traits.
             </p>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuat2">
             <b>
              TuAT2
             </b>
            </a>
           </td>
           <td class="r">
            Room T2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuat2" title="Click to go to the Program at a Glance">
             <b>
              Virtual and Augmented Tele-Presence Environments I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat2_01">
             10:15-10:30, Paper TuAT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('53'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Designing Augmented Reality Robot Guidance Interactions through the Metaphors of Re-Embodiment and Telepresence
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#356281" title="Click to go to the Author Index">
             Zhu, Yifei
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#394063" title="Click to go to the Author Index">
             Brush, Colin
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#174132" title="Click to go to the Author Index">
             Williams, Tom
            </a>
            (Colorado School of Mines)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab53" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots deployed into real-world task-based environments may need to provide assistance, troubleshooting, and on-the-fly instruction for human users. While previous work has considered how robots can provide this assistance while co-located with human teammates, it is unclear how robots might best support users once they are no longer co-located. We propose the use of Augmented Reality as a medium for conveying long-distance task guidance from humans’ existing robot teammates, through Augmented Reality facilitated Robotic Guidance (ARRoG). Moreover, because there are multiple ways that a robot might project its identity through an Augmented Reality Head Mounted Display, we identify two candidate designs inspired by existing interaction patterns in the human-robot interaction (HRI) literature (re-embodiment-based and telepresence-based identity projection designs), present the results of a design workshop to explore how these designs might be most effectively implemented, and the results of a human-subject study intended to validate these designs
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat2_02">
             10:30-10:45, Paper TuAT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('40'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Approach of Team Avatar-Hubo to the ANA Avatar XPRIZE Finals
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#225526" title="Click to go to the Author Index">
             KIM, BaekSeok
            </a>
            (University of Nevada, Las Vegas),
            <a href="ROMAN24_AuthorIndexWeb.html#317470" title="Click to go to the Author Index">
             Kassai, Nathan
            </a>
            (University of Nevada, Las Vegas),
            <a href="ROMAN24_AuthorIndexWeb.html#317471" title="Click to go to the Author Index">
             Castrejon, Zahir
            </a>
            (University of Nevada Las Vegas),
            <a href="ROMAN24_AuthorIndexWeb.html#317469" title="Click to go to the Author Index">
             Kosanovic, Nicolas
            </a>
            (University of Louisville),
            <a href="ROMAN24_AuthorIndexWeb.html#215308" title="Click to go to the Author Index">
             Chagas Vaz, Jean
            </a>
            (University of Louisville),
            <a href="ROMAN24_AuthorIndexWeb.html#100181" title="Click to go to the Author Index">
             Oh, Paul Y.
            </a>
            (University of Nevada, Las Vegas (UNLV))
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab40" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#embodiment__empathy_and_intersubjectivity" title="Click to go to the Keyword Index">
               Embodiment, Empathy and Intersubjectivity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents Team Avatar-Hubo's Avatar system designed for telepresence, along with our strategy to fulfill the testing task requirements in the ANA Avatar XPRIZE Finals. Our system is built upon the foundation of DRC-Hubo and incorporates a VR Motion Tracker system. During the ANA Avatar XPRIZE Finals evaluation, our proposed Avatar system achieved the 11th Place Award among a pool of 17 Finalists. We provide comprehensive insights into our Avatar System, including its Hardware-Software Architecture, capabilities in Auditory-Vision, manipulation, Texture perception, locomotion, and Interface. Furthermore, we extensively discuss the key lessons we learned during our participation in the competition.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat2_03">
             10:45-11:00, Paper TuAT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('429'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Advancing Behavior Generation in Mobile Robotics through High-Fidelity Procedural Simulations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#289578" title="Click to go to the Author Index">
             Kich, Victor Augusto
            </a>
            (University of Tsukuba),
            <a href="ROMAN24_AuthorIndexWeb.html#253548" title="Click to go to the Author Index">
             Bottega, Jair Augusto
            </a>
            (UFSM),
            <a href="ROMAN24_AuthorIndexWeb.html#392301" title="Click to go to the Author Index">
             Steinmetz, Raul
            </a>
            (Universidade Federal De Santa Maria),
            <a href="ROMAN24_AuthorIndexWeb.html#270442" title="Click to go to the Author Index">
             Grando, Ricardo
            </a>
            (Federal University of Rio Grande),
            <a href="ROMAN24_AuthorIndexWeb.html#164643" title="Click to go to the Author Index">
             Yorozu, Ayanori
            </a>
            (University of Tsukuba),
            <a href="ROMAN24_AuthorIndexWeb.html#101293" title="Click to go to the Author Index">
             Ohya, Akihisa
            </a>
            (University of Tsukuba)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab429" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces YamaS, a simulator integrating Unity3D Engine with Robotic Operating System for robot navigation research and aims to facilitate the development of both Deep Reinforcement Learning (Deep-RL) and Natural Language Processing (NLP). It supports single and multi-agent configurations with features like procedural environment generation, RGB vision, and dynamic obstacle navigation. Unique to YamaS is its ability to construct single and multi-agent environments, as well as generating agent's behaviour through textual descriptions. The simulator's fidelity is underscored by comparisons with the real-world Yamabiko Beego robot, demonstrating high accuracy in sensor simulations and spatial reasoning. Moreover, YamaS integrates Virtual Reality (VR) to augment Human-Robot Interaction (HRI) studies, providing an immersive platform for developers and researchers. This fusion establishes YamaS as a versatile and valuable tool for the development and testing of autonomous systems, contributing to the fields of robot simulation and AI-driven training methodologies.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat2_04">
             11:00-11:15, Paper TuAT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('168'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Immersive Control of a Quadruped Robot with Virtual Reality Eye-Wear
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#375732" title="Click to go to the Author Index">
             Yousefi, Ali
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#360905" title="Click to go to the Author Index">
             Betta, Zoe
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#269854" title="Click to go to the Author Index">
             Mottola, Giovanni
            </a>
            (University of Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#155018" title="Click to go to the Author Index">
             Recchiuto, Carmine Tommaso
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#108312" title="Click to go to the Author Index">
             Sgorbissa, Antonio
            </a>
            (University of Genova)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab168" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work describes an immersive control system for a quadruped robot, designed to track the head movements of the operator wearing a virtual reality eye-wear, while also utilizing joystick commands for locomotion control. The article details the implemented closed-loop velocity control approach and the locomotion task specifications. The proposed method has been implemented on a Spot robot from Boston Dynamics, with Meta Quest 2 virtual reality system. Evaluation of the approach involved a user study, where participants engaged in immersive control of the quadruped robot within an indoor experimental environment and provided feedback through standardized questionnaires. Pairwise comparison of the resulting data revealed significant advantages for the proposed immersive control system over a standard remote controller, with enhanced performance observed in the second trial of using the control system. However, participants lacking experience with virtual reality systems reported increased distress symptoms following the experiment.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuat3">
             <b>
              TuAT3
             </b>
            </a>
           </td>
           <td class="r">
            Room T3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuat3" title="Click to go to the Program at a Glance">
             <b>
              Affective Computing
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat3_01">
             10:15-10:30, Paper TuAT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('271'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Evaluation of Social Robots with Social Signals in Public Spaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400155" title="Click to go to the Author Index">
             Schiffmann, Michael
            </a>
            (TH Köln University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#326261" title="Click to go to the Author Index">
             Richert, Anja
            </a>
            (University of Applied Sciences Cologne)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab271" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Evaluating the users` subjective impression of social robots with questionnaires in field trials is time-consuming, and it is impossible to analyze every interaction as users cannot always participate for various reasons. In social robotics, social signals (e.g., gestures, facial expressions, body language) can be used to control and adapt the robot's behavior to improve interaction. Social signals should allow a statement to be made about how the user feels in a situation or what attitude or opinion they have about something. This paper focuses on finding out to what extent social signals can contribute to an automatic evaluation of the subjective user satisfaction without the need for user questionnaires and solely through social signals. For this purpose, it is first relevant whether social signals occur and if they can be used in the present use case. For this purpose, a field test was carried out in the entrance area of a city administration, in which the social robot Furhat was used as a point for service and information. Four interactions were recorded on video with the consent of the users, and a post-questionnaire on satisfaction was collected for each case. A qualitative video analysis was used to examine the interactions concerning potential social signals displayed by the users. The results suggest that the direction of view and the head orientation are the body parts that are mostly moved in the interaction. All users interacted with the system with relatively little movement of the rest of the body. The results suggest that it is not recommended to focus only on social signals alone.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat3_02">
             10:30-10:45, Paper TuAT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('148'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sigh!!! There Is More Than Just Faces and Verbal Speech to Recognize Emotion in Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#359669" title="Click to go to the Author Index">
             Maharjan, Rahul Singh
            </a>
            (The University of Manchester),
            <a href="ROMAN24_AuthorIndexWeb.html#229547" title="Click to go to the Author Index">
             Romeo, Marta
            </a>
            (Heriot-Watt University),
            <a href="ROMAN24_AuthorIndexWeb.html#240194" title="Click to go to the Author Index">
             Cangelosi, Angelo
            </a>
            (University of Manchester)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab148" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Understanding human emotions is paramount for effective human-human interactions. As technology advances, social robots are increasingly being developed with the capability to discern and respond to human emotions, with the ultimate aim of providing assistance and companionship. However, existing research on emotion recognition for human-robot interaction predominantly focuses on facial expressions or verbal speech, neglecting other potential mediums of emotional expression. In this study, we shed light on the significance of considering various forms of emotional expression, mainly non-verbal vocalization known as vocal bursts, which have been overlooked in the context of emotion modeling for human-robot interaction. Vocal bursts, characterized by brief and intense vocal utterances, represent a rich source of emotional cues that can significantly enhance the capabilities of social robots in understanding and responding to human emotions. Driven by the increasing interest in vocal bursts within speech and affective computing research, we propose a baseline model for affective vocal burst recognition that can outperform large audio models. The proposed baseline model achieves weighted F1 scores of 0.606, 0.342, and 0.287 on 10, 24, and 30 emotion classes, respectively. Additionally, we identify challenges that must be addressed to enhance affective vocal burst recognition for human-robot interaction. Code available at github.com/rahullabs/Sigh
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat3_03">
             10:45-11:00, Paper TuAT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('42'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Feeling of Kawaii Toward a Robot's Head-Tilting Motion: Effects of Speed, Direction, and Accompanying Hand Gestures
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#101690" title="Click to go to the Author Index">
             Shiomi, Masahiro
            </a>
            (ATR),
            <a href="ROMAN24_AuthorIndexWeb.html#391246" title="Click to go to the Author Index">
             Kato, Yuina
            </a>
            (Nara Women's University),
            <a href="ROMAN24_AuthorIndexWeb.html#275577" title="Click to go to the Author Index">
             Nittono, Hiroshi
            </a>
            (Osaka University),
            <a href="ROMAN24_AuthorIndexWeb.html#266512" title="Click to go to the Author Index">
             Anzai, Emi
            </a>
            (Nara Women's University),
            <a href="ROMAN24_AuthorIndexWeb.html#202204" title="Click to go to the Author Index">
             Saiwaki, Naoki
            </a>
            (Nara Women's University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab42" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Head-tilting motions are social signals that play a key role in communication. In human-robot interaction, researchers have found that this motion increases the observer's feelings of kawaii toward a robot. Kawaii is a Japanese word meaning cute and adorable, and the feeling of kawaii refers to such affectionate, tender feelings toward an object. The feeling of kawaii has proven to be crucial for designing socially acceptable robots. However, the impact of the head-tilting speed of social robots on the observer's kawaii feelings remains unclear because previous research mainly used static images. This study determined the optimal head-tilting speed and direction to deepen the observer's feeling of kawaii toward a robot. Our findings found that participants reported feeling more kawaii toward a robot when it tilted its head to its right within 0.5 or 1 second. Our results also showed that gestures associated with head tilting are critical for robot behavior design.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat3_04">
             11:00-11:15, Paper TuAT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('140'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Analysis of Heart-To-Heart Communication with Robot Using Transfer Entropy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399657" title="Click to go to the Author Index">
             Sato, Moe
            </a>
            (Osaka University),
            <a href="ROMAN24_AuthorIndexWeb.html#104030" title="Click to go to the Author Index">
             Minato, Takashi
            </a>
            (RIKEN),
            <a href="ROMAN24_AuthorIndexWeb.html#196617" title="Click to go to the Author Index">
             Funayama, Tomo
            </a>
            (Osaka University),
            <a href="ROMAN24_AuthorIndexWeb.html#147904" title="Click to go to the Author Index">
             Sumioka, Hidenobu
            </a>
            (ATR),
            <a href="ROMAN24_AuthorIndexWeb.html#163804" title="Click to go to the Author Index">
             Sakai, Kurima
            </a>
            (ATR),
            <a href="ROMAN24_AuthorIndexWeb.html#223774" title="Click to go to the Author Index">
             Mikata, Ryusuke
            </a>
            (ATR),
            <a href="ROMAN24_AuthorIndexWeb.html#101596" title="Click to go to the Author Index">
             Ishiguro, Hiroshi
            </a>
            (Osaka University),
            <a href="ROMAN24_AuthorIndexWeb.html#399665" title="Click to go to the Author Index">
             Akane, Kikuchi
            </a>
            (KiQ Co., Ltd.,),
            <a href="ROMAN24_AuthorIndexWeb.html#399666" title="Click to go to the Author Index">
             Kaito, Sakuma
            </a>
            (Apples and Oranges Co., Ltd.,),
            <a href="ROMAN24_AuthorIndexWeb.html#399671" title="Click to go to the Author Index">
             Kazuya, Horibe
            </a>
            (Osaka University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab140" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#androids" title="Click to go to the Keyword Index">
               Androids
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human robot interaction studies have investigated how various non-verbal expressions of robots can enhance feelings of familiarity and trust in users' and establish good relationships between users and robots. In the field of art, this approach has been taken one step further by creating robot artworks and demonstrations through which people feel as if they are experiencing heart-to-heart communication with a robot. Although robotic behavior that conveys such a feeling is useful for human coexistence and providing a sense of security and trust, no engineering methodology can yet achieve such behavior. This study attempts to explain what kind of robot behavior is connected to such feelings by analyzing demonstrations of such a robot based on information theory. We found that the intensity of these feelings can be partially explained by transfer entropy between humans and the robot's body movements. We expect this research to clarify how to design a robot's behavior so that it can provide heart-to-heart communication with people as well as how to construct a robot that can build solid relationships with people in daily life.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuat4">
             <b>
              TuAT4
             </b>
            </a>
           </td>
           <td class="r">
            Room T4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuat4" title="Click to go to the Program at a Glance">
             <b>
              Human Factors and Ergonomics I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat4_01">
             10:15-10:30, Paper TuAT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('244'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Exploratory Study on the Impact of Varying Levels of Robot Control on Presence in Robot-Mediated Communication
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#281588" title="Click to go to the Author Index">
             Arevalo Arboleda, Stephanie
            </a>
            (Technische Universität Ilmenau),
            <a href="ROMAN24_AuthorIndexWeb.html#354574" title="Click to go to the Author Index">
             Fischedick, Söhnke Benedikt
            </a>
            (Ilmenau University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#390860" title="Click to go to the Author Index">
             Diao, Chenyao
            </a>
            (TU-Ilmenau),
            <a href="ROMAN24_AuthorIndexWeb.html#391877" title="Click to go to the Author Index">
             Richter, Kay
            </a>
            (Ilmenau University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#102246" title="Click to go to the Author Index">
             Gross, Horst-Michael
            </a>
            (Ilmenau University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#349015" title="Click to go to the Author Index">
             Raake, Alexander
            </a>
            (Ilmenau University of Technology, Audiovisual Technology Group)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab244" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_presence_for_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Social Presence for Robots and Virtual Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Telepresence robots can enhance communication experiences by providing a sense of physical presence, embodiment and may evoke co-presence. In spite of that, telepresence robots have not made it fully to consumer markets. In this paper, we investigate how different levels of controlling a telepresence robot (teleoperation, shared control, and no control) influence presence. To this aim, we conducted a study (N=45) where participants were evenly distributed to one of the robot control conditions. The task involved navigating an unknown room and listening to stories told by a person co-located with the robot. We collected subjective impressions of presence using the temple presence inventory and performed a thematic content analysis on a post-experiment interview. Our results suggest nuances in perceived presence under different levels of robot control after performing a thematic content analysis. Co-presence can be experienced during teleoperation and shared control, and teleoperation may evoke negative sentiments if it does not provide enough spatial information during navigation. However, our results did not point to significant differences in spatial or social presence. We consider that these findings encourage further discussions on how presence is perceived in robot-mediated communication.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat4_02">
             10:30-10:45, Paper TuAT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('240'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              All Too White? Effects of Anthropomorphism on the Stereotypical Perception of Robot Color
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400068" title="Click to go to the Author Index">
             Barenbrock, Julia
            </a>
            (University of Applied Sciences Ruhr West),
            <a href="ROMAN24_AuthorIndexWeb.html#281767" title="Click to go to the Author Index">
             Eimler, Sabrina C.
            </a>
            (Hochschule Ruhr West, University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#349668" title="Click to go to the Author Index">
             Helgert, André
            </a>
            (University of Applied Sciences Ruhr West),
            <a href="ROMAN24_AuthorIndexWeb.html#281844" title="Click to go to the Author Index">
             Straßmann, Carolin
            </a>
            (University of Applied Sciences Ruhr West)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab240" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#anthropomorphic_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Anthropomorphic Robots and Virtual Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Looking at state-of-the-art robots, the majority have a white surface. This raises the questions of why robots are designed predominantly white, and how surface color affects the robot’s social perception. This not only reinforces human stereotypes but also influences the acceptance and usage of robots. Accordingly, this online study with a 2 (black vs. white) x 3 (Pepper robot vs. Temi robot vs. Kuka robot) within-subjects design (N = 100) investigated the effect on racist stereotypical perception and the role of anthropomorphism using implicit (Implicit Association Test) and explicit (Robotic Social Attributes Scale) measures and questionnaires on behavioral intentions. Our results show that robot color influences the implicit perception of robots but rarely affects explicit perception and behavior measures. Moreover, no effects of the robots' anthropomorphism on racist perceptions were found. As more effects of anthropomorphism on our outcome variables were found than for robot color, we assume that robot color activates stereotypes in terms of color rather than race.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat4_03">
             10:45-11:00, Paper TuAT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('467'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Large Language Model Driven Interactive Learning for Real-Time Cognitive Load Prediction in Human-Swarm Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#382332" title="Click to go to the Author Index">
             Zang, Wenshuo
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#364081" title="Click to go to the Author Index">
             Hu, Mengsha
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#238447" title="Click to go to the Author Index">
             Liu, Rui
            </a>
            (Kent State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab467" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_and_sensorimotor_development" title="Click to go to the Keyword Index">
               Cognitive and Sensorimotor Development
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The rapid advancements of drones have demonstrated the versatility and promising potential of human-swarm systems (HSS) across various domains. However, human performance within these systems may be impaired by factors such as limited domain knowledge and mental stress, often leading to cognitive overload and hindering the efficiency and effectiveness of human-swarm teaming. Consequently, the accurate monitoring of cognitive load levels is crucial for optimizing HSS performance. To address the challenges of existing measurement methods, which are often expensive, time-consuming, or lack real-time capabilities, we propose a Large Language Model driven cognitive load prediction framework. This framework integrates comprehensive task context, domain knowledge, and behavior analysis to provide fast and cost-effective predictions in complex scenarios. By leveraging the capabilities of Large Language Models and employing reinforcement learning to model the cognitive load generation, our framework aims to offer real-time insights into human-related factors causing high cognitive load and predict cognitive levels over time, ultimately enhancing the performance of HSS teaming.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat4_04">
             11:00-11:15, Paper TuAT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('80'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Improving Transparency in Human-Collective Visualizations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#396791" title="Click to go to the Author Index">
             Bhagat Smith, Joshua
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#236611" title="Click to go to the Author Index">
             Baskaran, Prakash
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#107051" title="Click to go to the Author Index">
             Adams, Julie
            </a>
            (Oregon State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab80" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quantifying transparency requires evaluating the transparency embedded in the various system design elements to determine how they impact one another and influence human-collective interactions. Prior work demonstrated limitations of an abstract collective interaction. Interface designs to address these limitations and improve human-collective interaction transparency were evaluated for a sequential best-of-N decision-making task with four collectives, each consisting of 200 individual entities. The Informed and Simple visualizations’ predictive progress bars improved transparency and the overall human-collective team performance.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuat5">
             <b>
              TuAT5
             </b>
            </a>
           </td>
           <td class="r">
            Room T5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuat5" title="Click to go to the Program at a Glance">
             <b>
              Machine Learning and Adaptation I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat5_01">
             10:15-10:30, Paper TuAT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('76'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Deep Learning-Based Adaptation of Robot Behaviour for Assistive Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#346246" title="Click to go to the Author Index">
             Stolarz, Michał Mikołaj
            </a>
            (Hochschule Bonn-Rhein-Sieg, Sankt Augustin),
            <a href="ROMAN24_AuthorIndexWeb.html#229547" title="Click to go to the Author Index">
             Romeo, Marta
            </a>
            (Heriot-Watt University),
            <a href="ROMAN24_AuthorIndexWeb.html#195746" title="Click to go to the Author Index">
             Mitrevski, Alex
            </a>
            (Hochschule Bonn-Rhein-Sieg),
            <a href="ROMAN24_AuthorIndexWeb.html#104247" title="Click to go to the Author Index">
             Plöger, Paul G.
            </a>
            (Hochschule Bonn Rhein Sieg)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab76" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot behaviour models in socially assistive robotics are typically trained using high-level features, such as a user's engagement, such that inaccuracies in the feature extraction can have a significant effect on a robot's subsequent performance. In this paper, we study whether a behaviour model can be meaningfully represented using an end-to-end approach, where multimodal input, concretely visual data and activity information, is directly processed by a neural network. This paper concretely analyses the different building blocks of such a model, such that the aim is to identify a suitable architecture that can meaningfully combine the different modalities for guiding a robot's behaviour. We conduct the analysis in the context of a sequence learning game, such that we compare different vision-only models that are then combined with an activity processing network into a joint multimodal model. The results of our evaluation on a dedicated dataset from the sequence learning game demonstrate that a multimodal end-to-end behaviour model has potential for assistive robotics — we report an F1 score of around 0.88 across different dataset-based test scenarios — but the real-life transferability strongly depends on whether the data is diverse enough for capturing meaningful variations in real-world scenarios, such as users being at different distances from a robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat5_02">
             10:30-10:45, Paper TuAT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('282'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Two-Stream Architecture with Contrastive and Self-Supervised Attention Feature Fusion for Error-Related Potentials Classification
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#157255" title="Click to go to the Author Index">
             Garrote, Luís Carlos
            </a>
            (Institute of Systems and Robotics, University of Coimbra),
            <a href="ROMAN24_AuthorIndexWeb.html#230193" title="Click to go to the Author Index">
             Perdiz, João
            </a>
            (University of Coimbra),
            <a href="ROMAN24_AuthorIndexWeb.html#400173" title="Click to go to the Author Index">
             Yasemin, Mine
            </a>
            (ISR-UC),
            <a href="ROMAN24_AuthorIndexWeb.html#127988" title="Click to go to the Author Index">
             Pires, Gabriel
            </a>
            (University of Coimbra),
            <a href="ROMAN24_AuthorIndexWeb.html#103575" title="Click to go to the Author Index">
             Nunes, Urbano J.
            </a>
            (Instituto De Sistemas E Robotica)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab282" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Error-related potentials (ErrPs) extracted from electroencephalographic signals hold potential for application in Brain-Machine Interfaces, in contexts such as robot teleoperation or shared control in assistive platforms. Due to difficulties in signal classification, in part caused by its non-stationary and noisy nature, their use has not been fully realized yet. This work proposes a new approach to ErrP classification based on a two-stream deep learning architecture with three training stages. Its first stage is a self-supervised autoencoder architecture with a multi-head attention layer providing relevant latent features. The second stage comprises a supervised contrastive learning approach considering two backbone networks, where one inherits weights from the first stage and the other is updated by considering the feature embeddings distribution. The final stage comprises supervised classification, where the two backbones are fused and used to classify the input EEG signal. At the end of the three stages, a data-driven two-stream ErrP model is obtained. Twenty-five variants of the proposed approach using the Deep Convolutional Network, Shallow Convolutional Network and EEGNet backbones were tested in an ablation study and benchmarked against a large number of classical classification methods, using data from the BNCI dataset intended to assess cross subject generalization capabilities. The proposed approach obtained the best results overall, highlighting the approach's capabilities in capturing relevant representations of the EEG signal.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat5_03">
             10:45-11:00, Paper TuAT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('453'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Discrimination and Prediction of Soft Surfaces for Cloth Classification
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#397801" title="Click to go to the Author Index">
             Duran Jimenez, Raul Ariel
            </a>
            (Kyutech),
            <a href="ROMAN24_AuthorIndexWeb.html#359952" title="Click to go to the Author Index">
             Suppaadirek, Natchanon
            </a>
            (Kyushu Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#106955" title="Click to go to the Author Index">
             Shibata, Tomohiro
            </a>
            (Kyushu Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab453" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The sense of touch plays an indispensable role in human perception by enabling the discernment of various objects based on their textures, roughness, and shapes. In the realm of robotics, touch sensors play a vital role in the manipulation and control of objects. By providing information on mechanical properties, these sensors facilitate the visualization of grip, shear, and pressure force at contact points. However, handling deformable objects such as clothing poses a significant challenge. While some research relies on visual sensors for the detection and classification of clothing types, this research focuses on leveraging tactile sensors for the discrimination and classification of deformable surfaces in real-time.
             <p>
              The aim is centered on soft surfaces, with a specific focus on examining seven different types of materials. The system we present focuses on signal processing and machine learning techniques for cloth recognition, and classification in real time. The system integrates a soft touch sensor fingertip into a robotic arm to achieve these objectives.
              <p>
               The current study has developed a classification model leveraging Long Short-Term Memory (LSTM) networks. The objective of this model is to accurately classify seven distinct types of clothing by analyzing their texture features.
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat5_04">
             11:00-11:15, Paper TuAT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('69'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              GraspPC: Generating Diverse Hand Grasp Point Clouds on Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#394681" title="Click to go to the Author Index">
             Megyeri, Ava
            </a>
            (Wright State University),
            <a href="ROMAN24_AuthorIndexWeb.html#341455" title="Click to go to the Author Index">
             Wiederhold, Noah
            </a>
            (Clarkson University),
            <a href="ROMAN24_AuthorIndexWeb.html#195306" title="Click to go to the Author Index">
             Kyrarini, Maria
            </a>
            (Santa Clara University),
            <a href="ROMAN24_AuthorIndexWeb.html#268974" title="Click to go to the Author Index">
             Banerjee, Sean
            </a>
            (Wright State Univeristy),
            <a href="ROMAN24_AuthorIndexWeb.html#191250" title="Click to go to the Author Index">
             Banerjee, Natasha Kholgade
            </a>
            (Wright State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab69" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present GraspPC, an approach to perform learning-based synthesis of multiple human hand grasps as point clouds from point clouds of objects. GraspPC benefits human-robot handover approaches by providing hypotheses of human grasp on objects to inform robotic manipulation algorithms on how to bias robotic grasp for safe handover. Existing learning-based approaches to conduct hand grasp prediction require datasets to contain annotated articulated hand models, making them difficult to train on datasets that lack hand model annotations. GraspPC treats the problem of hand point cloud generation from object point clouds as a set-to-set translation problem. We contribute a Transformer architecture to synthesize point clouds via GraspPC. To generate diverse hand grasps, we generate multiple object-dependent queries and train the network using a winner-takes-gradient strategy. We show results of diverse grasps by training and testing on a variety of real-world datasets. We demonstrate how human grasps generated by GraspPC can be used to filter robotic grasp candidates to inform human-robot handover. Our code is available at [link on acceptance].
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuat6">
             <b>
              TuAT6
             </b>
            </a>
           </td>
           <td class="r">
            Room T6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuat6" title="Click to go to the Program at a Glance">
             <b>
              Motion Planning and Navigation in Human-Centered Environments I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat6_01">
             10:15-10:30, Paper TuAT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('245'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Why Did My Robot Choose This Path? Explainable Path Planning for Off-Road Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#323647" title="Click to go to the Author Index">
             Eder, Matthias
            </a>
            (Graz University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#109833" title="Click to go to the Author Index">
             Steinbauer-Wagner, Gerald
            </a>
            (Graz University of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab245" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the field of off-road navigation, where traditional maps often fall short, intuitive and efficient path planning is essential for autonomous off-road vehicles. Navigating in off-road terrain poses unique challenges, requiring innovative solutions for users to understand and trust path suggestions made by an autonomous system. In this paper, we explore the integration of Explainable AI into off-road navigation systems to better understand the complexity of off-road environments. Our research introduces a method tailored to generate contextual explanations for chosen paths using terrain features, environmental factors, and robot capabilities. By combining inverse optimization techniques with shortest path algorithms, our approach aims to answer the question "Why is path p* recommended over path p', which was expected by the user?" These explanations aim to shed light on the process of a robot's path planning task, focusing on elevation changes, terrain obstacles, and optimal path choices, thus improving the user's understanding of the chosen paths. A short user study evaluating the provided explanations generated in different off-road environments validates the effectiveness of our explanation algorithm and shows that it contributes to understanding the planning process of off-road navigation systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat6_02">
             10:30-10:45, Paper TuAT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('298'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              But Where Are You Going?! Motion Is What Is Most Important for Real-World Co-Present Mobile Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#173114" title="Click to go to the Author Index">
             Fraune, Marlena
            </a>
            (New Mexico State University),
            <a href="ROMAN24_AuthorIndexWeb.html#106579" title="Click to go to the Author Index">
             Knepper, Ross
            </a>
            (--)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab298" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile robots are being introduced to industrial workplaces in roles that require copresence with humans. To develop effective robots that do not negatively impact humans, including their subjective experience and ability to get their work done, we must understand humans’ needs for working near these robots. In this paper, we examine what human workers need from copresent robots’ motion during work at a large warehouse. To do so, report and synthesize findings about robot motion from across five studies (e.g., focus group, observation, experiment). Results indicate that workers were most focused on robot movement, including consistency, distance, prioritizing people, and indicating when it sensed people. Researchers and practitioners can use these findings to prioritize what aspects of mobile robots to develop to improve human worker experiences around robots and team efficiency.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat6_03">
             10:45-11:00, Paper TuAT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('413'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Invariant Extended Kalman Filter for IMU-UWB Sensor Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#396817" title="Click to go to the Author Index">
             Oursland, Jacob
            </a>
            (South Dakota School of Mines and Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#380650" title="Click to go to the Author Index">
             Mehrabian, Mohammadreza
            </a>
            (South Dakota School of Mines and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab413" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Orientation estimation is crucial for the successful operation of robots in autonomous control, enabling effective navigation, environmental interaction, and precise task execution. Obtaining a low-error orientation estimate promptly is essential for ensuring safe and successful autonomous robot operation. However, existing approaches, such as the Error-State Kalman Filter (ESKF), suffer from slow convergence, leading to unstable flights and increased collision risks, particularly when initial heading alignment errors occur. We introduce an Invariant Extended Kalman Filter (InEKF) for inertial measurement unit (IMU) and ultra-wideband (UWB) sensor fusion that addresses the slow convergence issue in orientation estimation of the Crazyflie Bolt drone. We demonstrate significant improvements over the reference ESKF, achieving a RMS heading error of 0.08 radians (4.6 degrees) compared to the ESKF's 1.34 radians (77 degrees), a 94% reduction in error, when starting from a 180 degree initial heading error, while maintaining similar accuracy levels in position and velocity estimates.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuat6_04">
             11:00-11:15, Paper TuAT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('118'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fusion of Inertial Sensor Suit and Monocular Camera for 3D Human Pelvis Pose Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#333814" title="Click to go to the Author Index">
             Popescu, Mihaela
            </a>
            (University of Bremen),
            <a href="ROMAN24_AuthorIndexWeb.html#256818" title="Click to go to the Author Index">
             Shinde, Kashmira
            </a>
            (German Aerospace Center (DLR)),
            <a href="ROMAN24_AuthorIndexWeb.html#399552" title="Click to go to the Author Index">
             Sharma, Proneet
            </a>
            (DFKI GmbH),
            <a href="ROMAN24_AuthorIndexWeb.html#310510" title="Click to go to the Author Index">
             Gutzeit, Lisa
            </a>
            (University of Bremen),
            <a href="ROMAN24_AuthorIndexWeb.html#109056" title="Click to go to the Author Index">
             Kirchner, Frank
            </a>
            (University of Bremen)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab118" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In real-world scenarios, robots come closer to humans in many applications, sharing the same workspace or even manipulating the same objects. To ensure safe and intuitive collaboration, it is crucial to have an accurate knowledge of the human's 3D position in space, which should be estimated with high precision, high frequency and low latency. However, individual sensors such as inertial measurement units (IMUs) or cameras cannot meet all requirements for reliable human pose estimation under conditions such as long operating times, large distances and occlusions. In this study, we highlight the limitations of different visual pose methods and present a fused approach for real-time estimation of the 3D position of the human pelvis using machine learning-based visual pose from a monocular camera and an IMU sensor suit. The multimodal fusion is based on the Invariant Extended Kalman filter (InEKF) on Lie Groups, which fuses drift-free visual poses with high-frequency inertial measurements in a loosely-coupled manner. The evaluation is performed on a recorded dataset of multiple subjects performing various experimental scenarios. The results show that the fused approach can increase the accuracy and robustness of the estimates, taking a step closer towards smooth human-robot collaboration.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuam2_br">
             <b>
              TuAM2_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuam2_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break II
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tubt1">
             <b>
              TuBT1
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tubt1" title="Click to go to the Program at a Glance">
             <b>
              Personalising Robots Behaviour in Social Human-Robot Interactions II
             </b>
            </a>
           </td>
           <td class="r">
            Special Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt1_01">
             11:30-11:45, Paper TuBT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('120'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring the Potential of a Robot-Assisted Frailty Assessment System for Elderly Care (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#372362" title="Click to go to the Author Index">
             Civit, Aniol
            </a>
            (Universitat Politècnica De Catalunya),
            <a href="ROMAN24_AuthorIndexWeb.html#214124" title="Click to go to the Author Index">
             Andriella, Antonio
            </a>
            (Artificial Intelligence Research Institute, CSIC),
            <a href="ROMAN24_AuthorIndexWeb.html#372595" title="Click to go to the Author Index">
             Antonio, Maite
            </a>
            (Institut Català D'Oncologia),
            <a href="ROMAN24_AuthorIndexWeb.html#373601" title="Click to go to the Author Index">
             Javierre, Casimiro
            </a>
            (Universitat De Barcelona),
            <a href="ROMAN24_AuthorIndexWeb.html#399591" title="Click to go to the Author Index">
             Boqué, Concepción
            </a>
            (Institut Català D'Oncologia),
            <a href="ROMAN24_AuthorIndexWeb.html#103995" title="Click to go to the Author Index">
             Alenyà, Guillem
            </a>
            (Institut De Robòtica I Informàtica Industrial CSIC-UPC)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab120" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#medical_and_surgical_applications" title="Click to go to the Keyword Index">
               Medical and Surgical Applications
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Frailty assessment plays a pivotal role in providing older adults care. However, the current process is time-consuming and only measures patients' completion time for each test. This paper introduces a set of algorithms to be used in robots to autonomously perform frailty assessments. In doing so we aim at reducing therapists' burden and provide additional frailty-related metrics that can enhance the effectiveness of diagnosis. We conducted a pilot study with 22 elderly participants and compared our system's performance with that of medical professionals to assess its precision. The results demonstrate that our approach achieved performances close to that of its human counterpart. This research represents an important step forward in the integration of social robotics in healthcare, offering potential benefits for patient care and clinical decision-making.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt1_02">
             11:45-12:00, Paper TuBT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('312'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Are Emotions Important? a Study on Social Distances for Path Planning Based on Emotions (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400234" title="Click to go to the Author Index">
             Mizaridis, Vasileios
            </a>
            (University of Naples Federico II),
            <a href="ROMAN24_AuthorIndexWeb.html#235787" title="Click to go to the Author Index">
             Vigni, Francesco
            </a>
            (Interdepartmental Center for Advances in Robotic Surgery - ICARO),
            <a href="ROMAN24_AuthorIndexWeb.html#400237" title="Click to go to the Author Index">
             Arampatzis, Efstratios
            </a>
            (Noosware B.V),
            <a href="ROMAN24_AuthorIndexWeb.html#128192" title="Click to go to the Author Index">
             Rossi, Silvia
            </a>
            (Universita' Di Napoli Federico II)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab312" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study explores the complex dynamics between humans and robots, focusing on how emotional states influence proxemics. We conducted a user study using a standard mobile robot to investigate whether emotions elicited from a loudspeaker, affect human perception of robot proximity. Based on previous research on Human-Human Interaction (HHI), we analysed participants’ responses to robots displaying different behaviours. Participants observed the robot’s approach while experiencing positive or negative emotions. Our findings suggest that emotional states induced by external stimuli can affect participants’ perception of robot proximity. In detail, the results indicate that while comfortable stopping distances were unaffected by participants’ emotional state, individuals who experienced positive emotions judged the same proxemics distance used while performing an avoidance behaviour to be more acceptable compared to the case of negative emotions. This study describes the extent to which our emotions can alter the perception of robot behaviours, ultimately affecting our acceptance of these novel social agents.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt1_03">
             12:00-12:15, Paper TuBT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('317'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Too Close to You? a Study on Emotion-Adapted Proxemics Behaviours (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#235787" title="Click to go to the Author Index">
             Vigni, Francesco
            </a>
            (Interdepartmental Center for Advances in Robotic Surgery - ICARO),
            <a href="ROMAN24_AuthorIndexWeb.html#400243" title="Click to go to the Author Index">
             Maglietta, Dimitri
            </a>
            (University of Naples Federico II),
            <a href="ROMAN24_AuthorIndexWeb.html#128192" title="Click to go to the Author Index">
             Rossi, Silvia
            </a>
            (Universita' Di Napoli Federico II)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab317" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motivations_and_emotions_in_robotics" title="Click to go to the Keyword Index">
               Motivations and Emotions in Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The development of social robots with advanced conversational capabilities aiming to engage humans in natural interactions has recently surged. This paper investigates the dynamic aspect of Human-Robot Interaction (HRI), focusing on the regulation of interpersonal distance based on human emotion. Through a user study with a humanoid robot, we explore how participants perceive and respond to rule-based versus randomly generated robot behaviours in adjusting interpersonal space during an unconstrained conversation. Results suggest that participants perceive the robot using rule-based behaviours as more socially competent and adaptable to human behaviour and emotions compared to the random ones. These findings highlight the importance of considering subtle nonverbal cues and adapting robot behaviour based on human emotions to improve the quality of HRI, and consequently facilitate the successful integration of human natural nuances in robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt1_04">
             12:15-12:30, Paper TuBT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             LEMMA: Learning Language-Conditioned Multi-Robot Manipulation
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#281518" title="Click to go to the Author Index">
             Gong, Ran
            </a>
            (UCLA),
            <a href="ROMAN24_AuthorIndexWeb.html#204428" title="Click to go to the Author Index">
             Gao, Xiaofeng
            </a>
            (Amazon),
            <a href="ROMAN24_AuthorIndexWeb.html#321637" title="Click to go to the Author Index">
             Gao, Qiaozi
            </a>
            (Amazon),
            <a href="ROMAN24_AuthorIndexWeb.html#361207" title="Click to go to the Author Index">
             M Shakiah, Suhaila
            </a>
            (Amazon),
            <a href="ROMAN24_AuthorIndexWeb.html#321673" title="Click to go to the Author Index">
             Thattai, Govind
            </a>
            (Amazon),
            <a href="ROMAN24_AuthorIndexWeb.html#101856" title="Click to go to the Author Index">
             Sukhatme, Gaurav
            </a>
            (University of Southern California)
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tubt2">
             <b>
              TuBT2
             </b>
            </a>
           </td>
           <td class="r">
            Room T2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tubt2" title="Click to go to the Program at a Glance">
             <b>
              Virtual and Augmented Tele-Presence Environments II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt2_01">
             11:30-11:45, Paper TuBT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('406'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A B-Spline Approach for Improved Environmental Awareness in Virtual Walking System Using Avatar Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#392217" title="Click to go to the Author Index">
             Miuccio, Alessandra
            </a>
            (Politecnico Di Milano),
            <a href="ROMAN24_AuthorIndexWeb.html#382430" title="Click to go to the Author Index">
             Manríquez-Cisterna, Ricardo
            </a>
            (Tohoku University),
            <a href="ROMAN24_AuthorIndexWeb.html#190763" title="Click to go to the Author Index">
             Ravankar, Ankit A.
            </a>
            (Tohoku University),
            <a href="ROMAN24_AuthorIndexWeb.html#194967" title="Click to go to the Author Index">
             Salazar Luces, Jose Victorio
            </a>
            (Tohoku University),
            <a href="ROMAN24_AuthorIndexWeb.html#100909" title="Click to go to the Author Index">
             Hirata, Yasuhisa
            </a>
            (Tohoku University),
            <a href="ROMAN24_AuthorIndexWeb.html#10019" title="Click to go to the Author Index">
             Rocco, Paolo
            </a>
            (Politecnico Di Milano)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab406" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advancements in medical science and technology have led to a remarkable increase in the lifespan of the global elderly population. However, this demographic often struggles with mobility issues, leading to a sedentary lifestyle fueled by concerns over their physical capabilities. Traditional treadmill gait training, although beneficial, often becomes monotonous and lacks the real-world feedback necessary for engaging and effective rehabilitation. Addressing this gap, research into virtual walking systems utilizing avatar robots has gained traction. Despite the progress, several challenges remain where the systems prioritize visual feedback without considering the crucial need for alerting users to potential dangers and obstacles. This lack of comprehensive environmental awareness and feedback undermines both user engagement and safety. To address this problem, this paper proposes an algorithm that employs B-splines for precise free space detection, integrated with a safety stop mechanism for an avatar robot. This novel approach enhances user awareness of their surroundings through a sophisticated Graphic User Interface (GUI) that leverages Augmented Reality (AR) technology. By superimposing free space boundaries and warning messages directly onto a real-time camera feed, the system provides an intuitive and immersive navigation aid. The efficacy of our proposed GUI was rigorously tested across a series of realistic scenarios, comparing teleoperation control performance with and without the augmented interface. Our findings reveal that our GUI markedly enhances user safety and navigational effectiveness, fostering a deeper understanding of and interaction with the surrounding environment, thereby redefining user experience in virtual mobility assistance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt2_02">
             11:45-12:00, Paper TuBT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('439'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              UnRealTHASC – a Cyber-Physical XR Testbed for Underwater Real-Time Human Autonomous Systems Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#340442" title="Click to go to the Author Index">
             Surve, Sushrut
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#342895" title="Click to go to the Author Index">
             Guo, Jia
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#400242" title="Click to go to the Author Index">
             Menezes, Jovan
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#400254" title="Click to go to the Author Index">
             Tate, Connor
            </a>
            (University of West Florida, Institute for Human and Machine Cogn),
            <a href="ROMAN24_AuthorIndexWeb.html#400238" title="Click to go to the Author Index">
             Jin, Yiting
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#400359" title="Click to go to the Author Index">
             Walker, Justin
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#132416" title="Click to go to the Author Index">
             Ferrari, Silvia
            </a>
            (Cornell University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab439" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Research on underwater human-robot teaming is particularly promising because of complementary sensory skills that may overcome unique challenges associated with the combination of limited connectivity and low visibility. Nevertheless, testing human-robot collaboration under water, especially in complex, real-world scenarios, poses severe safety, cost, and time constraints that significantly hinder academic research in this space. This paper presents a novel cyber-physical extended-reality (XR) testbed, referred to as Underwater Real-Time Human Autonomous Systems Collaboration (UnRealTHASC), designed to enable human-robot interactions in simulated photorealistic environments with mixed real and virtual wearables, and advanced autonomous underwater sensors in-the-loop. Novel sensor interfaces are designed to integrate real and virtual sensors for measuring physiological and cognitive human states underpinning decision-making abilities. Physics-based human and robot motion models are developed along with new sensor simulations in order to capture the couplings between underwater behaviors and perception based on measurements from optical and sonar sensors. Real-time data acquisition pipelines are created to access and share data from both real and virtual sensors and robots, such that new methods for online planning and collaboration may be tested via human-in-the-loop demonstrations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt2_03">
             12:00-12:15, Paper TuBT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('333'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Human(s) on the Loop Demand Aware Robot Scheduling: A Mixed Reality Based User Study
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#344266" title="Click to go to the Author Index">
             SANDULA, AJAY KUMAR
            </a>
            (Indian Institute of Science, Bengaluru),
            <a href="ROMAN24_AuthorIndexWeb.html#379374" title="Click to go to the Author Index">
             M, Rajatsurya
            </a>
            (Indian Institute of Science, Bangalore),
            <a href="ROMAN24_AuthorIndexWeb.html#118356" title="Click to go to the Author Index">
             Ghose, Debasish
            </a>
            (Indian Institute of Science),
            <a href="ROMAN24_AuthorIndexWeb.html#307862" title="Click to go to the Author Index">
             Biswas, PRADIPTA
            </a>
            (Indian Institute of Science)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab333" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Scheduling tasks and subtasks for multiple heterogeneous robots poses a significant challenge, particularly in scenarios where human supervision is essential. The complexity is further compounded when considering human factors in the scheduling process, especially in demand-aware task scheduling environments(tasks are generated based on external demand). The primary goal of this research is to understand human decision-making and how it affects their scheduling process, and we undertook two studies. In the first study, we employed a mixed reality based user study to explore how human perception of the scheduling environment influences task scheduling and facilitates personalized resource allocation. Our findings indicate that human task schedulers exhibit enhanced performance when assisted by autonomous agents, compared to scenarios with limited autonomy in robotic systems. To explore the impact of robot planning on human decision-making and its effects on task scheduling and collision-aware scheduling capabilities, we conducted the second study. This study employed a mixed reality-based warehouse environment, where two users controlled different robots with shared objectives. The findings suggest that human operators exhibited improved collision-aware scheduling without compromising their demand-aware scheduling capabilities when visual aids such as collision cones were incorporated.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt2_04">
             12:15-12:30, Paper TuBT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('336'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Immersive Teleoperation of Collaborative Robot for Remote Vital Monitoring in Healthcare
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#344270" title="Click to go to the Author Index">
             Vishwakarma, Himanshu
            </a>
            (Indian Institute of Science),
            <a href="ROMAN24_AuthorIndexWeb.html#400066" title="Click to go to the Author Index">
             Sampath, Ananyan
            </a>
            (All India Institute of Medical Sciences, Bhopal),
            <a href="ROMAN24_AuthorIndexWeb.html#307751" title="Click to go to the Author Index">
             Vinay Krishna Sharma, Vinay Krishna
            </a>
            (Indian Institute of Science),
            <a href="ROMAN24_AuthorIndexWeb.html#307862" title="Click to go to the Author Index">
             Biswas, PRADIPTA
            </a>
            (Indian Institute of Science)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab336" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Healthcare robotics has emerged as an innovative system with vast potential across various domains. This paper explores the development and application of collaborative robots for teleoperation, which facilitate safe and remote medical general examinations of patients. The study aims to help medical staff to provide vital sign check-ups remotely using legacy medical devices. We proposed an immersive media tool that was integrated with augmented reality (AR) and virtual reality (VR) interfaces to enhance the reach of medical professionals for vital signs check-ups. The medical cobot was teleoperated through AR and VR interfaces. A qualitative user study was used to compare the AR and VR interface systems within the loop of healthcare professionals.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tubt3">
             <b>
              TuBT3
             </b>
            </a>
           </td>
           <td class="r">
            Room T3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tubt3" title="Click to go to the Program at a Glance">
             <b>
              Social Touch in Human–Robot Interaction
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt3_01">
             11:30-11:45, Paper TuBT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('309'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Human-Robot Handshaking Preferences for Quadruped Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400137" title="Click to go to the Author Index">
             Chappuis, Alessandra
            </a>
            (Ecole Polytechnique Fédérale De Lausanne),
            <a href="ROMAN24_AuthorIndexWeb.html#216565" title="Click to go to the Author Index">
             Bellegarda, Guillaume
            </a>
            (EPFL),
            <a href="ROMAN24_AuthorIndexWeb.html#105018" title="Click to go to the Author Index">
             Ijspeert, Auke
            </a>
            (EPFL)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab309" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Quadruped robots are showing impressive abilities to navigate the real world. If they are to become more integrated into society, social trust in interactions with humans will become increasingly important. Additionally, robots will need to be adaptable to different humans based on individual preferences. In this work, we study the social interaction task of learning optimal handshakes for quadruped robots based on user preferences. While maintaining balance on three legs, we parameterize handshakes with a Central Pattern Generator consisting of an amplitude, frequency, stiffness, and duration. Through 10 binary choices between handshakes, we learn a belief model to fit individual preferences for 25 different subjects. Our results show that this is an effective strategy, with 76% of users feeling happy with their identified optimal handshake parameters, and 20% feeling neutral. Moreover, compared with random and test handshakes, the optimized handshakes have significantly decreased errors in amplitude and frequency, lower Dynamic Time Warping scores, and improved energy efficiency, all of which indicate robot synchronization to the user's preferences
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt3_02">
             11:45-12:00, Paper TuBT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('395'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Unified Understanding of Environment, Task, and Human for Human-Robot Interaction in Real-World Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400087" title="Click to go to the Author Index">
             Yano, Yuga
            </a>
            (Kyushu Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400098" title="Click to go to the Author Index">
             Mizutani, Akinobu
            </a>
            (Kyushu Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400134" title="Click to go to the Author Index">
             Fukuda, Yukiya
            </a>
            (Kyushu Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400133" title="Click to go to the Author Index">
             Kanaoka, Daiju
            </a>
            (Kyushu Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#371149" title="Click to go to the Author Index">
             Ono, Tomohiro
            </a>
            (Toyota Motor Corporation),
            <a href="ROMAN24_AuthorIndexWeb.html#179275" title="Click to go to the Author Index">
             Tamukoh, Hakaru
            </a>
            (Kyushu Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab395" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To facilitate human-robot interaction (HRI) tasks in real-world scenarios, service robots must adapt to dynamic environments and understand the required tasks while effectively communicating with humans. To accomplish HRI in practice, we propose a novel indoor dynamic map, task understanding system, and response generation system. The indoor dynamic map optimizes robot behavior by managing an occupancy grid map and dynamic information, such as furniture and humans, in separate layers. The task understanding system targets tasks that require multiple actions, such as serving ordered items. Task representations that predefine the flow of necessary actions are applied to achieve highly accurate understanding. The response generation system is executed in parallel with task understanding to facilitate smooth HRI by informing humans of the subsequent actions of the robot. In this study, we focused on waiter duties in a restaurant setting as a representative application of HRI in a dynamic environment. We developed an HRI system that could perform tasks such as serving food and cleaning up while communicating with customers. In experiments conducted in a simulated restaurant environment, the proposed HRI system successfully communicated with customers and served ordered food with 90% accuracy. In a questionnaire administered after the experiment, the HRI system of the robot received 4.2 points out of 5. These outcomes indicated the effectiveness of the proposed method and HRI system in executing waiter tasks in real-world environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt3_03">
             12:00-12:15, Paper TuBT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('153'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Soothing Sensations: Enhancing Interactions with a Socially Assistive Robot through Vibrotactile Heartbeats
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#329267" title="Click to go to the Author Index">
             Borgstedt, Jacqueline
            </a>
            (University of Glasgow),
            <a href="ROMAN24_AuthorIndexWeb.html#399741" title="Click to go to the Author Index">
             Macdonald, Shaun
            </a>
            (University of Glasgow),
            <a href="ROMAN24_AuthorIndexWeb.html#399750" title="Click to go to the Author Index">
             Marky, Karola
            </a>
            (Ruhr-Universität Bochum),
            <a href="ROMAN24_AuthorIndexWeb.html#280616" title="Click to go to the Author Index">
             Pollick, Frank
            </a>
            (University of Glasgow),
            <a href="ROMAN24_AuthorIndexWeb.html#124017" title="Click to go to the Author Index">
             Brewster, Stephen
            </a>
            (University of Glasgow)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab153" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Physical interactions with socially assistive robots (SARs) positively affect user wellbeing. However, haptic experiences when touching a SAR are typically limited to perceiving the robot’s movements or shell texture, while other modalities that could enhance the touch experience with the robot, such as vibrotactile stimulation, are under-explored. In this exploratory qualitative study, we investigate the potential of enhancing human interaction with the PARO robot through vibrotactile heartbeats, with the goal to regulate subjective wellbeing during stressful situations. We conducted in-depth one-on-one interviews with 30~participants, who watched three horror movie clips alone, with PARO, and with a PARO that displayed a vibrotactile heartbeat. Our findings show that PARO’s presence and its interactive capabilities can help users regulate emotions through attentional redeployment from a stressor toward the robot. The vibrotactile heartbeat further reinforced PARO’s physical and social presence, enhancing the socio-emotional support provided by the robot and its perceived life-likeness. We discuss the impact of individual differences in user experience and implications for the future design of life-like vibrotactile stimulation for SARs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt3_04">
             12:15-12:30, Paper TuBT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('176'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Impact of Social Inter-Robot Encounters on User Perception
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#303225" title="Click to go to the Author Index">
             Nikolovska, Kristina
            </a>
            (Constructor University),
            <a href="ROMAN24_AuthorIndexWeb.html#360997" title="Click to go to the Author Index">
             Pohl, Jan
            </a>
            (Technische Universität Dresden),
            <a href="ROMAN24_AuthorIndexWeb.html#360993" title="Click to go to the Author Index">
             Hommel, Bernhard
            </a>
            (Shandong Normal University, Jinan),
            <a href="ROMAN24_AuthorIndexWeb.html#187914" title="Click to go to the Author Index">
             Kappas, Arvid
            </a>
            (Constructor University),
            <a href="ROMAN24_AuthorIndexWeb.html#123717" title="Click to go to the Author Index">
             Maurelli, Francesco
            </a>
            (Constructor University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab176" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, there has been a growing trend of integrating robots into various dimensions of daily life, to improve user experiences and offer a range of services. This study explores the interactions of robots showing social behavior towards other robots and their impact on human perceptions. We focus on three types of social behavior: social sensitivity, attention-sharing, and helping, aiming to understand how these interactions affect human perceptions of robots. Utilizing Duckiebot mobile robots in carefully crafted experimental setups, participants observed video recordings of these robots' interactions, which either included or excluded each targeted social behavior. The study measured user responses using established scales such as the Mind Attribution Scale (MAS), the Goodspeed Scale, and the Robotic Social Attributes Scale (RoSAS). The results demonstrated that robots displaying social behavior towards other robots were perceived more positively compared to those that did not exhibit such behavior. Specifically, social sensitivity positively impacted animacy, experience, likability, perceived intelligence, safety, and warmth. Attention-sharing improved perceptions of competence, experience, likability, perceived intelligence, and warmth. Additionally, helping behavior positively affected agency, animacy, anthropomorphism, competence, experience, likability, perceived intelligence, safety, and warmth. This research contributes valuable insights into Human-Robot Interaction (HRI), highlighting the significant impact of robots' interactions with each other on user experiences and perceptions. The exploration of social behavior lays a foundation for designing robots that evoke positive responses, fostering smoother integration of robotic technology into various
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tubt4">
             <b>
              TuBT4
             </b>
            </a>
           </td>
           <td class="r">
            Room T4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tubt4" title="Click to go to the Program at a Glance">
             <b>
              Human Factors and Ergonomics II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt4_01">
             11:30-11:45, Paper TuBT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('126'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Evaluating Human-Robot Interaction User Experiences in Manufacturing: An Initial Assessment Framework
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#327389" title="Click to go to the Author Index">
             tong, yanzhang
            </a>
            (Cardiff University),
            <a href="ROMAN24_AuthorIndexWeb.html#358749" title="Click to go to the Author Index">
             Zhang, Qiyuan
            </a>
            (Cardiff University),
            <a href="ROMAN24_AuthorIndexWeb.html#138920" title="Click to go to the Author Index">
             Ji, Ze
            </a>
            (Cardiff University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab126" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the manufacturing sector, enhancing user experiences (UX) in Human-Robot Interaction (HRI) and Human-robot collaboration (HRC) are becoming increasingly essential. This study is dedicated to the development of an initial assessment framework specifically designed for evaluating HRI UX in manufacturing. By qualitative semi-structured interviews, we focus on identifying key factors that constitutes UX in manufacturing context. The core contribution of this research is the development of a UX assessment framework tailored for evaluating HRI in manufacturing, include 5 facets of UX. This framework is derived from in-depth analysis of user feedback, providing a structured approach to understanding user interactions with robots. Our findings contribute valuable insights to the HRI discourse in manufacturing.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt4_02">
             11:45-12:00, Paper TuBT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('200'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sequential Transfer Learning-Based Human Decision Making Model for Human-Robot Co-Learning and Insights from User Feedback Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#382441" title="Click to go to the Author Index">
             Kumar, Rajul
            </a>
            (George Mason University),
            <a href="ROMAN24_AuthorIndexWeb.html#399863" title="Click to go to the Author Index">
             Som, Vaidehi
            </a>
            (University of Pennsylvania),
            <a href="ROMAN24_AuthorIndexWeb.html#186935" title="Click to go to the Author Index">
             Yao, Ningshi
            </a>
            (George Mason University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab200" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For robots to transform work processes and seamlessly integrate into human settings, such as warehouses and assistive roles, they must adeptly manage a wide range of repetitive and complex interactions with various humans. Despite great advances in developing robots capable of modeling and predicting human actions for smooth collaboration, a gap persists on how these prediction models can account for the impact of robot actions on human behavior. In this paper, we introduced a human-to-human, sequential transfer learning-based, data-driven model for human decision-making in co-learning scenarios, where humans and robots mutually learn and observe from each other. We model the problem as a sequence prediction task, employing one-dimensional convolutional neural network (or 1D CNN) and dual-input bidirectional gated recurrent units (or Bi-GRU) models learned on interaction-rich, closed-loop human-robot co-learning scenarios. Our model inherently captures the dynamic irrational strategies humans develop in response to their robot counterparts. Additionally, the transfer learning approach improved the generalizability of our model, enabling it to accommodate diverse human behaviors, ideologies, and personal beliefs with reasonable accuracy. Using user feedback, we presented an in-depth analysis of some of the key factors considered by participants during interactions, which can further help in refining prediction models.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt4_03">
             12:00-12:15, Paper TuBT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('243'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Advanced Robots in Healthcare and Their Impact on the Health and Safety of Medical Workers
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#295204" title="Click to go to the Author Index">
             Heinold, Eva
            </a>
            (Federal Institute for Occupational Safety and Health (BAuA)),
            <a href="ROMAN24_AuthorIndexWeb.html#205290" title="Click to go to the Author Index">
             Rosen, Patricia Helen
            </a>
            (Federal Institute for Occupational Safety and Health),
            <a href="ROMAN24_AuthorIndexWeb.html#201912" title="Click to go to the Author Index">
             Wischniewski, Sascha
            </a>
            (Federal Institute for Occupational Safety and Health (BAuA))
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab243" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#medical_and_surgical_applications" title="Click to go to the Keyword Index">
               Medical and Surgical Applications
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This article reviews recent work on advanced robotic systems in the medical field, with a focus on implications for occupational safety and health.	 The Human Health and Social Activities sector is currently the research field with the highest activity when it comes to the topic of human-robot interaction. The technology holds the potential to impact occupational safety and health for medical workers. Based on an extensive literature review we categorize and analyze the current research activity in the healthcare sector. This includes a mapping of the type of robotic system used, what tasks are automated, how tasks and jobs are transformed and the possible OSH impact occurs. We identify trends and gaps in research and derive recommendations for the implementation of robotic system in the sector. A total of 13 relevant meta-analysis and systematic reviews were selected and analyzed. The results show a strong focus on the automation of person related tasks via socially assistive robots. Secondly, robotic systems for rehabilitation and mobility are highly present in the sample. The OSH impacts are predominantly psychosocial or organizational. However, there are significant research gaps when it comes ergonomic impact on the medical personal. Our results can be used to guide new research and to design and implement robotic systems with a human centered approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt4_04">
             12:15-12:30, Paper TuBT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('52'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Interaction of Robot Speed and Distance in Human-Robot Collaboration: Impact on Human Trust, Safety, and Comfort
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#360341" title="Click to go to the Author Index">
             Büttner, Sebastian Thomas
            </a>
            (University of Duisburg-Essen),
            <a href="ROMAN24_AuthorIndexWeb.html#360330" title="Click to go to the Author Index">
             Alhaji, Basel
            </a>
            (Duisburg-Essen University),
            <a href="ROMAN24_AuthorIndexWeb.html#360345" title="Click to go to the Author Index">
             Katariya, Kaushikbhai
            </a>
            (Clausthal University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#360369" title="Click to go to the Author Index">
             Prilla, Michael
            </a>
            (University of Duisburg-Essen)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab52" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots are gradually being freed from their safety fences due to the advances in safety features integrated with most new robots. These robots enable a new form of human-robot interaction in which contact is possible. There are two robot-related factors that play a decisive role in the interaction dynamics and human perception in such a case – the motion speed and distance the robot keeps away from the human. Literature indicates that these factors influence whether humans perceive trust, safety, and comfort, which are essential components in the acceptance of robots by their end users. However, although speed and distance were intensively investigated in isolation, little is known about their combined effect. To this end, we conducted an experiment investigating the impact of the industrial robot arm’s approaching speed and stopping distance on the users’ trust, safety, and comfort while they were performing a collaborative task. Our results provide interesting insights into the implications of speed and trust. While we were able to replicate former studies in terms of implications of speed and distance, our data does not show any interaction effect between the two variables. However, our participants’ observations indicated that distance impacts the dependent variables more severely than speed.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tubt5">
             <b>
              TuBT5
             </b>
            </a>
           </td>
           <td class="r">
            Room T5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tubt5" title="Click to go to the Program at a Glance">
             <b>
              Machine Learning and Adaptation II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt5_01">
             11:30-11:45, Paper TuBT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('145'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fast LiDAR Upsampling Using Conditional Diffusion Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399695" title="Click to go to the Author Index">
             Helgesen, Sander
            </a>
            (University of Oslo),
            <a href="ROMAN24_AuthorIndexWeb.html#202611" title="Click to go to the Author Index">
             Nakashima, Kazuto
            </a>
            (Kyushu University),
            <a href="ROMAN24_AuthorIndexWeb.html#192025" title="Click to go to the Author Index">
             Torresen, Jim
            </a>
            (University of Oslo),
            <a href="ROMAN24_AuthorIndexWeb.html#103231" title="Click to go to the Author Index">
             Kurazume, Ryo
            </a>
            (Kyushu University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab145" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The search for refining 3D LiDAR data has attracted growing interest motivated by recent techniques such as supervised learning or generative model-based methods. Existing approaches have shown the possibilities for using diffusion models to generate refined LiDAR data with high fidelity, although the performance and speed of such methods have been limited. These limitations make it difficult to execute in real-time, causing the approaches to struggle in real-world tasks such as autonomous navigation and human-robot interaction. In this work, we introduce a novel approach based on conditional diffusion models for fast and high-quality sparse-to-dense upsampling of 3D scene point clouds through an image representation. Our method employs denoising diffusion probabilistic models trained with conditional inpainting masks, which have been shown to give high performance on image completion tasks. We introduce a series of experiments, including multiple datasets, sampling steps, and conditional masks. This paper illustrates that our method outperforms the baselines in sampling speed and quality on upsampling tasks using the KITTI-360 dataset. Furthermore, we illustrate the generalization ability of our approach by simultaneously training on real-world and synthetic datasets, introducing variance in quality and environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt5_02">
             11:45-12:00, Paper TuBT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('108'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              D3D: Conditional Diffusion Model for Decision-Making under Random Frame Dropping
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#212803" title="Click to go to the Author Index">
             Xia, Bo
            </a>
            (Tsinghua Shenzhen International Graduate School),
            <a href="ROMAN24_AuthorIndexWeb.html#379535" title="Click to go to the Author Index">
             Luo, yifu
            </a>
            (Tsinghua University),
            <a href="ROMAN24_AuthorIndexWeb.html#351574" title="Click to go to the Author Index">
             Chang, Yongzhe
            </a>
            (Tsinghua Shenzhen International Graduate School),
            <a href="ROMAN24_AuthorIndexWeb.html#198738" title="Click to go to the Author Index">
             Yuan, Bo
            </a>
            (Tsinghua University),
            <a href="ROMAN24_AuthorIndexWeb.html#208883" title="Click to go to the Author Index">
             Li, Zhiheng
            </a>
            (Tsinghua University),
            <a href="ROMAN24_AuthorIndexWeb.html#241746" title="Click to go to the Author Index">
             WANG, xueqian
            </a>
            (Center for Artificial Intelligence and Robotics, Graduate School)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab108" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The occurrence of frame drops due to issues such as corrupted communications or malfunctioning sensors presents a significant challenge to an agent's decision-making, especially in remote control scenarios. Classical reinforcement learning (RL) usually assumes a continuous data stream without frame drops and relies heavily on online interactions, which is time-consuming, resource-intensive, and often impractical in certain scenarios. Consequently, the performance of RL may deteriorate significantly in face of non-negligible frame drops. To tackle this challenge caused by frame dropping, We propose Conditional underline{bm{text{D}}}iffusion Model for underline{bm{text{D}}}ecision-Making under Random Frame underline{bm{text{D}}}ropping (D3D), an offline algorithm that can effectively enhance performance robustness in frame dropping scenarios. D3D addresses this issue through a two-phase approach: 1) During the policy generation phase, D3D adopts a return-conditional decision diffusion model rather than the temporal difference learning, whose policy is derived using offline datasets of return-labeled trajectories without information loss. 2) When frame dropping occurs during evaluation, D3D seamlessly substitutes the missing state with its corresponding prediction in the horizon made by the diffusion model. Extensive experiments are conducted on MuJoCo and Adroit tasks to validate D3D’s robustness and efficiency. The results demonstrate that D3D consistently outperforms state-of-the-art RL algorithms, especially excelling on tasks featuring severe drop rates.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt5_03">
             12:00-12:15, Paper TuBT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('433'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Deep Learning-Based Grasp Pose Estimation Approach for Large-Size Deformable Objects in Clutter
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#318629" title="Click to go to the Author Index">
             YU, Minghao
            </a>
            (The Chinese University of Hong Kong),
            <a href="ROMAN24_AuthorIndexWeb.html#358680" title="Click to go to the Author Index">
             Li, Zhuo
            </a>
            (The Chinese University of Hong Kong),
            <a href="ROMAN24_AuthorIndexWeb.html#287720" title="Click to go to the Author Index">
             Li, Zhihao
            </a>
            (The Chinese University of Hong Kong),
            <a href="ROMAN24_AuthorIndexWeb.html#273261" title="Click to go to the Author Index">
             Liu, Junjia
            </a>
            (The Chinese University of Hong Kong),
            <a href="ROMAN24_AuthorIndexWeb.html#281102" title="Click to go to the Author Index">
             Teng, Tao
            </a>
            (The Chinese University of Hong Kong &amp; Hong Kong Centre for Logis),
            <a href="ROMAN24_AuthorIndexWeb.html#132532" title="Click to go to the Author Index">
             Chen, Fei
            </a>
            (The Chinese University of Hong Kong)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab433" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deformable objects especially large-size deformable objects grasping is unappreciated but widespread in industrial applications (e.g., clothes recycling). While it encounters several challenges, for example, the existing methods didn't take large-size deformable objects into account, no typical boundary of deformable objects. To solve the challenges, we proposed a grasp detection framework consisting of a self-trained object detection network, an instance segmentation module, and a grasp pose generation pipeline. The experiments were successfully conducted on the industrial laundry mock-up with an 88.9% success ratio. The experiments result indicates the effectiveness of the proposed framework on spatial-constrained large-size deformable objects grasping in clutter.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt5_04">
             12:15-12:30, Paper TuBT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('463'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Secure and Efficient Face Recognition Via Supervised Federated Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400370" title="Click to go to the Author Index">
             Solomon, Enoch
            </a>
            (Virginia State University),
            <a href="ROMAN24_AuthorIndexWeb.html#407667" title="Click to go to the Author Index">
             Zewoudie, Abraham Woubie
            </a>
            (Silo AI),
            <a href="ROMAN24_AuthorIndexWeb.html#328427" title="Click to go to the Author Index">
             Abdelzaher, Ahmed
            </a>
            (Virginia State University),
            <a href="ROMAN24_AuthorIndexWeb.html#408249" title="Click to go to the Author Index">
             Emiru, Eyael Solomon
            </a>
            (University of Trento)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab463" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The state-of-the-art face recognition systems frequently rely on training with large image dataset acquired from several users on a single device. However, these databases usually contain sensitive information, which raises substantial privacy concerns. To address this issue, we propose the use of federated learning for a supervised face recognition. Federated learning enables model training while maintaining privacy by leveraging decentralized edge devices. In our method, each edge device trains a model individually, which is subsequently sent to a secure aggregator. We use GAN to introduce different data without sending actual data. The aggregator combines various models into a global one, which is subsequently disseminated to the edge devices. Experiments with CelebA datasets show that federated learning effectively protects privacy while maintaining performance levels.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tubt6">
             <b>
              TuBT6
             </b>
            </a>
           </td>
           <td class="r">
            Room T6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tubt6" title="Click to go to the Program at a Glance">
             <b>
              Motion Planning and Navigation in Human-Centered Environments II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt6_01">
             11:30-11:45, Paper TuBT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('165'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Age and Spatial Cue Effects on User Performance for an Adaptable Verbal Wayfinding System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399488" title="Click to go to the Author Index">
             Takahira, Rin
            </a>
            (Kobe University),
            <a href="ROMAN24_AuthorIndexWeb.html#131744" title="Click to go to the Author Index">
             Liu, Chaoran
            </a>
            (Riken),
            <a href="ROMAN24_AuthorIndexWeb.html#109330" title="Click to go to the Author Index">
             Ishi, Carlos Toshinori
            </a>
            (RIKEN),
            <a href="ROMAN24_AuthorIndexWeb.html#193078" title="Click to go to the Author Index">
             Ohkawa, Takenao
            </a>
            (Kobe University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab165" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#linguistic_communication_and_dialogue" title="Click to go to the Keyword Index">
               Linguistic Communication and Dialogue
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study aims on developing an interactive verbal wayfinding system that can adapt directions based on the user's context. Rather than simply providing the shortest route like in existing navigation systems, the developed system searches for routes with fewer turns and generates easy-to-understand verbal explanations. A key feature is prioritizing the inclusion of landmark references at potential points of confusion to provide appropriate contextual cues. Base functions for route searching and verbal direction generation were implemented in this study. A subjective experiment was then conducted to evaluate how user age and the number of provided landmark references impact wayfinding performance when using the system's verbal directions. Results revealed an age-dependent interaction: increasing landmarks did not universally improve wayfinding success. The optimal number of landmarks varied across age groups. The findings highlight the need to adapt the level of spatial cue details in verbal directions based on the user's context.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt6_02">
             11:45-12:00, Paper TuBT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('190'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EnQuery: Ensemble Policies for Diverse Query-Generation in Preference Alignment of Robot Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#307784" title="Click to go to the Author Index">
             de Heuvel, Jorge
            </a>
            (University of Bonn),
            <a href="ROMAN24_AuthorIndexWeb.html#399611" title="Click to go to the Author Index">
             Seiler, Florian
            </a>
            (Rheinische Friedrich-Wilhelms-Universität Bonn),
            <a href="ROMAN24_AuthorIndexWeb.html#113220" title="Click to go to the Author Index">
             Bennewitz, Maren
            </a>
            (University of Bonn)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab190" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To align mobile robot navigation policies with user preferences through reinforcement learning from human feedback (RLHF), reliable and behavior-diverse user queries are required. However, deterministic policies fail to generate a variety of navigation trajectory suggestions for a given navigation task. In this paper, we introduce EnQuery, a query generation approach using an ensemble of policies that achieve behavioral diversity through a regularization term. For a given navigation task, EnQuery produces multiple navigation trajectory suggestions, thereby optimizing the efficiency of preference data collection with fewer queries. Our methodology demonstrates superior performance in aligning navigation policies with user preferences in low-query regimes, offering enhanced policy convergence from sparse preference queries. The evaluation is complemented with a novel explainability representation, capturing full scene navigation behavior of the mobile robot in a single plot. Our code is available online at https://github.com/hrl-bonn/EnQuery.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt6_03">
             12:00-12:15, Paper TuBT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('173'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              YoloTag: Vision-Based Robust UAV Navigation with Fiducial Markers
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399351" title="Click to go to the Author Index">
             Raxit, Sourav
            </a>
            (University of New Orleans),
            <a href="ROMAN24_AuthorIndexWeb.html#399597" title="Click to go to the Author Index">
             Singh, Simant Bahadur
            </a>
            (The University of New Orleans),
            <a href="ROMAN24_AuthorIndexWeb.html#163828" title="Click to go to the Author Index">
             Redwan Newaz, Abdullah Al
            </a>
            (University of New Orleans)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab173" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             By harnessing fiducial markers as visual landmarks in the environment, Unmanned Aerial Vehicles (UAVs) can rapidly build precise maps and navigate spaces safely and efficiently, unlocking their potential for fluent collaboration and coexistence with humans. Existing fiducial marker methods rely on handcrafted feature extraction, which sacrifices accuracy. On the other hand, deep learning pipelines for marker detection fail to meet real-time runtime constraints crucial for navigation applications. In this work, we propose YoloTag: a real-time fiducial marker-based localization system. YoloTag uses a lightweight YOLO v8 object detector to accurately detect fiducial markers in images while meeting the runtime constraints needed for navigation. The detected markers are then used by an efficient perspective-n-point algorithm to estimate UAV states. However, this localization system introduces noise, causing instability in trajectory tracking. To suppress noise, we design a higher-order Butterworth filter that effectively eliminates noise through frequency domain analysis. We evaluate our algorithm through real-robot experiments in an indoor environment, comparing the trajectory tracking performance of our method against other approaches in terms of several distance metrics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tubt6_04">
             12:15-12:30, Paper TuBT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('83'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Text-Based Human Search and Approach Using a Robot Dog
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#312084" title="Click to go to the Author Index">
             Park, Jeongeun
            </a>
            (Korea University),
            <a href="ROMAN24_AuthorIndexWeb.html#281288" title="Click to go to the Author Index">
             Silveira, Jefferson
            </a>
            (Queen's University),
            <a href="ROMAN24_AuthorIndexWeb.html#172093" title="Click to go to the Author Index">
             Pan, Matthew
            </a>
            (Queen's University),
            <a href="ROMAN24_AuthorIndexWeb.html#163618" title="Click to go to the Author Index">
             Choi, Sungjoon
            </a>
            (Korea University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab83" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose a SOCratic model for Robots Approaching humans based on TExt System (SOCRATES) focusing on the human search and approach based on free-form textual description; the robot first searches for the target user, then the robot proceeds to approach in a human-friendly manner. We present a Socratic human search model that connects large pre-trained foundation models to solve the downstream task of searching for the target person based on textual descriptions. In particular, textual descriptions used for searching are composed of appearance (e.g., wearing a white shirt with black hair) and location clues (e.g., a student that works with robots). Additionally, we propose a hybrid learning-based framework for generating human-friendly robotic motion to approach a person, consisting of a learning-from-demonstration module and a knowledge distillation module utilizing LLMs. We evaluate the search performance of the proposed method in both simulation and real-world environments using the Boston Dynamics Spot robot. Moreover, we evaluate the effectiveness of our proposed framework through analysis involving human participants and investigate the perceived warmth of our system.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tulu_br">
             <b>
              TuLU_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tulu_br" title="Click to go to the Program at a Glance">
             <b>
              Boxed Lunch
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tukn">
             <b>
              TuKN
             </b>
            </a>
           </td>
           <td class="r">
            Room T9 - Ballroom
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tukn" title="Click to go to the Program at a Glance">
             <b>
              Keynote Talk - Shelly Levy-Tzedek
             </b>
            </a>
           </td>
           <td class="r">
            Keynote Session
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tupm1_br">
             <b>
              TuPM1_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tupm1_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break - III
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuct1">
             <b>
              TuCT1
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuct1" title="Click to go to the Program at a Glance">
             <b>
              Bridging Trust and Context: Dynamic Interactions in HAI I
             </b>
            </a>
           </td>
           <td class="r">
            Special Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct1_01">
             14:40-16:10, Paper TuCT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('86'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Associative Games with Virtual Agents: Unexpectedness Increases Willingness to Continue Dialog (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#194674" title="Click to go to the Author Index">
             Matsui, Tetsuya
            </a>
            (Osaka Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab86" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#interaction_with_believable_characters" title="Click to go to the Keyword Index">
               Interaction with Believable Characters
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the field of human-agent interaction, increasing agents' trustworthiness, inelligence perceived and users' motivation to continue interacting are important issues. Prior research has attempted to solve this problem by having "agents meet user expectations". In this paper, we attempted to show that, on the contrary, "agents contradicting user expectations" can increase agents' trustworthiness and users' motivation to continue dialogue. In this study, we conducted an experiment using an "association game" task. In this task, participants were given three hints and answered what those hints have in common. The correct answer is then given, along with a fourth hint. In the first condition, the "expected" condition, the correct answer was the one that the majority of participants associate with. In the second condition, the "unexpected" condition, the correct answer is different from the answer that the majority of participants associate with. After this task, participants answered the question about the extent to which they felt the agents' trustworthiness, intelligence and motivation to continue the interaction using a 7-point Likert scale. The experiment was conducted on the web. The results of the experiment showed that agents' trustworthiness and users' motivation to continue the interaction were significantly higher in the "unexpected" condition than in the "expected" condition. This result indicates that "the agent contradicts the user's expectations" can increase the agent's intelligence perceived and the user's motivation to continue the interaction. The results show the effectiveness of a new method in human-agent interaction that has not received much attention.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct1_02">
             14:40-16:10, Paper TuCT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('252'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Changing Human's Impression of Empathy from Agent by Verbalizing Agent's Position (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#281124" title="Click to go to the Author Index">
             Tsumura, Takahiro
            </a>
            (Toyo University),
            <a href="ROMAN24_AuthorIndexWeb.html#109985" title="Click to go to the Author Index">
             Yamada, Seiji
            </a>
            (National Institute of Informatics)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab252" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#anthropomorphic_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Anthropomorphic Robots and Virtual Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#embodiment__empathy_and_intersubjectivity" title="Click to go to the Keyword Index">
               Embodiment, Empathy and Intersubjectivity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As anthropomorphic agents (AI and robots) are increasingly used in society, empathy and trust between people and agents are becoming increasingly important. A better understanding of agents by people will help to improve the problems caused by the future use of agents in society. In the past, there has been a focus on the importance of self-disclosure and the relationship between agents and humans in their interactions. In this study, we focused on the attributes of self-disclosure and the relationship between agents and people. An experiment was conducted to investigate hypotheses on trust and empathy with agents through six attributes of self-disclosure (opinions and attitudes, hobbies, work, money, personality, and body) and through competitive and cooperative relationships before a robotic agent performs a joint task. The experiment consisted of two between-participant factors: six levels of self-disclosure attributes and two levels of relationship with the agent. The results showed that the two factors had no effect on trust in the agent, but there was statistical significance for the attribute of self-disclosure regarding a person's empathy toward the agent. In addition, statistical significance was found regarding the agent's ability to empathize with a person as perceived by the person only in the case where the type of relationship, competitive or cooperative, was presented. The results of this study could lead to an effective method for building relationships with agents, which are increasingly used in society.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct1_03">
             14:40-16:10, Paper TuCT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('51'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              User Decision Guidance with Selective Explanation Presentation from Explainable-AI (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#317572" title="Click to go to the Author Index">
             Fukuchi, Yosuke
            </a>
            (Tokyo Metropolitan University),
            <a href="ROMAN24_AuthorIndexWeb.html#109985" title="Click to go to the Author Index">
             Yamada, Seiji
            </a>
            (National Institute of Informatics)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab51" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the challenge of selecting explanations for XAI (Explainable AI)-based Intelligent Decision Support Systems (IDSSs). IDSSs have shown promise in improving user decisions through XAI-generated explanations along with AI predictions, and the development of XAI made it possible to generate a variety of such explanations. However, how IDSSs should select explanations to enhance user decision-making remains an open question. This paper proposes X-Selector, a method for selectively presenting XAI explanations. It enables IDSSs to strategically guide users to an AI-suggested decision by predicting the impact of different combinations of explanations on a user’s decision and selecting the combination that is expected to minimize the discrepancy between an AI suggestion and a user decision. We compared the efficacy of X-Selector with two naive strategies (all possible explanations and explanations only for the most likely prediction) and two baselines (no explanation and no AI support) in virtual stock-trading support scenarios. The results suggest the potential of X-Selector to guide users to AI-suggested decisions and improve task performance under the condition of a high AI accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct1_04">
             14:40-16:10, Paper TuCT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('230'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Empirical Investigation of How Robot Head Motion Influences Acceptance of Heatmap-Based XAI: Designing XAI with Social Robot (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#318690" title="Click to go to the Author Index">
             Maehigashi, Akihiro
            </a>
            (Shizuoka University),
            <a href="ROMAN24_AuthorIndexWeb.html#317572" title="Click to go to the Author Index">
             Fukuchi, Yosuke
            </a>
            (Tokyo Metropolitan University),
            <a href="ROMAN24_AuthorIndexWeb.html#109985" title="Click to go to the Author Index">
             Yamada, Seiji
            </a>
            (National Institute of Informatics)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab230" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study investigated how a robot head motion towards an AI attention heatmap during a visual identification task influences a human user’s trust in eXplainable AI (XAI). The findings revealed that the robot head motion presented in a video increased the user's acceptance of AI-generated results compared to the robot eye gaze displayed in a static image with or without the AI attention heatmap. However, displaying the heatmap improved task performance more than displaying no heatmap with or without the robot. Overall, these results suggest a possibility that showing a robot head motion towards an AI attention heatmap in a movie can serve as an interpretable XAI for visual tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct1_05">
             14:40-16:10, Paper TuCT1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('88'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Two Is Better Than One: Cultural Differences in the Number of Apologizing Robots in the U.S. and Japan (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#101690" title="Click to go to the Author Index">
             Shiomi, Masahiro
            </a>
            (ATR),
            <a href="ROMAN24_AuthorIndexWeb.html#398654" title="Click to go to the Author Index">
             Hirayama, Taichi
            </a>
            (Doshisha University),
            <a href="ROMAN24_AuthorIndexWeb.html#182286" title="Click to go to the Author Index">
             Kimoto, Mitsuhiko
            </a>
            (Meiji University),
            <a href="ROMAN24_AuthorIndexWeb.html#128282" title="Click to go to the Author Index">
             Iio, Takamasa
            </a>
            (Doshisha University),
            <a href="ROMAN24_AuthorIndexWeb.html#137057" title="Click to go to the Author Index">
             Shimohara, Katsunori
            </a>
            (Doshisha University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab88" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#interaction_with_believable_characters" title="Click to go to the Keyword Index">
               Interaction with Believable Characters
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Apology behavior design is becoming important for social robots that work in daily environments because of their widespread use. Robotics researchers have reported that multiple robots can effectively achieve more acceptance and trust in apology situations. Unfortunately, such effects have only been confirmed in a single country: Japan. Some studies investigated cultural differences in apologies between Japan and other cultures and reported how the former influences the perceived function and meaning of apologies. Therefore, we conducted a web-based survey to investigate whether using multiple robots in apology situations is effective in another country. We compared such perceived feelings as forgiveness and trust toward a robot’s apologies between Japan and the U.S. by using the visual stimuli of one and two robots. The experiment results showed that U.S. people felt that multiple robot apologies are more acceptable than apologies from just one robot, similar to results with Japanese participants. Perceived trust did show a different phenomenon between the two countries.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct1_06">
             14:40-16:10, Paper TuCT1.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('94'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Measuring Algorithm Aversion and Betrayal Aversion to Humans and AI Using Trust Games (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#396497" title="Click to go to the Author Index">
             Takagi, Hisashi
            </a>
            (BeNEXT Solutions Inc),
            <a href="ROMAN24_AuthorIndexWeb.html#396891" title="Click to go to the Author Index">
             Li, Yang
            </a>
            (Nagoya University),
            <a href="ROMAN24_AuthorIndexWeb.html#396849" title="Click to go to the Author Index">
             Komori, Masashi
            </a>
            (Osaka Electro-Communication University),
            <a href="ROMAN24_AuthorIndexWeb.html#110729" title="Click to go to the Author Index">
             Terada, Kazunori
            </a>
            (Gifu University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab94" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#philosophical_issues_in_human_robot_coexistence" title="Click to go to the Keyword Index">
               Philosophical Issues in Human-Robot Coexistence
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Behavioral trust is to entrust resources to the trustee, expecting a high return while accepting the risk of betrayal. Previous studies have demonstrated bias on behavioral trust, that trust is not necessarily rationally to the expected values, and is inconsistent to human vs. non-human counterparts. People on the one hand seem to be aversive to intentional betrayal (Betrayal Aversion), but on the other hand, people avoid being dependent on algorithms rather than human (Algorithm Aversion). Yet, these aversions are not comprehensively investigated. The present study conducted a well-controlled behavioral game experiment that systematically explored the entrusting (risk-taking) behavior when facing a counterpart (human vs. AI) or a natural risk, and further explored the effect of the counterpart's computational ability (intentional for human and algorithmic for AI). Participants (n=284) played a trust game with (a) a human with intentional decisions, (b) an AI with algorithmic decisions, (c) a human with random decisions, or (d) an AI with random decisions whose probability of return was known, as well as a lottery task structurally equivalent to the trust game. Entrusting decisions to different levels of probability of return were measured to compute the Minimum Acceptable Probability (MAP) as a quantitative measure of trust. The results showed that participants were more trusting of a counterpart than a lottery machine, yet this tendency did not differ by counterparts (i.e., human vs. AI counterparts), or between high computational ability or lower (i.e., intentional/algorithmic decision vs. random decisions). The results suggest an overtrust bias, rather than an aversive bias to a counterpart with agency, whether they are AIs or humans, regardless of their intentions.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuct2">
             <b>
              TuCT2
             </b>
            </a>
           </td>
           <td class="r">
            Room T2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuct2" title="Click to go to the Program at a Glance">
             <b>
              Assistive Robotics I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct2_01">
             14:40-16:10, Paper TuCT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('295'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Control in Assistive Application - a Study Evaluating Shared Control by Users with Limited Upper Limb Mobility
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#241840" title="Click to go to the Author Index">
             Goldau, Felix Ferdinand
            </a>
            (DFKI GmbH),
            <a href="ROMAN24_AuthorIndexWeb.html#281606" title="Click to go to the Author Index">
             Pascher, Max
            </a>
            (TU Dortmund University),
            <a href="ROMAN24_AuthorIndexWeb.html#350868" title="Click to go to the Author Index">
             Baumeister, Annalies
            </a>
            (Frankfurt University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#353068" title="Click to go to the Author Index">
             Tolle, Patrizia
            </a>
            (Deutsch),
            <a href="ROMAN24_AuthorIndexWeb.html#281601" title="Click to go to the Author Index">
             Gerken, Jens
            </a>
            (TU Dortmund University),
            <a href="ROMAN24_AuthorIndexWeb.html#101852" title="Click to go to the Author Index">
             Frese, Udo
            </a>
            (Universität Bremen)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab295" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Shared control in assistive robotics blends human autonomy with computer assistance, thus simplifying complex tasks for individuals with physical impairments. This study assesses an adaptive Degrees of Freedom control method specifically tailored for individuals with upper limb impairments. It employs a between-subjects analysis with 24 participants, conducting 81 trials across three distinct input devices in a realistic everyday-task setting. Given the diverse capabilities of the vulnerable target demographic and the known challenges in statistical comparisons due to individual differences, the study focuses primarily on subjective qualitative data. The results reveal consistently high success rates in trial completions, irrespective of the input device used. Participants appreciated their involvement in the research process, displayed a positive outlook, and quick adaptability to the control system. Notably, each participant effectively managed the given task within a short time frame.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct2_02">
             14:40-16:10, Paper TuCT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('207'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Do Mistakes Matter? Comparing Trust Responses of Different Age Groups to Errors Made by Physically Assistive Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#349236" title="Click to go to the Author Index">
             Wald, Sasha
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#310157" title="Click to go to the Author Index">
             Puthuveetil, Kavya
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#191362" title="Click to go to the Author Index">
             Erickson, Zackory
            </a>
            (Carnegie Mellon University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab207" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robotic_etiquette" title="Click to go to the Keyword Index">
               Robotic Etiquette
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Trust is a key factor in ensuring acceptable human-robot interaction, especially in settings where robots may be assisting with critical activities of daily living. When practically deployed, robots are bound to make occasional mistakes, yet the degree to which these errors will impact a care recipient's trust in the robot, especially in performing physically assistive tasks, remains an open question. To investigate this, we conducted experiments where participants interacted with physically assistive robots which would occasionally make intentional mistakes while performing two different tasks: bathing and feeding. Our study considered the error response of two populations: younger adults at a university (median age 26) and older adults at an independent living facility (median age 83). We observed that the impact of errors on a users' trust in the robot depends on both their age and the task that the robot is performing. We also found that older adults tend to evaluate the robot on factors unrelated to the robot's performance, making their trust in the system more resilient to errors when compared to younger adults. Code and supplementary materials are available on our project webpage.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct2_03">
             14:40-16:10, Paper TuCT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('274'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Contrastive Learning for Body Gesture Detection During Adapted Physical Activity
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#357306" title="Click to go to the Author Index">
             Carlos Martinez, Juan
            </a>
            (Lisv Systems Engineering Laboratory of Versailles),
            <a href="ROMAN24_AuthorIndexWeb.html#400163" title="Click to go to the Author Index">
             Pennino, Federico
            </a>
            (Bologna University),
            <a href="ROMAN24_AuthorIndexWeb.html#400160" title="Click to go to the Author Index">
             Dubois, Cécile
            </a>
            (Versailles Engineering Systems Laboratory-LISV),
            <a href="ROMAN24_AuthorIndexWeb.html#119291" title="Click to go to the Author Index">
             Monacelli, Eric
            </a>
            (LISV, University of Versailles),
            <a href="ROMAN24_AuthorIndexWeb.html#400164" title="Click to go to the Author Index">
             Gabbrielli, Maurizio
            </a>
            (Bologna University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab274" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel approach to body gesture recognition for powered wheelchair users, leveraging inertial data from wrist-mounted sensors to facilitate movement and enhance autonomy in Adapted Physical Activity (APA). Gesture recognition technology interprets human gestures to allow non-direct communication with devices, enhancing human-machine interaction across various fields. APA fosters inclusion and well-being through tailored physical engagement. Our model not only identifies known gestures with high accuracy, as indicated by a mean Average Precision (mAP) score of 0.92 and a Recall@1 score of 0.983, but also demonstrates the ability to recognize gestures not included in the training set. This research contributes to the field of human-robot interaction by offering a more dynamic and inclusive form of interaction for individuals reliant on powered mobility aids.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct2_04">
             14:40-16:10, Paper TuCT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('352'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Weight Compensation in Assistive Upper-Limb Exoskeletons: An EMG Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400286" title="Click to go to the Author Index">
             Salman, Nada
            </a>
            (University of Paris-Saclay),
            <a href="ROMAN24_AuthorIndexWeb.html#115443" title="Click to go to the Author Index">
             Benali, Abderraouf
            </a>
            (Université De Versailles)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab352" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#medical_and_surgical_applications" title="Click to go to the Keyword Index">
               Medical and Surgical Applications
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Abstract— This study explores the integration of adaptive weight compensation within an upper-limb exoskeleton by analyzing muscle activity from electromyography (EMG) signals. We enabled the exoskeleton to translate user-applied forces into precise joint movements by implementing an admittance controller. Through our experimental protocol, we measured EMG signals from the biceps brachii muscle, focusing on changes in two time-domain features -RMS and variance- under varying conditions. Then, we analyzed these changes to evaluate the impact on the muscle when users were assisted in carrying a one-kilogram object, compared to when they were not assisted. Our initial findings indicate that adaptive weight compensation significantly reduces muscle activity and enhances stability during lifting tasks, suggesting potential for real-time EMG-based activation of weight compensation. This research highlights the importance of combining force and EMG data to develop adaptive control laws for assistive robotics, paving the way for personalized Human-Robot Interaction (HRI).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct2_05">
             14:40-16:10, Paper TuCT2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('419'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Smart Music Therapist 1.0: Rhythmic Auditory Stimulation Integrated Robotic Walker As a Therapeutic Companion for Gait Rehabilitation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#393698" title="Click to go to the Author Index">
             Thalahitiya Vithanage, Ranul Helitha Vithanage
            </a>
            (University of Moratuwa),
            <a href="ROMAN24_AuthorIndexWeb.html#399678" title="Click to go to the Author Index">
             Senaratne, Nissanka Arachchi Appuhamilage Nidula Ransika
            </a>
            (University of Moratuwa),
            <a href="ROMAN24_AuthorIndexWeb.html#399736" title="Click to go to the Author Index">
             Welangalle, Pesala Dulkith
            </a>
            (University of Moratuwa),
            <a href="ROMAN24_AuthorIndexWeb.html#126927" title="Click to go to the Author Index">
             Amarasinghe, Ranjith
            </a>
            (University of Moratuway),
            <a href="ROMAN24_AuthorIndexWeb.html#399679" title="Click to go to the Author Index">
             Jayathilaka, Wanasinghe Arachchige Dumith Madushanka
            </a>
            (Department of Mechanical Engineering, University of Moratuwa),
            <a href="ROMAN24_AuthorIndexWeb.html#400117" title="Click to go to the Author Index">
             Jayawardena, Madura
            </a>
            (University of Sri Jayewardenepura)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab419" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#medical_and_surgical_applications" title="Click to go to the Keyword Index">
               Medical and Surgical Applications
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study investigates the development of a robotic walker for gait rehabilitation that integrates social assistive robotics and principles of Rhythmic Auditory Stimulation (RAS), a form of music therapy. The robotic system is intended to supplement, rather than replace, the work of professional music therapists by enabling therapeutic interventions to continue outside of therapy sessions. Robot-facilitated Music Therapy is an evolutionary step of telerehabilitation, providing patients with greater autonomy while still benefiting from the expertise and guidance of therapists. Contributions of the research include 1) Robot-facilitated Music Therapy through a robotic walker as a platform for gait rehabilitation. 2) Remote Music Therapy through a robotic walker as a telerehabilitation platform for music therapists to remotely monitor and consult with patients. 3) Intelligent companionship integrated with a robotic walker for comprehensive user assistance. The proposed system, tested for its functionalities including personified RAS, social interaction, user monitoring, mobility control, and emergency response yielded promising results for real world application.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct2_06">
             14:40-16:10, Paper TuCT2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('445'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Analysis of Hesitation Behavior by Human Receiver Toward Facing Motion of Robot During Handover
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#394273" title="Click to go to the Author Index">
             Goto, Ryoya
            </a>
            (University of Tsukuba),
            <a href="ROMAN24_AuthorIndexWeb.html#155691" title="Click to go to the Author Index">
             Hassan, Modar
            </a>
            (University of Tsukuba),
            <a href="ROMAN24_AuthorIndexWeb.html#107465" title="Click to go to the Author Index">
             Suzuki, Kenji
            </a>
            (University of Tsukuba)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab445" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper examines whether the robot face-turning to a human during handover can be a behavior cue that supports the human action plan in accomplishing a task. A series of behavior cues observed in interactions, such as Joint action, are understood as social signals, which can be used as cues to help each other make decisions about their own action plans. We conducted temporal analysis and subjective evaluation of the receiver's behavior in an object handover experiment. The results showed that the subjective experience did not significantly change when the robot turned its head toward the receiver, but the time to receive the participant significantly increased. This suggests that the robot's face-turning behavior may have altered the receiver's human prediction and action plan, causing the receiver to hesitate to respond. These findings lead to the basis for constructing a mathematical model of empathic behavior that allows humans and robots to share the intention of action goals in the real world.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuct3">
             <b>
              TuCT3
             </b>
            </a>
           </td>
           <td class="r">
            Room T3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuct3" title="Click to go to the Program at a Glance">
             <b>
              HRI and Collaboration in Manufacturing Environments I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct3_01">
             14:40-16:10, Paper TuCT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('259'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Intuitive Force Feedback for Robot-Assisted Load Transport
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400004" title="Click to go to the Author Index">
             Celik, Cankan
            </a>
            (Middle East Technical University),
            <a href="ROMAN24_AuthorIndexWeb.html#128137" title="Click to go to the Author Index">
             Ankarali, Mustafa Mert
            </a>
            (Middle East Technical University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab259" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#innovative_robot_designs" title="Click to go to the Keyword Index">
               Innovative Robot Designs
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a force-guided robotic platform that aims to reduce musculoskeletal injuries in manufacturing and healthcare environments. The platform utilizes force feedback algorithms, system modeling with distinct force and virtual torque processing, and virtual dynamics for seamless control of heavy loads. Similar mobile robot platforms in previous studies were designed for rehabilitation purposes. This study focuses on heavy-duty applications using a cost-effective load cell configuration on a differential drive system. We propose a novel model that simplifies the human-robot interaction analysis by decomposing pushing forces into linear and rotational components represented by a combination of linear and rotational spring-damper systems. These elements are integrated into the dynamics of the mobile robot platform for a comprehensive representation of human-robot interaction. Additionally, we developed a new control method to counteract large force fluctuations and anti-integral windup with dynamic clamping to improve system robustness.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct3_02">
             14:40-16:10, Paper TuCT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('204'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Methodology on the Cyber-Physical System Construction for a User-Friendly Smart Clothing Manufacturing Robot System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#390926" title="Click to go to the Author Index">
             Kang, Bonggu
            </a>
            (Korea Institute of Industrial Technology(KITECH)),
            <a href="ROMAN24_AuthorIndexWeb.html#407397" title="Click to go to the Author Index">
             Park, Hong-Sun
            </a>
            (Korea Institute of Industrial Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#348625" title="Click to go to the Author Index">
             Lee, Jinmyeong
            </a>
            (Korea Institute of Industrial Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#399897" title="Click to go to the Author Index">
             YUN, JUNGMIN
            </a>
            (Korea Institute of Industrial Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab204" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#computational_architectures" title="Click to go to the Keyword Index">
               Computational Architectures
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In clothing manufacturing, automated equipment systems capable of customized small-scale production using robots are receiving much attention. To flexibly produce various types of clothing using such a system, a monitoring and control environment that is friendly to user decision-making is required. Nevertheless, existing equipment systems do not provide such an environment because they are focused on the operation itself, and there are time and financial limitations in establishing a monitoring and control platform environment in equipment systems intended for small-quantity production. This paper verified data collection, simulation-based analysis, and control on an actual clothing production micro-factory system for verifying the operation behavior. Experimentally, we performed the operation verification of the proposed CPS methodology. We confirmed that control of existing scenarios and simulation-based analysis and control of new scenarios are possible. We expect that the proposed method will provide a user-friendly decision-making environment for existing manufacturing facilities and improve the efficiency of the production line.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct3_03">
             14:40-16:10, Paper TuCT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('294'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring Continuous Awareness Modelling for Improving Worker Safety and Trust
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#345663" title="Click to go to the Author Index">
             Solovov, Andrey
            </a>
            (University of Coimbra),
            <a href="ROMAN24_AuthorIndexWeb.html#400198" title="Click to go to the Author Index">
             Zarei, Mohammad
            </a>
            (Instituto De Sistemas E Robótica - Universidade De Coimbra),
            <a href="ROMAN24_AuthorIndexWeb.html#342925" title="Click to go to the Author Index">
             Gautam, Vishal
            </a>
            (Institute of Systems and Robotics - University of Coimbra),
            <a href="ROMAN24_AuthorIndexWeb.html#270455" title="Click to go to the Author Index">
             Ferreira, Bruno
            </a>
            (Instituto De Sistemas E Robótica - Universidade De Coimbra),
            <a href="ROMAN24_AuthorIndexWeb.html#243194" title="Click to go to the Author Index">
             Assunção, Gustavo
            </a>
            (Institute of Systems and Robotics - University of Coimbra),
            <a href="ROMAN24_AuthorIndexWeb.html#102907" title="Click to go to the Author Index">
             Marín Hernández, Antonio
            </a>
            (Universidad Veracruzana),
            <a href="ROMAN24_AuthorIndexWeb.html#107662" title="Click to go to the Author Index">
             Menezes, Paulo
            </a>
            (Institute of Systems and Robotics)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab294" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The Industry 5.0 HRI goals have the potential for significantly reducing production costs, but require factory workers to trust that robots will not cause them physical harm when sharing the workspace in order to be achieved. To this end, we propose a model that adjusts the robot's actions to an estimate of the workers' awareness of its behavior. Its premise is that mutual awareness should be continuous, and if no recent visual contact has been established, then the robot should adapt its range of motion and speed in order to minimize the intersection between the robot's space and the worker's space. On the other hand, if the worker steadily establishes visual contact from times to times, the robot operates under a normal or a faster configuration. After a period of habituation, this awareness model can be further tweaked to continuously improve production times, while improving worker's safety and trust. We present preliminary results of a pilot study with 32 participants, to asses if the existence of this awareness model impacts the outcomes of the combined task. The obtained tepidly optimistic results are the key to build upon in future designs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct3_04">
             14:40-16:10, Paper TuCT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('189'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Robust Filter for Marker-Less Multi-Person Tracking in Human-Robot Interaction Scenarios
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#295086" title="Click to go to the Author Index">
             Martini, Enrico
            </a>
            (Università Di Verona),
            <a href="ROMAN24_AuthorIndexWeb.html#399837" title="Click to go to the Author Index">
             Parekh, Harshil
            </a>
            (University of Pennsylvania),
            <a href="ROMAN24_AuthorIndexWeb.html#360030" title="Click to go to the Author Index">
             Peng, Shaoting
            </a>
            (University of Pennsylvania),
            <a href="ROMAN24_AuthorIndexWeb.html#244803" title="Click to go to the Author Index">
             Bombieri, Nicola
            </a>
            (University of Verona),
            <a href="ROMAN24_AuthorIndexWeb.html#151054" title="Click to go to the Author Index">
             Figueroa, Nadia
            </a>
            (University of Pennsylvania)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab189" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Pursuing natural and marker-less human-robot interaction (HRI) has been a long-standing robotics research focus, driven by the vision of seamless collaboration without physical markers. Marker-less approaches promise an improved user experience, but state-of-the-art struggles with the challenges posed by intrinsic errors in human pose estimation (HPE) and depth cameras. These errors can lead to issues such as robot jittering, which can significantly impact the trust users have in collaborative systems. We propose a filtering pipeline that refines incomplete 3D human poses from an HPE backbone and a single RGB-D camera to address these challenges, solving for occlusions that can degrade the interaction. Experimental results show that using the proposed filter leads to more consistent and noise-free motion representation, reducing unexpected robot movements and enabling smoother interaction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct3_05">
             14:40-16:10, Paper TuCT3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('307'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Human Planning of Robot Actions through LLM-Guided State Machine Synthesis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400202" title="Click to go to the Author Index">
             Swick, Brennan
            </a>
            (The Ohio State University),
            <a href="ROMAN24_AuthorIndexWeb.html#400204" title="Click to go to the Author Index">
             Donegan, Sean
            </a>
            (Air Force Research Laboratory),
            <a href="ROMAN24_AuthorIndexWeb.html#294479" title="Click to go to the Author Index">
             Gillman, Andrew
            </a>
            (Air Force Research Laboratory),
            <a href="ROMAN24_AuthorIndexWeb.html#400221" title="Click to go to the Author Index">
             Groeber, Michael
            </a>
            (The Ohio State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab307" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Developments in natural language processing have made it easier than ever to program robots using language. Specifically, Large Language Models (LLMs) capture both language patterns and some embedded knowledge that enables emergent capabilities, such as planning. However, it is not well understood how to best use LLMs to reduce robotic programming effort. We present a system that aims to reduce the burden of robot programming without affecting the safety and observability of the system. This is achieved through a system that: 1) translates human plans to robotic plans via an LLM; 2) displays robotic plans in a state machine; and 3) demonstrates robotic plans in simulation. Our results show that the system can convert natural language to synthesized state machines that achieve goal specifications in 13/30 cases, with two sets of exemplar instructions producing 12 of the 13 state machines. Thus, given proper instructions, the programming burden can be reduced while maintaining safety and observability through verifiable discrete actions, state machine visualizations, and simulated demonstrations. These results suggest that the given instructions have a significant effect on the success of the plan and additional tools need to be added to support directability and corrective feedback from the human before a human-in-the-loop study for a manufacturing scenario should be performed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct3_06">
             14:40-16:10, Paper TuCT3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('264'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Hierarchical Gesture-Guided Framework for Multi-Manipulator Fabrication and Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#392821" title="Click to go to the Author Index">
             Wei, Jiaying
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#305828" title="Click to go to the Author Index">
             Bard, Joshua
            </a>
            (Carnegie Mellon University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab264" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the rapid development of human-robot collaboration in workspaces, the interest in human-in-the-loop design and fabrication is expanding. In this research, we introduce a modularized gesture-guided framework for multi-manipulator fabrication. Echoing human-centered HRI in robotic construction and design fabrication, we explored different hierarchies of gesture-instructed tasks in a fabrication setting that involves manipulating and operating on materials. This gesture-guided framework for multi-manipulator fabrication contains instruction types at different control levels, making both gesture sets and assigned instructions modular and easily accessible for diverse tasks. Based on the flexibility of the high-level instruction pipeline, we also implemented a preliminary reachability-guided solver specifically for multi-manipulators in fabrication scenarios, improving the success rate for mutually constrained multi-goal assignment in the baseline Multi-Robot System (MRS).
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuct4">
             <b>
              TuCT4
             </b>
            </a>
           </td>
           <td class="r">
            Room T4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuct4" title="Click to go to the Program at a Glance">
             <b>
              Non-Verbal Cues and Expressiveness I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct4_01">
             14:40-16:10, Paper TuCT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('82'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              What Kinds of Facial Self-Touches Strengthen Expressed Emotions?
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#396919" title="Click to go to the Author Index">
             Kawano, Junya
            </a>
            (Doshisha University),
            <a href="ROMAN24_AuthorIndexWeb.html#182286" title="Click to go to the Author Index">
             Kimoto, Mitsuhiko
            </a>
            (Meiji University),
            <a href="ROMAN24_AuthorIndexWeb.html#128282" title="Click to go to the Author Index">
             Iio, Takamasa
            </a>
            (Doshisha University),
            <a href="ROMAN24_AuthorIndexWeb.html#101690" title="Click to go to the Author Index">
             Shiomi, Masahiro
            </a>
            (ATR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab82" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#anthropomorphic_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Anthropomorphic Robots and Virtual Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As virtual spaces continue to attract more and more attention, the development of virtual agents is also gaining momentum. Agents capable of expressing emotions are being developed, and various methods exist through which they express such feelings. Although much research has focused on the implementation of human actions in agents, no research has yet implemented facial self-touch for agents used in emotional expressions, even though such transmission of human emotions is a critical tool for them. In this study, we developed a system that enriches agents' emotional expressions by implementing self-touches in virtual agents. From an evaluation of our system, we found that it enhances the emotions of virtual agents. The naturalness of their emotional expressions varied depending on the emotion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct4_02">
             14:40-16:10, Paper TuCT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('458'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Eyes on the Game: Deciphering Implicit Human Signals to Infer Human Proficiency, Trust, and Intent
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399773" title="Click to go to the Author Index">
             Hulle, Nikhil
            </a>
            (University of Colorado, Boulder),
            <a href="ROMAN24_AuthorIndexWeb.html#314338" title="Click to go to the Author Index">
             Aroca-Ouellette, Stephane
            </a>
            (University of Colorado Boulder),
            <a href="ROMAN24_AuthorIndexWeb.html#399823" title="Click to go to the Author Index">
             Ries, Anthony
            </a>
            (DEVCOM ARL),
            <a href="ROMAN24_AuthorIndexWeb.html#226180" title="Click to go to the Author Index">
             Brawer, Jake
            </a>
            (University of Colorado, Boulder),
            <a href="ROMAN24_AuthorIndexWeb.html#400240" title="Click to go to the Author Index">
             von der Wense, Katharina
            </a>
            (Johannes Gutenberg University Mainz),
            <a href="ROMAN24_AuthorIndexWeb.html#168176" title="Click to go to the Author Index">
             Roncone, Alessandro
            </a>
            (University of Colorado Boulder)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab458" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Effective collaboration between humans and AIs hinges on transparent communication and alignment of mental models. However, explicit, verbal communication is not always feasible in many scenarios. Under such circumstances, human-human teams often depend on implicit, nonverbal cues to glean important information about their teammates such as future intentions and expertise, thereby bolstering team alignment and adaptability. Among these cues a human’s actions in the environment and their visual attention are the most salient. In this paper, we propose a novel method to combine eye gaze data and behavioral data and evaluate their respective predictive power for human proficiency, trust, and intentions. We first collect a dataset of paired eye gaze and gameplay data in the fast-paced collaborative “Overcooked” environment. We then train variations of our models on this dataset to compare how the predictive powers differ between gaze data, gameplay data, and their combination. We additionally compare our method to prior works that aggregate eye gaze data and demonstrate how these aggregation methods can substantially reduce the predictive ability of eye gaze relative to our approach. Our results indicate that although there are certain situations where eye gaze data or gameplay data yield superior results, the model that integrates both types of data consistently outperforms all baselines. This work paves the way for developing intuitive and responsive agents that can efficiently adapt themselves to new and unseen teammates.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct4_03">
             14:40-16:10, Paper TuCT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('242'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Legibot: Generating Legible Motions for Service Robots Using Cost-Based Local Planners
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#287033" title="Click to go to the Author Index">
             Amirian, Javad
            </a>
            (Sorbonne University),
            <a href="ROMAN24_AuthorIndexWeb.html#359611" title="Click to go to the Author Index">
             Abrini, Mouad
            </a>
            (Sorbonne University),
            <a href="ROMAN24_AuthorIndexWeb.html#123191" title="Click to go to the Author Index">
             Chetouani, Mohamed
            </a>
            (Sorbonne University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab242" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the increasing presence of social robots in various environments and applications, there is an increasing need for these robots to exhibit socially-compliant behaviors. Legible motion, characterized by the ability of a robot to clearly and quickly convey intentions and goals to the individuals in its vicinity, through its motion, holds significant importance in this context. This will improve the overall user experience and acceptance of robots in human environments. In this paper, we introduce a novel approach to incorporate legibility into local motion planning for mobile robots. This can enable robots to generate legible motions in real-time and dynamic environments. To demonstrate the effectiveness of our proposed methodology, we also provide a robotic stack designed for deploying legibility-aware motion planning in a social robot, by integrating perception and localization components.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct4_04">
             14:40-16:10, Paper TuCT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('262'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Attentiveness: A Key Factor in Fostering Affective and Cognitive Trust with Non-Humanoid Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#320270" title="Click to go to the Author Index">
             Manor, Adi
            </a>
            (Reichman University),
            <a href="ROMAN24_AuthorIndexWeb.html#272485" title="Click to go to the Author Index">
             parush, avi
            </a>
            (Israel Institute of Technology - Technion),
            <a href="ROMAN24_AuthorIndexWeb.html#230312" title="Click to go to the Author Index">
             Erel, Hadas
            </a>
            (Media Innovation Lab, Interdisciplinary Center Herzliya)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab262" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motivations_and_emotions_in_robotics" title="Click to go to the Keyword Index">
               Motivations and Emotions in Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Affective trust and cognitive trust are fundamental elements in human-robot interactions. They impact robots’ acceptance, the level of engagement, and the tendency to rely on robots in various contexts. Opposite effects are observed when the interaction with a robot is unreliable and untrustworthy. In this study, we examined the possibility of enhancing both aspects of trust by manipulating the robot’s level of attentiveness to the participant. We focused on robotic attentiveness since it can be easily applied even to highly simple non-humanoid robots, positioning it as a method for enhancing trust for robots with different morphologies. Specifically, we evaluated whether minimal attentive robotic gestures can enhance affective and cognitive aspects of trust and whether inattentive robotic behavior can decrease them. Quantitative and qualitative results indicated that the robot’s attentiveness impacted both aspects of trust. Participants in the Attentive Robot condition reported higher affective and cognitive trust scores, smaller interpersonal distance, and a higher number of participants reported that the robot would ”be there for them”, in comparison to the Inattentive Robot and Baseline conditions. Our findings suggest that an attentive robotic behavior, can support human affective and cognitive trust and enhance human-robot interaction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct4_05">
             14:40-16:10, Paper TuCT4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('175'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Labeling Sentences with Symbolic and Deictic Gestures Via Semantic Similarity
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#361817" title="Click to go to the Author Index">
             Gjaci, Ariel
            </a>
            (University of Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#155018" title="Click to go to the Author Index">
             Recchiuto, Carmine Tommaso
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#108312" title="Click to go to the Author Index">
             Sgorbissa, Antonio
            </a>
            (University of Genova)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab175" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Co-speech gesture generation on artificial agents has gained attention recently, mainly when it is based on data-driven models. However, end-to-end methods often fail to generate co-speech gestures related to semantics with specific forms, i.e., Symbolic and Deictic gestures. In this work, we identify which words in a sentence are contextually related to Symbolic and Deictic gestures. Firstly, we appropriately chose 12 gestures recognized by people from the Italian culture, which different humanoid robots can reproduce. Then, we implemented two rule-based algorithms to label sentences with Symbolic and Deictic gestures. The rules depend on the semantic similarity scores computed with the RoBerta model between sentences that heuristically represent gestures and sub-sentences inside an objective sentence that artificial agents have to pronounce. We also implemented a baseline algorithm that assigns gestures without computing similarity scores. Finally, to validate the results, we asked 30 persons to label a set of sentences with Deictic and Symbolic gestures through a Graphical User Interface (GUI), and we compared the labels with the ones produced by our algorithms. For this scope, we computed Average Precision (AP) and Intersection Over Union (IOU) scores, and we evaluated the Average Computational Time (ACT). Our results show that semantic similarity scores are useful for finding Symbolic and Deictic gestures in utterances.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct4_06">
             14:40-16:10, Paper TuCT4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('47'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Two Is Better Than One: Explanations by Multiple Robots Strengthen the Feeling of Kawaii Toward Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#392934" title="Click to go to the Author Index">
             Kimura, Yuki
            </a>
            (Nara Women's University),
            <a href="ROMAN24_AuthorIndexWeb.html#266512" title="Click to go to the Author Index">
             Anzai, Emi
            </a>
            (Nara Women's University),
            <a href="ROMAN24_AuthorIndexWeb.html#202204" title="Click to go to the Author Index">
             Saiwaki, Naoki
            </a>
            (Nara Women's University),
            <a href="ROMAN24_AuthorIndexWeb.html#101690" title="Click to go to the Author Index">
             Shiomi, Masahiro
            </a>
            (ATR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab47" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Using multiple robots in presentation effectively attracts and provides information to people compared to a single robot. Many robotics researchers employed this approach to realize effective robot-presentation systems in real environments. These systems implicitly equalize the amount of speech in conversational content between multiple robots; however, it is not known whether that rule is actually important. For example, the effects of the second robot that provides only a back-channel have not yet been investigated. If we unveil the relationships between the presentation effects and the amount of speech in multiple robot presentations, it is crucial in designing the content of multiple robots' conversations. For this purpose, we conducted a video-based web-survey. Our finding showed that product recommendations by multiple robots make them more positive, and a part of the advantages of equalizing the amount of speech compared to biasing the amount of speech in presentation.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuct5">
             <b>
              TuCT5
             </b>
            </a>
           </td>
           <td class="r">
            Room T5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuct5" title="Click to go to the Program at a Glance">
             <b>
              Computational Architectures and Cognitive Systems
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct5_01">
             14:40-16:10, Paper TuCT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('473'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Strategies for Moment Compensation in Supernumerary Robotic Limbs Manipulation Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#356649" title="Click to go to the Author Index">
             Moon, Chaerim
            </a>
            (University of Illinois, Urbana-Champaign),
            <a href="ROMAN24_AuthorIndexWeb.html#150747" title="Click to go to the Author Index">
             Kim, Joohyung
            </a>
            (University of Illinois at Urbana-Champaign)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab473" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_and_sensorimotor_development" title="Click to go to the Keyword Index">
               Cognitive and Sensorimotor Development
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When designing the motions of wearable robotic systems, physical Human-Robot Interaction (pHRI) is a crucial consideration. This study suggests a coordinated motion planner for Supernumerary Robotic Limbs (SRLs) to minimize the applied moment from the robotic system’s operation to the human body. Given task motion trajectories, moment compensation motions are generated to reduce the asymmetric moment elements while maintaining the other element below a nominal value. The performance of the motion planner is evaluated by the simulation and the hardware experiment. An object pick-and-place scenario is used as a goal task. The simulation results demonstrate that under different conditions, the compensation motions significantly decrease the generated roll and yaw moments when compared to the results without the motions. The hardware experiment validates its practicality as a real-time motion controller of a physical robotic system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct5_02">
             14:40-16:10, Paper TuCT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('185'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi and Binary Task Classification for Real and Imagined Motor Movements Using EEG Signals
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#375535" title="Click to go to the Author Index">
             Cheng, Yibo
            </a>
            (Princeton International School of Mathematics and Science),
            <a href="ROMAN24_AuthorIndexWeb.html#375531" title="Click to go to the Author Index">
             Yadav, Nikhil
            </a>
            (St. John's University, Queens, New York)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab185" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_and_sensorimotor_development" title="Click to go to the Keyword Index">
               Cognitive and Sensorimotor Development
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vascular dysfunction, trauma, and other disorders often cause a loss in mobility. Brain-computer-aided prosthetics have the potential to help restore critical motor functions and carry out normal daily activities. Through brain-computer interfaces (BCI), electroencephalogram (EEG) brain signals can be used to generate prosthetic gesture commands and control external devices. However, due to the complexity posed by noise in EEG signals, it can be difficult to extract useful features and classify a wide range of motor tasks. In this study, an EEG Motor Movement and Imagery Dataset collected by the New York State Department of Health using a BCI2000 system, is used to perform detailed motor task classification. This dataset includes both real and imagined motions across six different tasks. Convolutional neural networks (CNN) are built to classify six tasks, ultimately leading to binary classification between real and imagined movements, together with motion and rest. The latter models achieve accuracies of 91.2% and 74.4%, respectively. Our approach has the potential for broader applications where comprehensive classifications are needed to discern multiple prosthetic movement tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct5_03">
             14:40-16:10, Paper TuCT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('144'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              What Would I Do If...? Promoting Understanding in HRI through Real-Time Explanations in the Wild
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#342302" title="Click to go to the Author Index">
             Love, Tamlin
            </a>
            (Institut De Robòtica I Informàtica Industrial CSIC-UPC),
            <a href="ROMAN24_AuthorIndexWeb.html#214124" title="Click to go to the Author Index">
             Andriella, Antonio
            </a>
            (Artificial Intelligence Research Institute, CSIC),
            <a href="ROMAN24_AuthorIndexWeb.html#103995" title="Click to go to the Author Index">
             Alenyà, Guillem
            </a>
            (Institut De Robòtica I Informàtica Industrial CSIC-UPC)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab144" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As robots become more and more integrated in human spaces, it is increasingly important for them to be able to explain their decisions to the people they interact with. These explanations need to be generated automatically and in real-time in response to decisions taken in dynamic and often unstructured environments. However, most research in explainable human-robot interaction only considers explanations (often manually selected) presented in controlled environments. We present an explanation generation method based on counterfactuals and demonstrate its use in an ``in-the-wild" experiment using automatically generated and selected explanations of autonomous interactions with real people to assess the effect of these explanations on participants' ability to predict the robot's behaviour in hypothetical scenarios. Our results suggest that explanations aid one's ability to predict the robot's behaviour, but also that the addition of counterfactual statements may add some burden and counteract this beneficial effect.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct5_04">
             14:40-16:10, Paper TuCT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('430'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Environment-Aware Robotic Arm Reaching Based on a Bio-Inspired Neurodynamical Computational Framework
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#329106" title="Click to go to the Author Index">
             Chatziparaschis, Dimitrios
            </a>
            (UC Riverside),
            <a href="ROMAN24_AuthorIndexWeb.html#400428" title="Click to go to the Author Index">
             Zhong, Shan
            </a>
            (University of California, Riverside),
            <a href="ROMAN24_AuthorIndexWeb.html#106405" title="Click to go to the Author Index">
             Christopoulos, Vassilios
            </a>
            (California Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#151110" title="Click to go to the Author Index">
             Karydis, Konstantinos
            </a>
            (University of California, Riverside)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab430" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#computational_architectures" title="Click to go to the Keyword Index">
               Computational Architectures
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_and_sensorimotor_development" title="Click to go to the Keyword Index">
               Cognitive and Sensorimotor Development
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Bio-inspired robotic systems are capable of adaptive learning, scalable control, and efficient information processing. Enabling real-time decision-making for such systems is critical to respond to dynamic changes in the environment. We focus on dynamic target tracking in open areas using a robotic six-degree-of-freedom manipulator with a bird-eye view camera for visual feedback, and by deploying the Neurodynamical Computational Framework (NeuCF). NeuCF is a recently developed bio-inspired model for target tracking based on Dynamic Neural Fields (DNFs) and Stochastic Optimal Control (SOC) theory. It has been trained for reaching actions on a planar surface toward localized visual beacons, and it can re-target or generate stop signals on the fly based on changes in the environment (e.g., a new target has emerged, or an existing one has been removed). We evaluated our system over various target-reaching scenarios. In all experiments, NeuCF had high end-effector positional accuracy, generated smooth trajectories, and provided reduced path lengths compared with a baseline cubic polynomial trajectory generator. In all, the developed system offers a robust and dynamic-aware robotic manipulation approach that affords real-time decision-making.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct5_05">
             14:40-16:10, Paper TuCT5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('50'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Quantum Exploration-Based Reinforcement Learning for Efficient Robot Path Planning in Sparse-Reward Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#257152" title="Click to go to the Author Index">
             Huang, Chao
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#297423" title="Click to go to the Author Index">
             Guo, Yibei
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#373788" title="Click to go to the Author Index">
             Zhu, Zhihui
            </a>
            (Ohio State University),
            <a href="ROMAN24_AuthorIndexWeb.html#373807" title="Click to go to the Author Index">
             Si, Mei
            </a>
            (Rensselaer Polytechnic Institute),
            <a href="ROMAN24_AuthorIndexWeb.html#374619" title="Click to go to the Author Index">
             Blankenberg, Daniel
            </a>
            (Cleveland Clinic),
            <a href="ROMAN24_AuthorIndexWeb.html#238447" title="Click to go to the Author Index">
             Liu, Rui
            </a>
            (Kent State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab50" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#computational_architectures" title="Click to go to the Keyword Index">
               Computational Architectures
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the latest developments in sensors, battery, and Artificial Intelligence (AI) technologies, robots can perform missions in unstructured environments (e.g., disaster sites). However, their adaptation speed is still slow due to their limited onboard computing capability and dynamic disruption from environments, leading to inaccurate planning and delayed emergency reactions. Even though reinforcement learning increases robots' exploration speed by fusing effective guidance, the numerous interactions make the learning time-consuming and even risky (e.g., collisions). To fundamentally improve robot adaptation speed, this work seeks help from quantum power. A novel Quantum Exploration based Dreamer model (QED) was developed to facilitate reinforcement learning explorations. QED based on stochastic quantum walker quickly explores environments, evaluates action quality, and obtains a global optimal exploration strategy; then, these high-quality exploration samples will be used to facilitate reinforcement learning speed. A theoretical benefit of QED is facilitating reinforcement learning speed without changing the underlying learning architecture, which makes the proposed QED applicable to general robot learning scenarios. To validate QED effectiveness, a robot path-planning task in an obstacle-dense environment was designed. The number of needed training episodes validated the effectiveness. The results show that QED achieves around ten times faster policy learning compared with Monte Carlo tree-based reinforcement learning methods and vanilla reinforcement learning methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct5_06">
             14:40-16:10, Paper TuCT5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('304'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Open Access NAO (OAN): A ROS2-Based Software Framework for HRI Applications with the NAO Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#269808" title="Click to go to the Author Index">
             Bono, Antonio
            </a>
            (University of Calabria),
            <a href="ROMAN24_AuthorIndexWeb.html#375873" title="Click to go to the Author Index">
             Brameld, Kenji
            </a>
            (TRACLabs),
            <a href="ROMAN24_AuthorIndexWeb.html#166072" title="Click to go to the Author Index">
             D'Alfonso, Luigi
            </a>
            (Università Della Calabria),
            <a href="ROMAN24_AuthorIndexWeb.html#269807" title="Click to go to the Author Index">
             Fedele, Giuseppe
            </a>
            (University of Calabria)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab304" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#computational_architectures" title="Click to go to the Keyword Index">
               Computational Architectures
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a new software framework for HRI experimentation with the sixth version of the common NAO robot produced by the United Robotics Group. Embracing the common demand of researchers for better performance and new features for NAO, the authors took advantage of the ability to run ROS2 onboard on the NAO to develop a framework independent of the APIs provided by the manufacturer. Such a system provides NAO with not only the basic skills of a humanoid robot such as walking and reproducing movements of interest but also features often used in HRI such as: speech recognition/synthesis, face and object detention, and the use of Generative Pre-trained Transformer (GPT) models for conversation. The developed code is therefore configured as a ready-to-use but also highly expandable and improvable tool thanks to the possibilities provided by the ROS community. The code repository is: https://github.com/antbono/OAN
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tuct6">
             <b>
              TuCT6
             </b>
            </a>
           </td>
           <td class="r">
            Room T6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tuct6" title="Click to go to the Program at a Glance">
             <b>
              Creating Human-Robot Relationships I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct6_01">
             14:40-16:10, Paper TuCT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('378'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              How Do Starship Robots Affect Everyday Campus Life? an Exploratory Posting Board Analysis and Interview-Based Study
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#371655" title="Click to go to the Author Index">
             Schneider, Adeline
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#378779" title="Click to go to the Author Index">
             Robinson, Ayan
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#191086" title="Click to go to the Author Index">
             Grimm, Cindy
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#160571" title="Click to go to the Author Index">
             Fitter, Naomi T.
            </a>
            (Oregon State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab378" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#long_term_experience_and_longitudinal_hri_studies" title="Click to go to the Keyword Index">
               Long-term Experience and Longitudinal HRI Studies
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The rapid emergence of food delivery robots in public spaces has raised important questions regarding public perceptions and policy creation. One method for addressing these questions is examining the relationship between delivery robots and the communities they already serve. We assessed our university community's experiences with and perceptions of the Starship Technologies robots (robots that currently operate on campus) using two efforts: analysis of online posting board content related to the robots and interviews of campus community members about the robots. Perspectives captured in the online post analysis tended to be negative, while views tended to be positive in the interview results. At the same time, both results showed differing opinions and complexity; one tension that emerged in both efforts, for example, is the potential of the robots to both benefit and impede disability communities on campus. Further, there were fundamental misunderstandings about what data the robots can and do record. This research can help to inform roboticists and policymakers whose work relates to autonomous robots in public spaces.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct6_02">
             14:40-16:10, Paper TuCT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('391'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Social Identity Performance Facilitates Contextually-Driven Trust Calibration and Accurate Human Assessments of Robot Capabilities
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400140" title="Click to go to the Author Index">
             Stull, Maria
            </a>
            (University of Colorado Boulder),
            <a href="ROMAN24_AuthorIndexWeb.html#288276" title="Click to go to the Author Index">
             Lohrmann, Clare
            </a>
            (University of Colorado Boulder),
            <a href="ROMAN24_AuthorIndexWeb.html#156445" title="Click to go to the Author Index">
             Hayes, Bradley
            </a>
            (University of Colorado Boulder)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab391" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_presence_for_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Social Presence for Robots and Virtual Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             People struggle to form accurate expectations of robots because we typically associate behavior (and capability) with the physical entity even when there are clear indicators of different software programs dictating behavior at different times. This is a harmful prior, as commercially available, visibly similar robots do not necessarily share any common ground in terms of capability, safety, or behavior. Prior efforts to calibrate people’s expectations of robots have not extended to anchoring on the robot’s control software rather than its embodiment. In this work, we leverage social participation and flexible identity presentation to facilitate coworkers' associations of robot capability with the currently running software rather than physical entity itself. By linking each of a robot's controllers to a social identity, we enable collaborators to more easily differentiate between them. In a human subjects study (n=30), participants who experienced our social identity signal understood differences between the robot's two controllers and prevented an unreliable controller from harming perceptions of the robot's other controller.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct6_03">
             14:40-16:10, Paper TuCT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('416'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vid2RealHRI: Align Video-Based HRI Study Designs with Real-World Settings
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#389776" title="Click to go to the Author Index">
             Hauser, Elliott
            </a>
            (University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#389777" title="Click to go to the Author Index">
             Chan, Yao-Cheng
            </a>
            (University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#359129" title="Click to go to the Author Index">
             Modak, Sadanand
            </a>
            (The University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#127043" title="Click to go to the Author Index">
             Biswas, Joydeep
            </a>
            (University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#186300" title="Click to go to the Author Index">
             Hart, Justin
            </a>
            (University of Texas at Austin)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab416" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robotic_etiquette" title="Click to go to the Keyword Index">
               Robotic Etiquette
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             HRI research using autonomous robots in real-world settings can produce results with the highest ecological validity of any study modality, but many difficulties limit such studies’ feasibility and effectiveness. We propose VID2REAL HRI, a research framework to maximize real-world insights offered by video-based studies. The VID2REAL HRI framework was used to design an online study using first-person videos of robots as real-world encounter surrogates. The online study (n = 385) distinguished the within-subjects effects of four robot behavioral conditions on perceived social intelligence and human willingness to help the robot enter an exterior door. A real-world, between-subjects replication (n = 26) using two conditions confirmed the validity of the online study’s findings and the sufficiency of the participant recruitment target (22) based on a power analysis of online study results. The VID2REAL HRI framework offers HRI researchers a principled way to take advantage of the efficiency of video-based study modalities while generating directly transferable knowledge of real-world HRI. Code and data from the study are provided at vid2real.github.io/vid2realHRI.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct6_04">
             14:40-16:10, Paper TuCT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('90'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fairness-Sensitive Policy-Gradient Reinforcement Learning for Reducing Bias in Robotic Assistance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#364122" title="Click to go to the Author Index">
             Zhu, Jie
            </a>
            (George Washington University),
            <a href="ROMAN24_AuthorIndexWeb.html#364081" title="Click to go to the Author Index">
             Hu, Mengsha
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#364131" title="Click to go to the Author Index">
             Zhang, Amy
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#364080" title="Click to go to the Author Index">
             Jin, Ruoming
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#238447" title="Click to go to the Author Index">
             Liu, Rui
            </a>
            (Kent State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab90" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots assist humans in various activities, from daily living to collaborative manufacturing. Because they have biased learning sources (e.g., data, demonstrations, human feedback), robots inevitably have discriminatory performance regarding individual differences (e.g., skin color, mobility, appearance); discriminatory performance will undermine robots' service quality, causes request ignorance and response delay, and even cause emotional offenses. Therefore, mitigating biases is critically important for delivering fair robotic services. In this paper, we design a bias-mitigation method -- Fairness-Sensitive Policy Gradient Reinforcement Learning (FSPGRL), to help robots self-identify and correct biased behaviors. FSPGRL identifies bias by examining the abnormal updates along particular gradients and updates the policy network to provide fair decisions. To validate FSPGRL's effectiveness, we designed a human-centered service scenario: a robot serving people in a restaurant. With a user study involving 24 humans and 1,000 service demonstrations, FSPGRL has proven effective in maintaining fairness during robot services.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct6_05">
             14:40-16:10, Paper TuCT6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('438'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Exploratory Analysis of the Harmonious Bond between Home Robots and Their Owners in Japan
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#349249" title="Click to go to the Author Index">
             Ichikura, Aiko
            </a>
            (University of Tokyo),
            <a href="ROMAN24_AuthorIndexWeb.html#106350" title="Click to go to the Author Index">
             Okada, Kei
            </a>
            (The University of Tokyo),
            <a href="ROMAN24_AuthorIndexWeb.html#106348" title="Click to go to the Author Index">
             Inaba, Masayuki
            </a>
            (The University of Tokyo)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab438" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In Japan, home robots have become popular and are widely loved. In particular, Sony’s aibo users have formed a big community, and a society where humans and robots live in harmony is becoming a reality. In order for robots to be more accepted in the society, exploratory analysis and explicit description of their bonds are important. However, the scale for measuring relationships with robots has not yet been defined, so a suitable measure was created from the Friendship Function Scale to measure the relationship of friends and the Pet Awareness Scale to measure the attitude toward pets. Using the proposed scale, we investigated the role of home robots, subjective intimacy, and attitudes toward robots among 54 robot owners. The survey found that more than half of them considered robots to be pets or children. In addition, the average value of subjective intimacy was very high to indicate a very high level of attachment to the robot. Furthermore, as a result of factor analysis of the scale items, five factors were extracted: “Comfort,” “Irreplaceability,” “Trust from the robot,” “Trust to the robot,” and “Social life.” At this time, there was no bias in how users felt about comfort and irreplaceability, but there was a bias in how users felt about trust. Additionally, those with the highest level of subjective intimacy felt significantly more “trusted by the robot” than those with the other level of intimacy, suggesting that recognition of trust from the robot may be an important bond-forming strategy. In the future, we will aim to form further bonds by investigating relationships with robots that play various roles, as well as in regions other than Japan.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tuct6_06">
             14:40-16:10, Paper TuCT6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('258'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Action Over Words: Predicting Human Trust in AI Partners through Gameplay Behaviors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400128" title="Click to go to the Author Index">
             Jafari Meimandi, Kiana
            </a>
            (University of Virginia),
            <a href="ROMAN24_AuthorIndexWeb.html#400131" title="Click to go to the Author Index">
             Bolton, Matthew L.
            </a>
            (University of Virginia),
            <a href="ROMAN24_AuthorIndexWeb.html#400130" title="Click to go to the Author Index">
             Beling, Peter A.
            </a>
            (Virginia Tech)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab258" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the burgeoning field of human-AI interaction, trust emerges as a cornerstone because many think that it is critical to the effectiveness of collaboration and the acceptance of AI systems. Traditional methods of assessing trust have predominantly relied on self-reported measures, requiring participants to articulate their perceptions and attitudes through questionnaires. However, these explicit methods may not fully capture the nuanced dynamics of trust, especially in real-time and complex interaction environments. This paper introduces an innovative approach to evaluating trust in human-AI teams, pivoting from the conventional reliance on verbal or written feedback to analyzing gameplay behaviors as implicit indicators of trust levels. Utilizing the Overcooked-AI environment, our study explores how participants' interactions with AI agents of varying performance levels can reveal underlying trust mechanisms without a single query posed to the human players. This approach not only bypasses the efficiency challenges posed by repetitive and lengthy trust assessment methods, but also provides insights comparable to them. We highlight the potential of non-verbal cues and action patterns as reliable trust indicators by comparing the predictive accuracies of questionnaire-based models with those derived from gameplay behavior analysis. Furthermore, our findings suggest that these implicit measures can be integrated into adaptive systems and algorithms for real-time trust calibration in human-agent teaming settings. This shift towards an action-oriented trust assessment challenges existing paradigms and opens new avenues for understanding and enhancing human-AI collaboration.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tulbr">
             <b>
              TuLBR
             </b>
            </a>
           </td>
           <td class="r">
            Room T10 - Poster Room
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tulbr" title="Click to go to the Program at a Glance">
             <b>
              Late Breaking Reports Session I
             </b>
            </a>
           </td>
           <td class="r">
            Interactive Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_01">
             16:10-17:10, Paper TuLBR.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('497'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Co-Design of a Robot with Incarcerated Youth
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#173021" title="Click to go to the Author Index">
             Alves-Oliveira, Patrícia
            </a>
            (Amazon Lab126),
            <a href="ROMAN24_AuthorIndexWeb.html#408037" title="Click to go to the Author Index">
             Cusworth Walker, Sarah
            </a>
            (University of Washington),
            <a href="ROMAN24_AuthorIndexWeb.html#106008" title="Click to go to the Author Index">
             Cakmak, Maya
            </a>
            (University of Washington)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab497" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_02">
             16:10-17:10, Paper TuLBR.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('486'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Development of a Robotic Fingertip-Shaped Hybrid Tactile Sensor Module for Multi-Sensory Detection During Robot Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#252877" title="Click to go to the Author Index">
             Jang, Jin-seok
            </a>
            (Korea Research Institute of Standards and Science),
            <a href="ROMAN24_AuthorIndexWeb.html#252873" title="Click to go to the Author Index">
             Bok, Bo-Gyu
            </a>
            (Korea Research Institute of Standards and Science),
            <a href="ROMAN24_AuthorIndexWeb.html#143923" title="Click to go to the Author Index">
             Kim, Min-Seok
            </a>
            (Korea Research Institute of Standards and Science)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab486" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We have developed a robot fingertip-shaped hybrid tactile sensor module that can be attached to robotic hands. The tactile sensor module consists of a dermis with an integrated three-axis force sensor and a temperature sensor, and an epidermis with an integrated pressure sensor. The three-axis force sensor detects various physical quantities (such as three-directional forces and vibrations) by attaching four commercial semiconductor strain gauges to a specially designed metal mechanism. The pressure sensor is composed of a total of 64 high-performance pressure cells in an 8 x 8 array, covering an area of approximately 2 cm x 3 cm, which allows for high spatial resolution in detecting pressure distribution. The temperature sensor uses a commercial thermistor capable of measuring from -10 ℃ to 80 ℃ to sense the temperature of objects. In conclusion, experiments have demonstrated that the developed robot fingertip module can detect multiple senses using the internal three-axis force, pressure, and temperature sensors.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_03">
             16:10-17:10, Paper TuLBR.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('507'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Force Profiling of a Shoulder Bidirectional Fabric-Based Pneumatic Actuator for a Pediatric Exosuit
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#369480" title="Click to go to the Author Index">
             Ayazi, Mehrnoosh
            </a>
            (University of California, Riverside),
            <a href="ROMAN24_AuthorIndexWeb.html#303295" title="Click to go to the Author Index">
             Sahin, Ipsita
            </a>
            (University of California, Riverside),
            <a href="ROMAN24_AuthorIndexWeb.html#196446" title="Click to go to the Author Index">
             Mucchiani, Caio
            </a>
            (University of California Riverside),
            <a href="ROMAN24_AuthorIndexWeb.html#231214" title="Click to go to the Author Index">
             Kokkoni, Elena
            </a>
            (University of California, Riverside),
            <a href="ROMAN24_AuthorIndexWeb.html#151110" title="Click to go to the Author Index">
             Karydis, Konstantinos
            </a>
            (University of California, Riverside)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab507" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a comprehensive analysis of the contact force profile of a single-cell bidirectional soft pneumatic actuator, specifically designed to aid in the abduction and adduction of the shoulder for pediatric exosuits. The actuator was embedded in an infant-scale test rig featuring two degrees of freedom: an actuated revolute joint supporting shoulder abduction/adduction and a passive (but lockable) revolute joint supporting elbow flexion/extension. Integrated load cells and an encoder within the rig were used to measure the force applied by the actuator and the shoulder joint angle, respectively. The actuator's performance was evaluated under various anchoring points and elbow joint angles. Experimental results demonstrate that optimal performance, characterized by maximum range of motion and minimal force applied on the torso and upper arm, can be achieved when the actuator is anchored at two-thirds the length of the upper arm, with the elbow joint positioned at a 90-degree angle. The force versus pressure and joint angle graphs reveal nonlinear and hysteresis behaviors. The findings of this study yield insights about optimal anchoring points and elbow angles to minimize exerted forces without reducing the range of motion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_04">
             16:10-17:10, Paper TuLBR.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('505'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Getting Ready: An Intelligent Robot System for Assisting with Activities of Daily Living
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#408082" title="Click to go to the Author Index">
             Tognotti, Matthew
            </a>
            (Santa Clara University),
            <a href="ROMAN24_AuthorIndexWeb.html#195306" title="Click to go to the Author Index">
             Kyrarini, Maria
            </a>
            (Santa Clara University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab505" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, mobile manipulators have shown their potential to support people with activities of daily living (ADLs). This paper proposed an intelligent robotic system that can assist people in preparing for ADLs, such as getting ready for work, shopping, or going to the doctor. The proposed system uses a rule-based approach to enable the robot to learn tasks from humans via speech commands and to execute the learned tasks. An experimental setup is developed to evaluate the proposed system in a pilot study. The preliminary results from the pilot study demonstrate the usefulness of the proposed robotic system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_05">
             16:10-17:10, Paper TuLBR.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('511'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              More Self-Regulation and Self-Esteem through Social Robots?
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400331" title="Click to go to the Author Index">
             Balalic, Sanja
            </a>
            (Hanze University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#408450" title="Click to go to the Author Index">
             Tolboom, Jos
            </a>
            (SLO, the Netherlands Institute for Curriculum Development),
            <a href="ROMAN24_AuthorIndexWeb.html#400341" title="Click to go to the Author Index">
             sanderman, robbert
            </a>
            (University of Groningen),
            <a href="ROMAN24_AuthorIndexWeb.html#400344" title="Click to go to the Author Index">
             degens, nick
            </a>
            (University of Arts Utrecht),
            <a href="ROMAN24_AuthorIndexWeb.html#400350" title="Click to go to the Author Index">
             Hagedoorn, Mariët
            </a>
            (University of Groningen)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab511" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The ideal of staying home till the end of their lives is not realistic for every older adult. However, even while dwelling in nursing homes, they can maintain a certain degree of self-regulation. Deployment of social robots might play a role in supporting this. In this qualitative study, seven residents of a nursing home were followed, while hosting Maatje, a small humanoid social robot, programmed in a personalized manner to give reminders, provide entertainment and answer some basic questions. A robot like Maatje seems to be a useful assistant to activate residents of a nursing home and also seems to make a positive contribution to self-esteem and sense of self-regulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_06">
             16:10-17:10, Paper TuLBR.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('513'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              EiIBM_Robot: An Emotionally Supportive Robot for Delivering Interpretation Bias Modification Exercises
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#408369" title="Click to go to the Author Index">
             Huang, Shiming
            </a>
            (The Hong Kong Polytechnic University),
            <a href="ROMAN24_AuthorIndexWeb.html#140171" title="Click to go to the Author Index">
             Hoorn, Johan
            </a>
            (VU University, Amsterdam)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab513" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces eiIBM_Robot, an emotionally supportive robot designed to deliver interpretation bias modification (IBM) exercises to individuals with depressive symptoms. The robot incorporates a conversational agent utilizing large language models and a user-centered architecture to enhance user engagement and experience. A preliminary user study (N = 38) revealed positive perceptions of the robot's performance and the effectiveness of the IBM exercises in promoting positive thinking patterns and emotional well-being. The findings highlight the potential of emotionally supportive robots to provide accessible mental health support and the importance of carefully balancing the robot's social presence and perceived role in the intervention.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_07">
             16:10-17:10, Paper TuLBR.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('508'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Draw a Robot Task: The Influence of Children's Age and Drawings on Robot Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#408092" title="Click to go to the Author Index">
             Howard, Lauren
            </a>
            (Franklin and Marshall College),
            <a href="ROMAN24_AuthorIndexWeb.html#213141" title="Click to go to the Author Index">
             Wilson, Jason
            </a>
            (Franklin &amp; Marshall College),
            <a href="ROMAN24_AuthorIndexWeb.html#408094" title="Click to go to the Author Index">
             Langer, Allison
            </a>
            (Temple University),
            <a href="ROMAN24_AuthorIndexWeb.html#408095" title="Click to go to the Author Index">
             Marshall, Peter
            </a>
            (Temple University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab508" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
             </span>
             <br/>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_08">
             16:10-17:10, Paper TuLBR.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('490'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Analysing Explanation-Related Interactions in Collaborative Perception-Cognition-Communication-Action
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#374581" title="Click to go to the Author Index">
             Roig Vilamala, Marc
            </a>
            (Cardiff University),
            <a href="ROMAN24_AuthorIndexWeb.html#407536" title="Click to go to the Author Index">
             Furby, Jack
            </a>
            (Cardiff University),
            <a href="ROMAN24_AuthorIndexWeb.html#374537" title="Click to go to the Author Index">
             de Gortari Briseno, Julian
            </a>
            (University of California, Los Angeles),
            <a href="ROMAN24_AuthorIndexWeb.html#172053" title="Click to go to the Author Index">
             Srivastava, Mani
            </a>
            (UCLA),
            <a href="ROMAN24_AuthorIndexWeb.html#374954" title="Click to go to the Author Index">
             Preece, Alun
            </a>
            (Cardiff University),
            <a href="ROMAN24_AuthorIndexWeb.html#358717" title="Click to go to the Author Index">
             Fuentes, Carolina
            </a>
            (Cardiff University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab490" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#anthropomorphic_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Anthropomorphic Robots and Virtual Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Effective communication is essential in collaborative tasks, so AI-equipped robots working alongside humans need to be able to explain their behaviour in order to cooperate effectively and earn trust. We analyse and classify communications among human participants collaborating to complete a simulated emergency response task. The analysis identifies messages that relate to various kinds of interactive explanation identified in the explainable AI literature. This allows us to understand what type of explanations humans expect from their teammates in such settings, and thus where AI-equipped robots most need explanation capabilities. We find that most explanation-related messages seek clarification in the decisions or actions taken. We also confirm that messages have an impact on the performance of our simulated task.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_09">
             16:10-17:10, Paper TuLBR.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('492'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Can Novice Supervisors Determine Successful from Unsuccessful Grasps from Tactile Sensing?
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#304684" title="Click to go to the Author Index">
             Puente, Karina
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#407978" title="Click to go to the Author Index">
             Sadler, Madelyn
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#344119" title="Click to go to the Author Index">
             Wilson, Cristina
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#187696" title="Click to go to the Author Index">
             Davidson, Joseph
            </a>
            (Oregon State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab492" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#curiosity__intentionality_and_initiative_in_interaction" title="Click to go to the Keyword Index">
               Curiosity, Intentionality and Initiative in Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Underwater manipulation is a challenging problem. To complete tasks requiring dexterous manipulation, the current standard practice is for human operators to remotely teleoperate a manipulator and end-effector, or monitor a semi-autonomous system. The energetic underwater environment (i.e. waves and currents) can cause the robot and objects in the environment to move, and water turbidity can interfere with visual perception of the scene. Turbidity is particularly problematic since cameras are the primary sensing modality operators use to monitor grasp execution. While tactile sensing has been shown to aid human-robot collaborative manipulation in terrestrial applications, it has not been studied for the underwater domain. This paper presents early work to study whether tactile feedback could help operators complete underwater manipulation. We conducted a within-subject in-the-lab user study to determine if the visual presentation of tactile sensing is sufficient to determine if a grasp is successful or unsuccessful. To complete the study, we created a mock interface that presents degraded videos of grasps besides tactile data from commercial off-the-shelf sensors. Twenty novice participants were recruited and asked to observe 36 grasp attempts across four types of objects. Results indicate that participants’ accuracy is better than chance, showing the potential for tactile sensing to increase operator performance during underwater manipulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_10">
             16:10-17:10, Paper TuLBR.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('483'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards a Human Diversity Wheel for Human Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400000" title="Click to go to the Author Index">
             Erle, Lukas
            </a>
            (Ruhr West University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#349669" title="Click to go to the Author Index">
             Timm, Lara
            </a>
            (University of Applied Sciences Ruhr West),
            <a href="ROMAN24_AuthorIndexWeb.html#407673" title="Click to go to the Author Index">
             Kleinhaus, Lara
            </a>
            (Ruhr West University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#281844" title="Click to go to the Author Index">
             Straßmann, Carolin
            </a>
            (University of Applied Sciences Ruhr West),
            <a href="ROMAN24_AuthorIndexWeb.html#281767" title="Click to go to the Author Index">
             Eimler, Sabrina C.
            </a>
            (Hochschule Ruhr West, University of Applied Sciences)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab483" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             While extant research on human-robot-interactions (HRI) has dealt with the examination of different human characteristics, quantifying and describing the various char- acteristics of human diversity remains a challenge for HRI research. This in turn often leads to a disregard of human diversity in the design of HRI. Addressing this issue, we conducted a systematic synthesis of existing models on human diversity, culminating in the development of a model we coined the Human Diversity Wheel for Robotic Interactions (HDWRI). The goal of this model is to provide designers and researchers in HRI with an analytical lens to ensure their work considers different human characteristics. To achieve this, we started to conduct expert interviews with HRI researchers to put our model into a practical context. This paper presents the development of our model, preliminary findings of our first interviews, and an outline of future steps.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_11">
             16:10-17:10, Paper TuLBR.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('499'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Revolutionizing Qualitative Human-Robot Interaction Research by Using GPT Models for Inductive Category Development
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#407542" title="Click to go to the Author Index">
             Arlinghaus, Clarissa Sabrina
            </a>
            (Bielefeld University),
            <a href="ROMAN24_AuthorIndexWeb.html#408052" title="Click to go to the Author Index">
             Wulff, Charlotte
            </a>
            (Bielefeld University),
            <a href="ROMAN24_AuthorIndexWeb.html#408051" title="Click to go to the Author Index">
             Maier, Günter W.
            </a>
            (Bielefeld University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab499" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Coding qualitative data is essential but time-consuming. This late-breaking report presents a new method for developing inductive categories utilizing GPT models. We examined two different GPT models (gpt-3.5-turbo-0125 and gpt-4o-2024-05-03) and three temperature settings (0, 0.5, 1), each with ten repetitions. The generated categories were fairly consistent across settings, although higher temperatures included less relevant aspects. The agreement for GPT-generated category assignments exceeded that of human coders, with the best performance observed at temperature setting 0. Thus, we recommend using a GPT model with the temperature setting 0 to create and assign inductive categories for qualitative data.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_12">
             16:10-17:10, Paper TuLBR.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('489'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Evaluating the Impact of Explainability on the Users' Mental Models of Robots Over Time
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399768" title="Click to go to the Author Index">
             Gebellí, Ferran
            </a>
            (PAL Robotics),
            <a href="ROMAN24_AuthorIndexWeb.html#136697" title="Click to go to the Author Index">
             Ros, Raquel
            </a>
            (PAL Robotics),
            <a href="ROMAN24_AuthorIndexWeb.html#133188" title="Click to go to the Author Index">
             Lemaignan, Séverin
            </a>
            (PAL Robotics),
            <a href="ROMAN24_AuthorIndexWeb.html#114978" title="Click to go to the Author Index">
             Garrell, Anais
            </a>
            (UPC-CSIC)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab489" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#long_term_experience_and_longitudinal_hri_studies" title="Click to go to the Keyword Index">
               Long-term Experience and Longitudinal HRI Studies
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#curiosity__intentionality_and_initiative_in_interaction" title="Click to go to the Keyword Index">
               Curiosity, Intentionality and Initiative in Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To evaluate how explanations affect the users' understanding of robots, researchers typically elicit the user's Mental Model (MM) of the robot and then compare it to the robot's actual decision-making and behaviour. However, the user's self-rating of their level of understanding, which we define as ``user-perceived understanding'', is generally not evaluated. Moreover, this evaluation is typically done only once, while robots are often expected to interact with the same users over long periods. In this work, we suggest a framework to analyse the evolution of the mental models over time across the dimensions of completeness and correctness. We argue that the goal of explainability should be two-fold. On one hand, it should help align the user's perceived understanding with the real one. On the other hand, explainability should enhance the completeness of the mental model to a target level, which varies depending on the user type, while also striving for maximum correctness.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_13">
             16:10-17:10, Paper TuLBR.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('506'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Explainable Autonomous Mobile Robots: Interface and Socially Aware Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#407878" title="Click to go to the Author Index">
             Chandra Shekar, Kiruthiga
            </a>
            (New York University),
            <a href="ROMAN24_AuthorIndexWeb.html#407875" title="Click to go to the Author Index">
             Doma, Pranav
            </a>
            (New York University),
            <a href="ROMAN24_AuthorIndexWeb.html#407877" title="Click to go to the Author Index">
             Prashanth, Chinmay
            </a>
            (New York University),
            <a href="ROMAN24_AuthorIndexWeb.html#407879" title="Click to go to the Author Index">
             Subramaniam, Vikram
            </a>
            (New York University),
            <a href="ROMAN24_AuthorIndexWeb.html#163772" title="Click to go to the Author Index">
             Arab, Aliasghar
            </a>
            (NYU)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab506" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#interaction_kinesics" title="Click to go to the Keyword Index">
               Interaction Kinesics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the growing use of mobile robots in urban settings, their interaction with people in shared spaces is increasing. It's crucial for these robots to navigate in a socially acceptable manner and interact properly with their environment. AI guides robot behavior by interpreting the environment and controlling actions. Poor explainability of robot decisions can lead to mental and physical risks. Proper robot-to-human interfaces can help mitigate these risks. This paper introduces systems and methods for self-explaining mobile robots in common areas using projected information. We analyze data on people's reactions to the robot, with and without self-explainable features, across various university campus settings like maker-spaces, hallways, cafeterias, and study areas. Initial analysis shows improved acceptance of the robot based on task performance and user questionnaires.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_14">
             16:10-17:10, Paper TuLBR.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('502'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Robotic Transport System Demonstrates Safety and Efficacy in Blood Sample Transport within Emergency Unit
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#333702" title="Click to go to the Author Index">
             Lewek, Pawel
            </a>
            (Medication Adherence Research Centre, Medical University of Lodz),
            <a href="ROMAN24_AuthorIndexWeb.html#333701" title="Click to go to the Author Index">
             Kardas, Przemyslaw
            </a>
            (Medication Adherence Research Centre, Medical University of Lodz),
            <a href="ROMAN24_AuthorIndexWeb.html#333697" title="Click to go to the Author Index">
             Lojewska, Ewelina
            </a>
            (Medical University of Lodz),
            <a href="ROMAN24_AuthorIndexWeb.html#333703" title="Click to go to the Author Index">
             Timler, Dariusz
            </a>
            (Medical University of Lodz),
            <a href="ROMAN24_AuthorIndexWeb.html#223957" title="Click to go to the Author Index">
             Chiurazzi, Marcello
            </a>
            (The BioRobotics Institute, Scuola Superiore Sant'Anna),
            <a href="ROMAN24_AuthorIndexWeb.html#250675" title="Click to go to the Author Index">
             Ciuti, Gastone
            </a>
            (Scuola Superiore Sant'Anna),
            <a href="ROMAN24_AuthorIndexWeb.html#408067" title="Click to go to the Author Index">
             Granda Sans, Sofia
            </a>
            (Inuetum),
            <a href="ROMAN24_AuthorIndexWeb.html#408068" title="Click to go to the Author Index">
             Votis, Konstantinos
            </a>
            (The Centre for Research and Technology Hellas),
            <a href="ROMAN24_AuthorIndexWeb.html#408069" title="Click to go to the Author Index">
             Lolis, Vasileios
            </a>
            (The Centre for Research and Technology Hellas),
            <a href="ROMAN24_AuthorIndexWeb.html#408070" title="Click to go to the Author Index">
             Flevarakis, Konstantinos
            </a>
            (The Centre for Research and Technology Hellas),
            <a href="ROMAN24_AuthorIndexWeb.html#248927" title="Click to go to the Author Index">
             SORIANO, Angel
            </a>
            (R&amp;D Department Robotnik Automation, SLL 46988 Valencia, SPAIN),
            <a href="ROMAN24_AuthorIndexWeb.html#408072" title="Click to go to the Author Index">
             Millet Pascual-Leone, Marta
            </a>
            (The Robotnik Automation S.L),
            <a href="ROMAN24_AuthorIndexWeb.html#291191" title="Click to go to the Author Index">
             Julia Ros, Raquel
            </a>
            (Robotnik),
            <a href="ROMAN24_AuthorIndexWeb.html#408074" title="Click to go to the Author Index">
             Solaz Estevan, Víctor
            </a>
            (Robotnik Automation S.L),
            <a href="ROMAN24_AuthorIndexWeb.html#411696" title="Click to go to the Author Index">
             Fico, Giuseppe
            </a>
            (Universidad Politécnica de Madrid)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab502" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#innovative_robot_designs" title="Click to go to the Keyword Index">
               Innovative Robot Designs
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#medical_and_surgical_applications" title="Click to go to the Keyword Index">
               Medical and Surgical Applications
              </a>
             </span>
             <br/>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_15">
             16:10-17:10, Paper TuLBR.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('509'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Mochi: Designing a Robot with Exaggerated Squash, Stretch, and Shape Alterations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#357557" title="Click to go to the Author Index">
             Iimori, Masato
            </a>
            (Tsukuba University),
            <a href="ROMAN24_AuthorIndexWeb.html#118358" title="Click to go to the Author Index">
             Tanaka, Fumihide
            </a>
            (University of Tsukuba)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab509" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#innovative_robot_designs" title="Click to go to the Keyword Index">
               Innovative Robot Designs
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motivations_and_emotions_in_robotics" title="Click to go to the Keyword Index">
               Motivations and Emotions in Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Animation principles are techniques employed in the animation of characters to create the illusion of life. These principles are employed in the field of human-robot interaction. Two fundamental animation principles that contribute to the generation of this illusion are squash and stretch, as well as exaggeration. Nevertheless, no robot has ever been observed to engage in both simultaneously. This research proposes Mochi, a robot that incorporates the aforementioned animation principles. This paper presents a discussion of the concept, design, and motion of the Mochi robot, as well as its current implementation status and prospects for application and improvement.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tulbr_16">
             16:10-17:10, Paper TuLBR.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('498'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Object-Centric Spatially-Aware Gesture-Based Motion Specification of Robotic Manipulation Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#408038" title="Click to go to the Author Index">
             Nazir, Rehan
            </a>
            (Santa Clara University),
            <a href="ROMAN24_AuthorIndexWeb.html#325320" title="Click to go to the Author Index">
             Sharma, Manoj
            </a>
            (Santa Clara University),
            <a href="ROMAN24_AuthorIndexWeb.html#100046" title="Click to go to the Author Index">
             Kitts, Christopher
            </a>
            (Santa Clara University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab498" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#interaction_kinesics" title="Click to go to the Keyword Index">
               Interaction Kinesics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
             </span>
             <br/>
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tudt1">
             <b>
              TuDT1
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tudt1" title="Click to go to the Program at a Glance">
             <b>
              Bridging Trust and Context: Dynamic Interactions in HAI II
             </b>
            </a>
           </td>
           <td class="r">
            Special Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt1_01">
             17:10-17:25, Paper TuDT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('106'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automatic Joint Gaze Generation between Local and Remote Persons through Telepresence Robot (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#358938" title="Click to go to the Author Index">
             Ikoma, Hibiki
            </a>
            (Shizuoka University),
            <a href="ROMAN24_AuthorIndexWeb.html#118238" title="Click to go to the Author Index">
             Takeuchi, Yugo
            </a>
            (Shizuoka University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab106" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#embodiment__empathy_and_intersubjectivity" title="Click to go to the Keyword Index">
               Embodiment, Empathy and Intersubjectivity
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             COVID-19 and technological developments have increased opportunities for remote communication. This has led to a focus on ``telepresence avatar robots,'' a technology that makes people feel as if they are sharing the same place, even from remote locations. The telepresence avatar robot takes the approach of supporting communication by generating a realistic sense of conversation that incorporates technologies such as camera motion and eye contact with the speaker. However, this approach does not enable smooth communication because the robot operator cannot recognize the intention of the local worker's behavior, since the information transmitted from the remote location is less than that available in face-to-face communication. In this study, we focus on gaze-following motion as an unconscious human behavior and implement it in a robot to verify whether a robot operator can perceive the intention of a remote worker through this autonomous motion. Eye-tracking is an action in which one tracks the other's gaze in a local conversation, and it is performed unconsciously. The experimental results suggest that this autonomous behavior can generate an estimation of gazing intention． Consequently, this approach may be a useful method for achieving smoother communication.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt1_02">
             17:25-17:40, Paper TuDT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('306'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Unveiling Trust Dynamics with a Mobile Service Robot: Exploring Various Interaction Styles for an Agricultural Task (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#389613" title="Click to go to the Author Index">
             Elias, Alex
            </a>
            (University of Lincoln),
            <a href="ROMAN24_AuthorIndexWeb.html#349648" title="Click to go to the Author Index">
             Galvez Trigo, Maria Jose
            </a>
            (Cardiff University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab306" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#programming_by_demonstration" title="Click to go to the Keyword Index">
               Programming by Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As robotics, particularly in agriculture, become more prevalent, understanding the role that different factors play on the trust levels that users have in these robots becomes crucial to facilitate their adoption and integration into the industry. In this paper we present the results of a within-subjects study that included between-subject factors exploring how prior experience with robotics and different interaction styles with a mobile manipulator robot may affect trust levels in said robot before and after the completion of an agriculture-related manipulation task. The results show that interacting with the robot helps improve trust levels, particularly for those without prior experience with robotics, who present a higher trust improvement score, and that an interaction style involving physical human-robot interaction (pHRI), more specifically Learning by Demonstration, was favoured versus less direct interaction styles. We found that incorporating Text-to-Speech (TTS) can be a good design choice when trying to improve trust, and that the improvement score for trust before and after interaction with the robot was significantly higher for older age groups, with these participants being more conservative with their reported trust level before the interaction. Overall, these results offer insights into different interaction styles and their effect on trust levels for an agriculture-related manipulation task, and open the door to future work exploring further interaction styles and task variations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt1_03">
             17:40-17:55, Paper TuDT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('435'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Effect of Robot Pose and Distance on Pedestrian and Observer Comfort During Passing (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#348310" title="Click to go to the Author Index">
             Ohnishi, Fumiya
            </a>
            (Keio University),
            <a href="ROMAN24_AuthorIndexWeb.html#102832" title="Click to go to the Author Index">
             Takahashi, Masaki
            </a>
            (Keio University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab435" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To facilitate the integration of autonomous mobile robots into environments shared with humans, ensuring the comfort of pedestrians and observers (individuals observing the interaction from a third-party perspective) is crucial. This study explores the correlation between pedestrian and observer comfort and the robot's distance and pose angle during pedestrian encounters. The findings indicate that when a robot passes a pedestrian after completing its avoidance motion, it enhances the comfort of both pedestrians and observers. In addition, when a robot maintains a large distance from pedestrians while passing by, it improves pedestrian comfort. Furthermore, designing the robot's pose during passing so that it appears to move away from the pedestrian from the observer's perspective further increases the observer's comfort.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt1_04">
             17:55-18:10, Paper TuDT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             No More Mumbles: Enhancing Robot Intelligibility through Speech Adaptation
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#385180" title="Click to go to the Author Index">
             ren, qiaoqiao
            </a>
            (AIRO - IDLab - University of Ghent - IMEC),
            <a href="ROMAN24_AuthorIndexWeb.html#385467" title="Click to go to the Author Index">
             Hou, Yuanbo
            </a>
            (Ghent University),
            <a href="ROMAN24_AuthorIndexWeb.html#385466" title="Click to go to the Author Index">
             Botteldooren, Dick
            </a>
            (Ghent University),
            <a href="ROMAN24_AuthorIndexWeb.html#126600" title="Click to go to the Author Index">
             Belpaeme, Tony
            </a>
            (Ghent University)
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tudt2">
             <b>
              TuDT2
             </b>
            </a>
           </td>
           <td class="r">
            Room T2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tudt2" title="Click to go to the Program at a Glance">
             <b>
              Assistive Robotics II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt2_01">
             17:10-17:25, Paper TuDT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('324'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Accessibility-Aware Reinforcement Learning for Inclusive Robotic Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#364081" title="Click to go to the Author Index">
             Hu, Mengsha
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#377827" title="Click to go to the Author Index">
             Wu, Yunhuan
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#297423" title="Click to go to the Author Index">
             Guo, Yibei
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#376059" title="Click to go to the Author Index">
             Wu, Yi
            </a>
            (Paris-Saclay University),
            <a href="ROMAN24_AuthorIndexWeb.html#394419" title="Click to go to the Author Index">
             Shi, Chao
            </a>
            (Binghamton University),
            <a href="ROMAN24_AuthorIndexWeb.html#373682" title="Click to go to the Author Index">
             Xu, Lei
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#238447" title="Click to go to the Author Index">
             Liu, Rui
            </a>
            (Kent State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab324" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic navigation aids humans in essential scenarios, including airport customer boarding, commercial meeting receptions, and disaster site navigation. While individuals benefit from robotic services, these services pose accessibility challenges for people with disabilities. Robots may face limitations in mobility, vision, hearing, and cognitive reasoning, hindering disabled individuals' access to these services. As robotic services become increasingly prevalent, ensuring they are inclusive and accessible is crucial, accommodating human disabilities with adaptations such as slower movements, standing support, and hazard detection. To address this issue, this study introduces a novel Accessibility-Aware Reinforcement Learning model (ARL). ARL extracts disability-related information from human behavioral observations using a context-embedding neural network. It then adjusts robot motions to provide inclusive assistance to individuals with disabilities. To assess the effectiveness of this approach, four assisting services in navigation scenarios ('lead', 'wait', 'assist', and 'protect') and three application scenarios involving navigating elderly individuals in a museum, through traffic, and across a building patio were designed. The results validate ARL's efficacy in enabling robots to make accessibility-aware decisions, thereby enhancing their support for individuals with disabilities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt2_02">
             17:25-17:40, Paper TuDT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('343'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Co-Designing an Accessible Quadruped Navigation Assistant
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#365412" title="Click to go to the Author Index">
             Doore, Stacy A.
            </a>
            (Colby College),
            <a href="ROMAN24_AuthorIndexWeb.html#400271" title="Click to go to the Author Index">
             Trikasemsak, Narit
            </a>
            (Colby College),
            <a href="ROMAN24_AuthorIndexWeb.html#400274" title="Click to go to the Author Index">
             Gillespie, Alexandra
            </a>
            (Colby College),
            <a href="ROMAN24_AuthorIndexWeb.html#400275" title="Click to go to the Author Index">
             Giudice, Andrea
            </a>
            (Pine Tree Guide Dog Users (PTGDU)),
            <a href="ROMAN24_AuthorIndexWeb.html#365491" title="Click to go to the Author Index">
             Hata, Rayna
            </a>
            (Carnegie Mellon University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab343" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             While there is no replacement for the learned expertise, devotion, and social benefits of a guide dog, there are scenarios in which a robot navigation assistant could be helpful for individuals with blindness or low vision (BLV). This case study investigated the potential for an industrial quadruped robot to perform guided navigation tasks based on a co-design model. The research was informed by a guide dog handler with over 30 years of experience of non-visual navigation. In order to communicate spatial information between the human-robot team, two interface prototypes were created and pilot tested: a voice-based app and a flexible, responsive guide handle. The pilot user study consisted of sighted participants and our BLV co-designer, who completed simple navigation tasks and a post-study survey about the prototype functionality and their trust in the robot. All participants successfully completed the navigation tasks and demonstrated that the interface prototypes were able to pass spatial information between the human and the robot. Findings of this exploratory study will help to inform human-robot teaming and collaboration. Future work will include expanding the voice-based app to allow the robot to directly communicate obstacles to the handler, adding haptic navigation signals to the handle design, and expanding the user study to include a larger sample of experienced guide dog handlers.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt2_03">
             17:40-17:55, Paper TuDT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('277'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Functional and Extra Motor Abilities: A Focus Group Study on the Re-Design of an Extra-Robotic Finger
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400158" title="Click to go to the Author Index">
             Hendriks, Sjoerd
            </a>
            (Chalmers University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#317935" title="Click to go to the Author Index">
             Hasanen, Basma
            </a>
            (Khalifa University of Science and Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#192013" title="Click to go to the Author Index">
             Afzal, Hafiz Malik Naqash
            </a>
            (UNSW Sydney),
            <a href="ROMAN24_AuthorIndexWeb.html#174021" title="Click to go to the Author Index">
             Hussain, Irfan
            </a>
            (Khalifa University),
            <a href="ROMAN24_AuthorIndexWeb.html#160322" title="Click to go to the Author Index">
             Obaid, Mohammad
            </a>
            (Chalmers University of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab277" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The utilization of extra-robotic limbs to regain and extend motor abilities poses challenges. Nonetheless, the evidence suggesting the extension of these abilities without functional interference holds promise for the advancement of extra-robotic fingers or limbs. To gain insights into design improvement from diverse perspectives, we conducted a focus group study utilizing a prototype supernumerary robotic finger (SRF). The main purpose of the study is to gain knowledge on the current prototype, and then use the outcomes in a future co-designing process of SRF having the potential to be used as an assistive and augmented tool. Fifteen healthy subjects with an engineering background participated in the current study. We analyzed the collected data through a thematic analysis process to generate key design recommendations. These recommendations contain suggestions for designing for personal and social acceptance, adaptability, and multi-modal interaction. The results serve as a subset of future subsequent studies to be carried out with other stakeholders of the prototype, including patients, clinicians, and designers. Future work focuses on using the recommendations in a co-design process to inform the prototyping of SRF technologies supporting and extending motor abilities in upper limbs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt2_04">
             17:55-18:10, Paper TuDT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('371'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Wheelchair Mobility: Virtual Training System Integrating Lateral Trunk Motion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#312673" title="Click to go to the Author Index">
             Callupe Luna, Jhedmar Jhonatan
            </a>
            (UVSQ University of Paris-Saclay),
            <a href="ROMAN24_AuthorIndexWeb.html#400019" title="Click to go to the Author Index">
             Bougherara, Selsabil
            </a>
            (Université De Versailles Saint-Quentin-En-Yvelines),
            <a href="ROMAN24_AuthorIndexWeb.html#119291" title="Click to go to the Author Index">
             Monacelli, Eric
            </a>
            (LISV, University of Versailles),
            <a href="ROMAN24_AuthorIndexWeb.html#100909" title="Click to go to the Author Index">
             Hirata, Yasuhisa
            </a>
            (Tohoku University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab371" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Following an accident or motor impairment from illness, individuals often experience reduced mobility. When lower limb function is affected, individuals may lose the ability to walk and rely on a wheelchair. Despite maintaining upper limb mobility, emotional and societal factors may reduce motivation to use them. The Volting project aims to address this by developing a wheelchair with improved mobility to help users achieve their maximum potential. To overcome the limitations of traditional wheelchair controls, a device called WISP was developed, allowing control of Volting through trunk movements. This not only frees the upper limbs but also encourages users to engage their bodies in mobility. Initial experiments with a professional wheelchair dancer showed rapid adaptation to WISP usage. However, gradual implementation is recommended for non-athletic wheelchair users due to the risk of injury from unfamiliar movements. Therefore, we present a training system for WISP utilization, combining a virtual environment with haptic feedback to aid learning and adjust difficulty levels. Preliminary results demonstrate significant improvements in task completion time, collisions, and spatial errors after three sessions in our virtual environment. Additionally, trunk movements decreased, indicating more efficient adaptation to WISP use. As the next step, we plan to conduct trials with individuals with disabilities in our virtual training system, followed by integrating WISP with Volting to explore enhanced mobility.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tudt3">
             <b>
              TuDT3
             </b>
            </a>
           </td>
           <td class="r">
            Room T3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tudt3" title="Click to go to the Program at a Glance">
             <b>
              Ethical Issues in Human-Robot Interaction Research II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt3_01">
             17:10-17:25, Paper TuDT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('92'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robots among Us: An Exploratory Study on the Situated Ethics of Human-Robot Interactions in a Public Library
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#351795" title="Click to go to the Author Index">
             Chan, Kok Hui Jeffrey
            </a>
            (Singapore University of Technology and Design),
            <a href="ROMAN24_AuthorIndexWeb.html#355927" title="Click to go to the Author Index">
             Jiang, Zhuoqun
            </a>
            (Singapore University of Technology and Design),
            <a href="ROMAN24_AuthorIndexWeb.html#196976" title="Click to go to the Author Index">
             Wang, Yixiao
            </a>
            (Georgia Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab92" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Social robots are rarely confined to a task-specific or problem-specific work envelope. For instance, an assistive robot working in a senior center can incidentally become a companion or even a trusted confidante. Increasingly operating in public spaces, social robots can be easily caught in mundane yet morally ambivalent urban situations that are often dynamic, complex, and unpredictable, presenting novel factors and agencies that can quickly exceed the scope of any projected human-robot interaction. Anticipating how a robot should respond in these situations may mitigate interaction failures. To improve foresight in these situations, we propose the frame of “Situated Ethics for Urban Encounters” (SEUE) and probe this frame through 18 mundane yet morally ambivalent scenarios of human-robot interaction in a public library using the speed-dating research method with storyboards. Our findings suggest: (i) There are ambiguous but morally important interactions even in a public library; (ii) Unlike humans, robots are expected to do many things at once and some actions may risk deception; (iii) The importance of children’s rights in these situations; (iv) A need for creatively opportunistic interaction when responding to the ethical challenges of the situation; and (v) Residual risks exist even in a perceived ethical response on the part of the robot. We contribute to empirical HRI ethics by demonstrating how the speed-dating technique in a participatory design setting could be used to unpack, interpret, discuss, and elicit possible HRI responses in urban situations, which are expected to only become more important as robots show up in cities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt3_02">
             17:25-17:40, Paper TuDT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('338'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Rust in Peace: Life-Cycle Management of Companion Robots and Implications for Human Users
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#176149" title="Click to go to the Author Index">
             Haring, Kerstin Sophie
            </a>
            (University of Denver),
            <a href="ROMAN24_AuthorIndexWeb.html#360841" title="Click to go to the Author Index">
             Laity, Weston
            </a>
            (University of Denver)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab338" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The interaction between humans and robots transcends mere utility, venturing into the realm of emotional bonds and perceived lifespans. Through a review of the literature and poignant case studies, this article reveals the intricate language and sentiments expressed by individuals as they navigate the loss of their mechanical companions. This exploration is relevant to understanding the complex social and emotional relationships that can form between humans and robots, emphasizing the need for thoughtful consideration of the life-cycle management of social robots. This research also underscores a significant gap in existing literature and industrial practices: the end-of-life phase of robots and the consequential emotional toll on humans.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt3_03">
             17:40-17:55, Paper TuDT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('217'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HRI Wasn't Built in a Day: A Call to Action for Responsible HRI Research
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#290489" title="Click to go to the Author Index">
             Spitale, Micol
            </a>
            (University of Cambridge),
            <a href="ROMAN24_AuthorIndexWeb.html#251576" title="Click to go to the Author Index">
             Stower, Rebecca
            </a>
            (KTH),
            <a href="ROMAN24_AuthorIndexWeb.html#354373" title="Click to go to the Author Index">
             Parreira, Maria Teresa
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#230957" title="Click to go to the Author Index">
             Yadollahi, Elmira
            </a>
            (KTH),
            <a href="ROMAN24_AuthorIndexWeb.html#115856" title="Click to go to the Author Index">
             Leite, Iolanda
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#151713" title="Click to go to the Author Index">
             Gunes, Hatice
            </a>
            (University of Cambridge)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab217" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, the awareness of the academy around responsible research has notably increased. For instance, with advances in machine learning and artificial intelligence, recent efforts have been made to promote ethical, fair, and inclusive AI and robotics. However, the field of human-robot-interaction (HRI) is seemingly lagging behind these practices. To better understand if and to what extent HRI is incentivizing researchers to engage in responsible research, we conducted an exploratory review of the publishing guidelines for the most popular HRI conference venues. We identified 18 conferences which published at least 7 HRI papers in 2022. From these, we discuss four themes relevant to conducting responsible HRI research in line with the Responsible Research and Innovation framework: ethical and human participant considerations, transparency and reproducibility, accessibility and inclusion, and plagiarism and LLM use. We identify several gaps and room for improvement within HRI regarding responsible research. Finally, we establish a call to action to provoke novel conversations among HRI researchers about the importance of conducting responsible research within emerging fields like HRI.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tudt4">
             <b>
              TuDT4
             </b>
            </a>
           </td>
           <td class="r">
            Room T4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tudt4" title="Click to go to the Program at a Glance">
             <b>
              Non-Verbal Cues and Expressiveness II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt4_01">
             17:10-17:25, Paper TuDT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('426'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Take It! Exploring Cartesian Features for Expressive Arm Motion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#360484" title="Click to go to the Author Index">
             Challa, Ramya
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#400417" title="Click to go to the Author Index">
             Sanchez, Luke
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#344119" title="Click to go to the Author Index">
             Wilson, Cristina
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#128348" title="Click to go to the Author Index">
             Knight, Heather
            </a>
            (Oregon State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab426" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_art_and_entertainment" title="Click to go to the Keyword Index">
               Robots in art and entertainment
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, a controlled user study explores four Cartesian spatial trajectories (high, low, direct, and from the side arcs), with three unique objects (flower, dagger, water). In mathematics, the Cartesian planes of horizontal, vertical, and forward/back are widely used for path-planning, much like any animal seeking to traverse the surface of the earth. Gravity naturally pulls objects down, and extra force must be exerted to push them upwards. In Laban notation, a dance annotation language, actors and dancers are similarly taught to think of table, door, and wheel planes (direct Cartesian correlates). What is the impact of these cardinal directions on interpretation of robot handovers? In these results, we find that direct paths, water, and flowers are generally seen positively, with daggers and high paths as least friendly. There is also a statistically significant interaction between path and object on robot predictability ratings. In terms of nuance, we also find that elegance is best predicted by path shape. These findings expand prior features of mobile robot expression to arcing planes, finding the up/down degree of freedom to significantly impact human interpretation and experience. Future work can continue such investigations grounded in real world tasks such as assistive robotics (passing a blanket or tissue), or service (retrieving a ticket for a bus).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt4_02">
             17:25-17:40, Paper TuDT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('188'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Decoding Engagement: The Role of Closeness Cues in Human-Robot Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399834" title="Click to go to the Author Index">
             Loos, Kira
            </a>
            (Bielefeld University),
            <a href="ROMAN24_AuthorIndexWeb.html#399835" title="Click to go to the Author Index">
             Brandt, Mara
            </a>
            (Bielefeld University),
            <a href="ROMAN24_AuthorIndexWeb.html#176179" title="Click to go to the Author Index">
             Vollmer, Anna-Lisa
            </a>
            (Bielefeld University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab188" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work examines the critical role of nonverbal cues in initiating successful human-robot interactions (HRI), focusing on the development of an algorithm capable of decoding subtle cues that indicate a human's intention to interact with a robot. The foundation for the analysis is the concept of closeness which relates to the individual feeling of intimacy and comfort, and indicates the engagement potential in diverse settings and interactions. This paper introduces a novel algorithm designed to quantify a person's willingness to interact based on analyzing the displayed closeness on a series of images. The algorithm's efficacy is evaluated through an empirical study involving 20 participants, generating a substantial dataset of 130 video sequences. These sequences were analyzed to assess the algorithm's ability to predict communication initiation intentions, with findings suggesting a significant distinction between participants engaged in interaction with a robot and those instructed to ignore it. The results underscore the algorithm's potential in facilitating more nuanced and socially sustainable HRI, laying the groundwork for future advancements in the field. This research contributes to the growing body of work on social robotics, emphasizing the importance of integrating nonverbal cue analysis for enhancing robots' interactive capabilities and fostering more meaningful human-robot connections.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt4_03">
             17:40-17:55, Paper TuDT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('58'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safe Spot: Exploring Perceived Safety of Dominant vs Submissive Quadruped Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#394345" title="Click to go to the Author Index">
             Hashimoto, Nanami
            </a>
            (Delft University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#394346" title="Click to go to the Author Index">
             Hagens, Emma
            </a>
            (Delft University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#332931" title="Click to go to the Author Index">
             Zgonnikov, Arkady
            </a>
            (Delft University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#274413" title="Click to go to the Author Index">
             Lupetti, Maria Luce
            </a>
            (Politecnico Di Torino)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab58" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_presence_for_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Social Presence for Robots and Virtual Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Unprecedented possibilities of quadruped robots have driven much research on the technical aspects of these robots. However, the social perception and acceptability of quadruped robots so far remain poorly understood. This work investigates whether the way we design quadruped robots' behaviors can affect people’s perception of safety in interactions with these robots. We designed and tested a dominant and submissive personality for the quadruped robot (Boston Dynamics Spot). These were tested in two different walking scenarios (head-on and crossing interactions) in a 2x2 within-subjects study. We collected both behavioral data and subjective reports on participants' perception of the interaction. The results highlight that participants perceived the submissive robot as safer compared to the dominant one. The behavioral dynamics of interactions did not change depending on the robot's appearance. Participants' previous in-person experience with the robot was associated with lower subjective safety ratings but did not correlate with the interaction dynamics. Our findings have implications for the design of quadruped robots and contribute to the body of knowledge on the social perception of non-humanoid robots. We call for a stronger standing of felt experiences in human-robot interaction research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt4_04">
             17:55-18:10, Paper TuDT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('421'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              “One Soy Latte for Daniel”: Visual and Movement Communication of Intention from a Robot Waiter to a Group of Customers
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400298" title="Click to go to the Author Index">
             Hong, Seung Chan
            </a>
            (Monash Universitiy),
            <a href="ROMAN24_AuthorIndexWeb.html#279417" title="Click to go to the Author Index">
             Tian, Leimin
            </a>
            (Monash University),
            <a href="ROMAN24_AuthorIndexWeb.html#147002" title="Click to go to the Author Index">
             Cosgun, Akansel
            </a>
            (Monash University),
            <a href="ROMAN24_AuthorIndexWeb.html#108781" title="Click to go to the Author Index">
             Kulic, Dana
            </a>
            (Monash University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab421" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#curiosity__intentionality_and_initiative_in_interaction" title="Click to go to the Keyword Index">
               Curiosity, Intentionality and Initiative in Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Service robots are increasingly employed in the hospitality industry for delivering food orders in restaurants. However, in current practice the robot often arrives at a fixed location for each table when delivering orders to different patrons in the same dining group, thus requiring a human staff member or the customers themselves to identify and retrieve each order. This study investigates how to improve the robot’s service behaviours to facilitate clear intention communication to a group of users, thus achieving accurate delivery and positive user experiences. Specifically, we conduct user studies (N=30) with a Temi service robot as a representative delivery robot currently adopted in restaurants. We investigated two factors in the robot’s intent communication, namely visualisation and movement trajectories, and their influence on the objective and subjective interaction outcomes. A robot personalising its movement trajectory and stopping location in addition to displaying a visualisation of the order yields more accurate intent communication and successful order delivery, as well as more positive user perception towards the robot and its service. Our results also showed that individuals in a group have different interaction experiences.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tudt5">
             <b>
              TuDT5
             </b>
            </a>
           </td>
           <td class="r">
            Room T5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tudt5" title="Click to go to the Program at a Glance">
             <b>
              Programming by Demonstration
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt5_01">
             17:10-17:25, Paper TuDT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('19'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Hybrid Approach of No-Code Robot Programming for Agile Production: Integrating Finger-Gesture and Point Cloud
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#302606" title="Click to go to the Author Index">
             Halim, Jayanto
            </a>
            (Fraunhofer Institute for Machine Tools and Forming Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#281825" title="Click to go to the Author Index">
             Eichler, Paul
            </a>
            (Fraunhofer Institute for Machine Tools and Forming Technology IW),
            <a href="ROMAN24_AuthorIndexWeb.html#204102" title="Click to go to the Author Index">
             Krusche, Sebastian
            </a>
            (Fraunhofer IWU),
            <a href="ROMAN24_AuthorIndexWeb.html#154370" title="Click to go to the Author Index">
             Bdiwi, Mohamad
            </a>
            (Fraunhofer Institute for Machine Tools and Forming Technology IW),
            <a href="ROMAN24_AuthorIndexWeb.html#294513" title="Click to go to the Author Index">
             Ihlenfeldt, Steffen
            </a>
            (TU Dresden)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab19" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#programming_by_demonstration" title="Click to go to the Keyword Index">
               Programming by Demonstration
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Industrial robot programming necessitates specialized expertise and significant time commitment, particularly for small-batch productions. In response to the escalating demand for production agility, novel approaches have emerged in intuitive robot programming. These inventive systems, rooted in diverse conceptual frameworks, are designed to expedite the deployment of robot systems. A prominent innovation in this domain is adopting no-code robot programming through finger-based gestures. A robot program can be generated by capturing and tracking non-expert users' finger movements and gestures, converting 3D coordinates into an executable robot programming language. However, accurately determining finger positions for 3D coordinates and precise geometrical features presents an ongoing challenge. In pursuit of heightened trajectory precision and reducing more significant effort for the users, we propose a hybrid methodology that amalgamates finger-gesture programming with point cloud data. This synergistic integration demonstrates promising outcomes, substantiating its potential to facilitate the precise and adaptive generation of robot paths within real industrial scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt5_02">
             17:25-17:40, Paper TuDT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('199'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Improving Learning from Visual Demonstration Methods by Target Localization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#358710" title="Click to go to the Author Index">
             Foggia, Pasquale
            </a>
            (University of Salerno),
            <a href="ROMAN24_AuthorIndexWeb.html#399384" title="Click to go to the Author Index">
             Rosa, Francesco
            </a>
            (University of Salerno),
            <a href="ROMAN24_AuthorIndexWeb.html#195260" title="Click to go to the Author Index">
             Vento, Mario
            </a>
            (University of Salerno)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab199" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#programming_by_demonstration" title="Click to go to the Keyword Index">
               Programming by Demonstration
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel approach to multi-task visual-guided imitation learning. Upon evaluating the current state-of-the-art method, we observed its capability to replicate the intent of the demonstrator, but with the flaw of manipulating incorrect objects. To address this issue, our study introduces a new approach based on the assumption that explicitly addressing task-relevant problems, such as target object localization, can enhance system performance. Our validation shows that the proposal overtakes the leading method thanks to its ability to drive the robot motion towards the target object.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt5_03">
             17:40-17:55, Paper TuDT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('221'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Robotic Skill Learning System Built Upon Diffusion Policies and Foundation Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399960" title="Click to go to the Author Index">
             Ingelhag, Nils
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400005" title="Click to go to the Author Index">
             Munkeby, Jesper
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#378015" title="Click to go to the Author Index">
             van Haastregt, Jonne
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#193766" title="Click to go to the Author Index">
             Varava, Anastasiia
            </a>
            (KTH, the Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#239713" title="Click to go to the Author Index">
             Welle, Michael C.
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#101716" title="Click to go to the Author Index">
             Kragic, Danica
            </a>
            (KTH)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab221" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#programming_by_demonstration" title="Click to go to the Keyword Index">
               Programming by Demonstration
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_learning_and_skill_acquisition_via_teaching_and_imitation" title="Click to go to the Keyword Index">
               Social Learning and Skill Acquisition Via Teaching and Imitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we build upon two major recent developments in the field, Diffusion Policies for visuomotor manipulation and large pre-trained multimodal foundational models to obtain a robotic skill learning system. The system can obtain new skills via the behavioral cloning approach of visuomotor diffusion policies given teleoperated demonstrations. Foundational models are being used to perform skill selection given the user's prompt in natural language. Before executing a skill the foundational model performs a precondition check given an observation of the workspace. We compare the performance of different foundational models to this end and give a detailed experimental evaluation of the skills taught by the user in simulation and the real world. Finally, we showcase the combined system on a challenging food serving scenario in the real world. Videos of all experimental executions, as well as the process of teaching new skills in simulation and the real world, are available on the project's website.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt5_04">
             17:55-18:10, Paper TuDT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('291'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Biologically Inspired Program-Level Imitation Approach for Robots: Proof-Of-Concept
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#302525" title="Click to go to the Author Index">
             Aliasghari, Pourya
            </a>
            (University of Waterloo),
            <a href="ROMAN24_AuthorIndexWeb.html#281635" title="Click to go to the Author Index">
             Ghafurian, Moojan
            </a>
            (University of Waterloo),
            <a href="ROMAN24_AuthorIndexWeb.html#115451" title="Click to go to the Author Index">
             Nehaniv, Chrystopher
            </a>
            (University of Waterloo),
            <a href="ROMAN24_AuthorIndexWeb.html#114390" title="Click to go to the Author Index">
             Dautenhahn, Kerstin
            </a>
            (University of Waterloo)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab291" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_learning_and_skill_acquisition_via_teaching_and_imitation" title="Click to go to the Keyword Index">
               Social Learning and Skill Acquisition Via Teaching and Imitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#programming_by_demonstration" title="Click to go to the Keyword Index">
               Programming by Demonstration
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#computational_architectures" title="Click to go to the Keyword Index">
               Computational Architectures
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For social robots to succeed in places such as homes, they must learn new skills from various people and act in a manner desirable to different users. We introduce a novel biologically inspired approach for robot learning through program-level imitation, inspired by the way primates, including humans, understand and perform complex actions. Our approach enables robots to discover the hierarchical structure of tasks by identifying sequential regularities and sub-goals from diverse human demonstrations. To do so, human-provided demonstrations, which can be obtained by a robot through different modalities (such as kinesthetic teaching, behavioural observation, and verbal instruction), are processed by an algorithm that discovers multiple possibilities for arranging observed sub-goals to achieve a final goal. Prior to acting, the available sequences are evaluated based on user-defined criteria, through mental simulation of the task by the robot, to find the optimal sequence of actions. As a proof-of-concept, we implemented our system on an iCub humanoid robot and present here how our method allowed the robot to adapt its action sequences for task execution when starting the task from different states, incorporating user preference for finishing the task as fast as possible. Our envisaged system is meant to accommodate variations in human teaching styles and is expected to help a robot perform tasks with greater flexibility and efficiency. This work contributes by proposing a framework for robots to learn from humans at an abstract level, opening the way to more adaptable and intelligent robotic assistants in everyday tasks.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tudt6">
             <b>
              TuDT6
             </b>
            </a>
           </td>
           <td class="r">
            Room T6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tudt6" title="Click to go to the Program at a Glance">
             <b>
              Personalities and Behaviors
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt6_01">
             17:10-17:25, Paper TuDT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('151'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Introducing a Note of Levity to Human-Robot Interaction with Dialogs Containing Irony, Sarcasm and Jocularity
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#348207" title="Click to go to the Author Index">
             Sievers, Thomas
            </a>
            (University of Lübeck),
            <a href="ROMAN24_AuthorIndexWeb.html#358849" title="Click to go to the Author Index">
             Russwinkel, Nele
            </a>
            (Universität Zu Lübeck)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab151" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#personalities_for_robotic_or_virtual_characters" title="Click to go to the Keyword Index">
               Personalities for Robotic or Virtual Characters
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dialogs between humans and social robots are often fact-oriented and not very mood-dependent, at least on the part of the robot. This raises the question of what influence the tonality of the utterances has on the perceived personality of the robot and how this influences the user's further expectations. One element that is not necessarily expected from robots in conversation is humor, especially the use of irony and sarcasm. We examined the effect of a conversation with cheerfully ironic or sullenly sarcastic remarks by the robot in contrast to utterances in a more seriously neutral tone. To measure these effects in terms of perceived warmth, competence and possible discomfort in conversation we used the Robotic Social Attributes Scale (RoSAS) on 20 participants with an age ranging from 26 to 58. In addition, we asked the participants for their assessment of the suitability of the robot's dialog style for different areas of application and which dialog style they personally liked best. The dialog parts of the robot were generated with ChatGPT using suitable prompts. Our results showed that the perceived warmth gained highest rates when when the robot hat a cheerfully ironic tone. Discomfort was rated significantly higher when a sullenly sarcastic tone was used by the robot. Perceived competence seemed to be slightly negatively influenced in the sarcastic condition. The results proved that tonality appears to be a very relevant design element for a successful interaction between humans and robots, but must be used carefully to achieve the desired goal.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt6_02">
             17:25-17:40, Paper TuDT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('113'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Personality and Memory-Based Framework for Emotionally Intelligent Agents
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#360362" title="Click to go to the Author Index">
             Nardelli, Alice
            </a>
            (University of Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#399557" title="Click to go to the Author Index">
             Maccagni, Giacomo
            </a>
            (Reply S.p.A),
            <a href="ROMAN24_AuthorIndexWeb.html#399558" title="Click to go to the Author Index">
             Minutoli, Federico
            </a>
            (Reply S.p.A),
            <a href="ROMAN24_AuthorIndexWeb.html#108312" title="Click to go to the Author Index">
             Sgorbissa, Antonio
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#155018" title="Click to go to the Author Index">
             Recchiuto, Carmine Tommaso
            </a>
            (University of Genova)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab113" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#personalities_for_robotic_or_virtual_characters" title="Click to go to the Keyword Index">
               Personalities for Robotic or Virtual Characters
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The goal-directed behavior observed in humans arises from the intricate interplay of various processes, including personality dynamics, emotional responses to others, memory encoding, the anticipation of future actions, and associated hedonic experiences. Integrating these multiple processes characteristic of human intelligence into a robotic framework aims to enhance the human-likeness of artificial agents and facilitate more natural and intuitive interactions with humans.
             <p>
              For this purpose, in this paper, we propose a comprehensive psychological and cognitive architecture where, personality, as it happens for humans, not only influences the execution of actions but also shapes internal reactions to human emotions and guides anticipatory decision-making processes tailored to the agent's traits. We demonstrate the framework's effectiveness in generating perceivable synthetic personalities through an experiment involving participants in a dyadic conversation scenario with a digital human, where the digital human's behavior is driven by its assigned personality. The results show that participants accurately perceive the artificial personality displayed by the digital human. We also demonstrate the potential of our robotic framework to bridge the gap between cognitive and psychological agents, as the findings highlight its ability to create a cognitively and emotionally intelligent digital human.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt6_03">
             17:40-17:55, Paper TuDT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('169'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Planning with Critical Decision Points: Robots That Influence Humans to Infer Their Strategy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#253194" title="Click to go to the Author Index">
             Ghose, Debasmita
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#247064" title="Click to go to the Author Index">
             Lewkowicz, Michal
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#399769" title="Click to go to the Author Index">
             Dong, David
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#397813" title="Click to go to the Author Index">
             Cheng, Andy
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#399630" title="Click to go to the Author Index">
             Doan, Tran
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#397114" title="Click to go to the Author Index">
             Adams, Emma
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#149191" title="Click to go to the Author Index">
             Vázquez, Marynel
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#106996" title="Click to go to the Author Index">
             Scassellati, Brian
            </a>
            (Yale)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab169" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To enable sophisticated interactions between humans and robots in a shared environment, robots must infer the intentions and strategies of their human counterparts. This inference can provide a competitive edge to the robot or enhance human-robot collaboration by reducing the necessity for explicit communication about task decisions. In this work, we identify specific states within the shared environment, which we refer to as Critical Decision Points, where the actions of a human would be especially indicative of their high-level strategy. A robot can significantly reduce uncertainty regarding the human's strategy by observing actions at these points. To demonstrate the practical value of Critical Decision Points, we propose a Receding Horizon Planning (RHP) approach for the robot to influence the movement of a human opponent in a competitive game of hide-and-seek in a partially observable setting. The human plays as the hider and the robot plays as the seeker. We show that the seeker can influence the hider to move towards Critical Decision Points, and this can facilitate a more accurate estimation of the hider's strategy. In turn, this helps the seeker catch the hider faster than estimating the hider's strategy whenever the hider is visible or when the seeker only optimizes for minimizing its distance to the hider.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="tudt6_04">
             17:55-18:10, Paper TuDT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('346'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Human Stress Response and Perceived Safety During Encounters with Quadruped Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#268434" title="Click to go to the Author Index">
             Gupta, Ryan
            </a>
            (University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#399871" title="Click to go to the Author Index">
             Shin, Hyonyoung
            </a>
            (The University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#399866" title="Click to go to the Author Index">
             Norman, Emily
            </a>
            (University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#399855" title="Click to go to the Author Index">
             Stephens, Keri
            </a>
            (The University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#342466" title="Click to go to the Author Index">
             Lu, Nanshu
            </a>
            (The University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#110037" title="Click to go to the Author Index">
             Sentis, Luis
            </a>
            (The University of Texas at Austin)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab346" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Despite the rise of mobile robot deployments in home and work settings, perceived safety of users and bystanders is understudied in the human-robot interaction (HRI) literature. To address this, we present a study designed to identify elements of a human-robot encounter that correlate with observed stress response. Stress is a key component of perceived safety and is strongly associated with human physiological response. In this study a Boston Dynamics Spot and a Unitree Go1 navigate autonomously through a shared environment occupied by human participants wearing multimodal physiological sensors to track their electrocardiography (ECG) and electrodermal activity (EDA). The encounters are varied through several trials and participants self-rate their stress levels after each encounter. The study resulted in a multidimensional dataset archiving various objective and subjective aspects of a human-robot encounter, containing insights for understanding perceived safety in such encounters. To this end, acute stress responses were decoded from the human participants' ECG and EDA and compared across different human-robot encounter conditions. Statistical analysis of data indicate that on average (1) participants feel more stress during encounters compared to baselines, (2) participants feel more stress encountering multiple robots compared to a single robot and (3) participants stress increases during navigation behavior compared with search behavior.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="turc">
             <b>
              TuRC
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#turc" title="Click to go to the Program at a Glance">
             <b>
              Robot Competition - Prep
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
        </div>
        <p>
         <br/>
        </p>
        <p>
         <br/>
        </p>
       </td>
       <td height="100%" style="background-color:#000;" width="5">
       </td>
      </tr>
     </table>
     <table border="0" cellpadding="0" cellspacing="0" height="80%" width="100%">
      <tr>
       <td alt="" border="0" height="8" style="background-color:#000;" valign="center" width="100%">
        <p align="center">
         <span style="font-size:8pt;line-height:10pt;color:white;">
          Technical Content ©
IEEE Robotics &amp; Automation Society
         </span>
        </p>
       </td>
      </tr>
      <tr>
       <td width="100%">
        <p align="right">
         <span style="text-decoration:none;">
          <img align="right" border="0" src="/images/pc_logo_small.png" style="margin-left: 10px; margin-right: 10px"/>
          This site is protected
by copyright and trademark laws under US and International law.
          <br/>
          All rights
reserved. © 2002-2024 PaperCept, Inc.
          <br/>
          Page generated 2024-07-31  01:22:01 PST
          <a href="" onclick="window.open('/conferences/scripts/about.pl','tc','width=1000,scrollbars=yes'); return false">
           Terms
of use
          </a>
         </span>
        </p>
       </td>
      </tr>
     </table>
    </body>
   </div>
  </form>
 </body>
</html>
