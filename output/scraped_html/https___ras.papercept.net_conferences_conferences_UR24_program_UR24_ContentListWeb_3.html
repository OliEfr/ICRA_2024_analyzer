<!DOCTYPE HTML>
<html>
 <head>
  <meta content="en-us" http-equiv="Content-Language"/>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="width=device-width" name="viewport"/>
  <script src="https://ras.papercept.net/conferences/scripts/dom-drag.js" type="text/javascript">
  </script>
  <script src="jquery-1.11.1.min.js">
  </script>
  <title>
   UR 2024 Program | Wednesday June 26, 2024
  </title>
  <style type="text/css">
   body, table, td, th{
	Font-Family : sans-serif;
	Font-Size : 10pt;
}
.r {text-align: right}
.blue {color: #0000FF;}
td {vertical-align: top; text-align: left}
.c {text-align: center}
table.s {
	border-collapse:collapse;
	border-width: 1px;
}
table.s td{
	border-width: 1px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
table.t {
	border-collapse: collapse;
	border-width: 0px;
}
table.t td{
	border-width: 0px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
.dots {
    background:url('./images/dot.gif') repeat-x center;
}
.field {
    background-color: #FFFFFF;
}
#pTitle { /* Page title */
   font-size: 14pt;
   line-height: 1.5em;
}
#pSubTitle { /* Page subtitle */
   color: #909090;
   font-size: 10pt; 
   line-height: 1.5em;
}
#container {
	position: absolute;
	width: 100%;
	margin-top: 2px;
/*	overflow: hidden; */
}

.sHdr {   /* Session header Content list */
   background-color: #F0E68C
}
      
.sSHdr {   /* Subsession header Content list */
   background-color: #f8f3c6 
}
      
table.trk { /* Track table Content list */
   border-collapse: collapse;
   border-width: 0px;
   margin: auto;
/**   width: 640px; **/
   width: 720px;
}
table.trk td{
   border-width: 0px;
   padding: 4px;
   border-style: solid;
   border-color: gray;
 }
      
.pHdr {  /* Paper header Content list */
   background-color: #E6E6FA;
   color: black;
}
hr.thin { /* Horizontal rule content list */
   border: 0px; 
   height: .8px; 
   background-color: #8888FF;
}
      
.pTtl {  /* Paper title Content list */
   font-size: 11pt;
   font-style: italic;
}
      
.ssHdr {  /* Subsession header container session Content list */
   background-color: #DDDDDD;
   color: black;
}
      
.ssTtl {  /* Subsession title container session Content list */
   font-size: 10pt;
   font-style: normal;
   font-weight: bold;
}
  </style>
  <script language="JavaScript">
   function initXMLHttp(){
   var oRequest = false;
   try {
      oRequest = new XMLHttpRequest();
   }  catch (trymicrosoft) {
      try {
         oRequest = new ActiveXObject("Msxml2.XMLHTTP");
      }  catch (othermicrosoft) {
         try {
            oRequest = new ActiveXObject("Microsoft.XMLHTTP");
         }  catch (failed) {
            oRequest = false;
         }
      }
   }
   if (!oRequest){
      alert("Error initializing XMLHttpRequest! Your browser does not support AJAX");
   }
   return oRequest;
}
function modify(number,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'Add';
   }
   else{
      action = 'Delete';
   }
   
//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=445&' + action + number;
//   window.open(url,'myprogrampage');

   modifyItem("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","445",action,number)

}


function modifyItem(url,ConfID,action,number){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&Number=' + number;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

var iIntervalId;  // Global variable
function modsession(id,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'AddSession';
   }
   else{
      action = 'DelSession';
   }

//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=445&' + action + id;
//   window.open(url,'myprogrampage');

   modifySession("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","445",action,id)

}

function modifySession(url,ConfID,action,id){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&ID=' + id;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

function getCookie(sName){
   var sRE = "(?:; )?" + sName + "=([^;]*);?";   
   var oRE = new RegExp(sRE);
   if (oRE.test(document.cookie)){
      return decodeURIComponent(RegExp["$1"]);}
   else{
      return null;
   }
}
function loadprogram(){
   var list = getCookie("UR24");
   if (list){
      var List = list.split(",");
      for (var i=0; i<List.length; i++){
         var names = document.getElementsByName('modify' + List[i]);
         if (names.length){
            for (var j=0; j<names.length; j++){
               names[j].checked = true;
            }
         }
      }
   }
}
function reset(){

   // Uncheck all modify and addsession checkboxes

   var ins = document.getElementsByTagName('input');
   for (var i=0; i<ins.length; i++){
      if (ins[i].type == 'checkbox' && ins[i].id && ins[i].id.substring(0,3) == 'mod'){
         ins[i].checked = false;
      }
   }
   
   // Reload the program
   
   loadprogram();
}
function startreset(){
   iIntervalId = setInterval(reset,2000);
}
function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
var uhash;
var pColor;
$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){
   
      // Mark the session
   
      pColor = $('#' + uhash).parent().css('backgroundColor');
      $('#' + uhash).parent().css('backgroundColor','#FF8888');
   }
});
  </script>
 </head>
 <body onresize="setwidth(); initialize()">
  <form action="https://ras.papercept.net/conferences/scripts/myprogram.pl" name="myprogram">
   <div id="container">
    <body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0">
     <table border="0" cellpadding="0" cellspacing="0" width="100%">
      <tr>
       <td alt="" height="140" style="background-color:#2E5286;" width="100%">
        <img alt="" border="0" height="140px" src="/images/ur/ur24.webp" style="position: absolute; margin: 0; left:0px;top:0px;"/>
       </td>
      </tr>
      <tr>
       <td height="0px" style="background-color:#2E5286;" width="100%">
       </td>
      </tr>
     </table>
     <table border="0" cellpadding="0" cellspacing="0" height="80%" width="100%">
      <tr>
       <td height="100%" style="background-color:#2E5286;" width="5">
       </td>
       <td width="5">
       </td>
       <td height="100%" valign="top" width="100%">
        <br/>
        <div class="c" id="TheTop">
         <span id="pTitle">
          <a href="http://2024.ubiquitousrobots.org" target="_blank">
           <b>
            2024 21st International Conference on Ubiquitous Robots (UR)
           </b>
          </a>
          <br/>
         </span>
         <span id="pSubTitle">
          <b>
           June 24-27, 2024, NYU, Manhattan, New York, USA
          </b>
         </span>
         <br/>
         <br/>
        </div>
        <div class="c" style="position: relative">
         <a href="UR24_ProgramAtAGlanceWeb.html">
          Program at a Glance
         </a>
         <a href="UR24_ContentListWeb_1.html">
          Monday
         </a>
         <a href="UR24_ContentListWeb_2.html">
          Tuesday
         </a>
         <a href="UR24_ContentListWeb_3.html">
          Wednesday
         </a>
         <a href="UR24_AuthorIndexWeb.html">
          Author Index
         </a>
         <a href="UR24_KeywordIndexWeb.html">
          Keyword Index
         </a>
        </div>
        <div class="c">
         <p style="color: gray">
          Last updated on July 1, 2024. This conference program is tentative and subject to change
         </p>
        </div>
        <div class="c">
         <h3>
          Technical Program for Wednesday June 26, 2024
         </h3>
        </div>
        <p class="c">
        </p>
        <div class="c">
         <span style="color:gray ">
          To show or hide the keywords and abstract (text summary) of a paper (if available), click on the paper title
         </span>
         <br/>
         <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">
          Open all abstracts
         </a>
         <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">
          Close all abstracts
         </a>
        </div>
        <div class="c">
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo2a">
             <b>
              WO2A
             </b>
            </a>
           </td>
           <td class="r">
            Rosenthal
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo2a" title="Click to go to the Program at a Glance">
             <b>
              Manipulation Planning and Control
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#202382" title="Click to go to the Author Index">
             Lin, Xuan
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#143938" title="Click to go to the Author Index">
             Pyo, Dongbum
            </a>
           </td>
           <td class="r">
            Korea Institute of Industrial Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2a_01">
             10:30-10:40, Paper WO2A.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Evaluating Data-Driven Performances of Mixed Integer Bilinear Formulations for Book Placement Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#202382" title="Click to go to the Author Index">
             Lin, Xuan
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#237912" title="Click to go to the Author Index">
             Fernandez, Gabriel Ikaika
            </a>
           </td>
           <td class="r">
            University of California Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#106488" title="Click to go to the Author Index">
             Hong, Dennis
            </a>
           </td>
           <td class="r">
            UCLA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mixed integer bilinear programs (MIBLPs) offer tools to resolve robotics motion planning problems with orthogonal rotation matrices or static moment balance, but require long solving times. Recent work utilizing data-driven methods has shown potential to overcome this issue allowing for applications on larger scale problems. To solve mixed-integer bilinear programs online with data-driven methods, several re-formulations exist including mathematical programming with complementary constraints (MPCC), and mixed-integer programming (MIP). In this work, we compare the data-driven performances of various MIBLP reformulations using a book placement problem that has discrete configuration switches and bilinear constraints. The success rate, cost, and solving time are compared along with non-data-driven methods. Our results demonstrate the advantage of using data-driven methods to accelerate the solving speed of MIBLPs, and provide references for users to choose the suitable re-formulation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2a_02">
             10:40-10:50, Paper WO2A.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('52'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              M2CURL: Sample-Efficient Multimodal Reinforcement Learning Via Self-Supervised Representation Learning for Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#327051" title="Click to go to the Author Index">
             Lygerakis, Fotios
            </a>
           </td>
           <td class="r">
            Montanuniversitaet Leoben
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#311294" title="Click to go to the Author Index">
             Dave, Vedant
            </a>
           </td>
           <td class="r">
            Montanuniversitaet Leoben
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#176888" title="Click to go to the Author Index">
             Rueckert, Elmar
            </a>
           </td>
           <td class="r">
            Montanuniversitaet Leoben
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab52" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#ai_reasoning_methods_for_robotics" title="Click to go to the Keyword Index">
               AI Reasoning Methods for Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             One of the most critical aspects of multimodal Reinforcement Learning (RL) is the effective integration of different observation modalities. Having robust and accurate representations derived from these modalities is key to enhancing the robustness and sample efficiency of RL algorithms. However, learning representations in RL settings for visuotactile data poses significant challenges, particularly due to the high dimensionality of the data and the complexity involved in correlating visual and tactile inputs with the dynamic environment and task objectives. To address these challenges, we propose Multimodal Contrastive Unsupervised Reinforcement Learning (M2CURL). Our approach employs a novel multimodal self-supervised learning technique that learns efficient representations and contributes to faster convergence of RL algorithms. Our method is agnostic to the RL algorithm, thus enabling its integration with any available RL algorithm. We evaluate M2CURL on the Tactile Gym 2 simulator and we show that it significantly enhances the learning efficiency in different manipulation tasks. This is evidenced by faster convergence rates and higher cumulative rewards per episode, compared to standard RL algorithms without our representation learning approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2a_03">
             10:50-11:00, Paper WO2A.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('114'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Adaptive Framework for Manipulator Skill Reproduction in Dynamic Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#367297" title="Click to go to the Author Index">
             Donald, Ryan
            </a>
           </td>
           <td class="r">
            University of Massachusetts, Lowell
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#283495" title="Click to go to the Author Index">
             Hertel, Brendan
            </a>
           </td>
           <td class="r">
            University of Masssachusetts Lowell
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#367302" title="Click to go to the Author Index">
             Misenti, Stephen
            </a>
           </td>
           <td class="r">
            UMass Lowell
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#165030" title="Click to go to the Author Index">
             Gu, Yan
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#160355" title="Click to go to the Author Index">
             Azadeh, Reza
            </a>
           </td>
           <td class="r">
            University of Massachusetts Lowell
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab114" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#robotics_in_hazardous_applications" title="Click to go to the Keyword Index">
               Robotics in Hazardous Applications
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot skill learning and execution in uncertain and dynamic environments is a challenging task. This paper proposes an adaptive framework that combines Learning from Demonstration (LfD), environment state prediction, and high-level decision making. Proactive adaptation prevents the need for reactive adaptation, which lags behind changes in the environment rather than anticipating them. We propose a novel LfD representation, Elastic-Laplacian Trajectory Editing (ELTE), which continuously adapts the trajectory shape to predictions of future states. Then, a high-level reactive system using an Unscented Kalman Filter (UKF) and Hidden Markov Model (HMM) prevents unsafe execution in the current state of the dynamic environment based on a discrete set of decisions. We first validate our LfD representation in simulation, then experimentally assess the entire framework using a legged mobile manipulator in 36 real-world scenarios. We show the effectiveness of the proposed framework under different dynamic changes in the environment. Our results show that the proposed framework produces robust and stable adaptive behaviors.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2a_04">
             11:00-11:10, Paper WO2A.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('127'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              PDCC: Peg-In-Hole Framework Via Dynamic Continuous Contact Data
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#274396" title="Click to go to the Author Index">
             Lee, Joosoon
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#274378" title="Click to go to the Author Index">
             Lee, Geonhyup
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#100825" title="Click to go to the Author Index">
             Lee, Kyoobin
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab127" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#contact__modeling__sensing_and_control_" title="Click to go to the Keyword Index">
               Contact: Modeling, Sensing and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In robotic peg-in-hole assembly, contact dynamics provide critical information for precise control. This study introduces a novel peg-in-hole framework for a dynamic continuous contact environment using a supervised learning-based peg-hole extrinsic pose estimation model. The transformer-based model is trained on contact data sequences, generated in simulation with various shapes. With a single model, pose estimation performance approached 3.62mm and 1.68° for position XY and orientation XYZ, respectively. The proposed framework demonstrates effective peg-in-hole performance across 12 shapes with 5 degrees of freedom, outperforming conventional search methodologies in accuracy and efficiency, over 1.8 and 5.6 times better respectively. The study highlights the potential in dynamic continuous contact situations, contributing to precise assembly in dynamic environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2a_05">
             11:10-11:20, Paper WO2A.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('190'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Study on Peg-In-Hole Insertion Based on Misalignment Error Estimation Network
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#361730" title="Click to go to the Author Index">
             Cho, Taeyeop
            </a>
           </td>
           <td class="r">
            Hanyang University, KITECH
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#242735" title="Click to go to the Author Index">
             Kim, Jinseok
            </a>
           </td>
           <td class="r">
            UST, KITECH
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#273535" title="Click to go to the Author Index">
             Choi, Iksu
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University, KITECH
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#143938" title="Click to go to the Author Index">
             Pyo, Dongbum
            </a>
           </td>
           <td class="r">
            Korea Institute of Industrial Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab190" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ongoing research is being conducted on the autonomous operation of robots for manufacturing automation. However, it faces challenges due to uncertainty in unstructured environments. Particularly, tasks with frequent contact, such as peg-in-hole assembly, pose a risk of hazardous forces on the robot system due to uncertainty-induced collisions. To address this, we propose a regression learning model to infer the orientation error angles of the peg and hole based on contact data. We demonstrate its effectiveness in overcoming jamming caused by misalignment in peg-in-hole insertion by integrating it with a traditional parallel force/position controller and a learned misalignment error estimation network (MEN). Experimental results show that the system integrated with MEN achieves a 100% insertion success rate and a faster average duration compared to the single system, demonstrating stable peg-in-hole task without jamming.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo2b">
             <b>
              WO2B
             </b>
            </a>
           </td>
           <td class="r">
            KC 905
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo2b" title="Click to go to the Program at a Glance">
             <b>
              Socially Assistive Robotics
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#116054" title="Click to go to the Author Index">
             Lee, Hui Sung
            </a>
           </td>
           <td class="r">
            UNIST (Ulsan National Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#275314" title="Click to go to the Author Index">
             Xie, Zhen
            </a>
           </td>
           <td class="r">
            Agency for Science, Technology and Research
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2b_01">
             10:30-10:40, Paper WO2B.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('48'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Deep Reinforcement Learning Based Mobile Robot Navigation in Crowd Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#268801" title="Click to go to the Author Index">
             YANG, Guang
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#105621" title="Click to go to the Author Index">
             Guo, Yi
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab48" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#learning_from_humans" title="Click to go to the Keyword Index">
               Learning From Humans
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots are becoming popular in assisting humans. The mobile robot navigation in human crowd environments has become more important. We propose a deep reinforcement learning-based mobile robot navigation method that takes the observation from the robot's onboard Lidar sensor as input and outputs the velocity control to the robot. A customized deep deterministic policy gradient (DDPG) method is developed that incorporates guiding points to guide the robot toward the global goal. We built a 3D simulation environment using an open dataset of real-world pedestrian trajectories that were collected in a large business center. The neural network models are trained and tested in such environments. We compare the performance of our proposed method with existing algorithms that include a classic motion planner, an existing DDPG method, and a generative adversarial imitation learning (GAIL) method. Using the measurement metrics of success rate, freezing times, and normalized path length, extensive simulation results show that our method outperforms other state-of-the-art approaches in both trained and untrained environments. Our method has also better generalizability compared with the GAIL method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2b_02">
             10:40-10:50, Paper WO2B.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('21'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Book-Toki: A Rabbit-Shaped Reading Companion Robot That Enhances Children's Reading Concentration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386637" title="Click to go to the Author Index">
             Lee, Dabin
            </a>
           </td>
           <td class="r">
            Ulsan National Institue of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#276443" title="Click to go to the Author Index">
             Park, Haeun
            </a>
           </td>
           <td class="r">
            Ulsan National Institue of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#116054" title="Click to go to the Author Index">
             Lee, Hui Sung
            </a>
           </td>
           <td class="r">
            UNIST (Ulsan National Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab21" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#biomimetic_and_bioinspired_robots" title="Click to go to the Keyword Index">
               Biomimetic and Bioinspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Reading books is a fundamental cornerstone in the formation of young minds, and it influences various facets of children's lives. Recent technological advances have led to extensive research into the potential role of robots as companions in children's reading experiences. This study explores how to engage children in reading through human-robot interaction, deriving three design specifications: a robot with an animal-like appearance; a robot that responds as a real organism; and a robot that reacts based on reading status. An interactive reading companion robot named Book-Toki was developed, which has the appearance of a rabbit. The robot's ears, made of deformable silicone, and the robot's head move in response to the child's reading. As the child reads the book aloud, the ears change shape from folded to unfolded, and the head slowly peeks out of the burrow, symbolizing active concentration. This visual response sparks children's imagination and gives them a sense of 'I am listening to you,' which enriches the reading experience. In this way, robots can play a significant role in making reading a more enjoyable experience for children.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2b_03">
             10:50-11:00, Paper WO2B.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('34'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Value of Specific, Expansive Imaginary Scenarios: An Exploration of Recent Science Fiction Literature through the Lens of Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#240505" title="Click to go to the Author Index">
             Indurkhya, Xela
            </a>
           </td>
           <td class="r">
            Tokyo University of Agriculture and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#213740" title="Click to go to the Author Index">
             Indurkhya, Bipin
            </a>
           </td>
           <td class="r">
            Jagiellonian University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#100628" title="Click to go to the Author Index">
             Venture, Gentiane
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab34" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#roboethics" title="Click to go to the Keyword Index">
               Roboethics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Science fiction has a great influence on how we view and develop technology, including robots. However, the science fiction we often cite in these discussions are visual: films and TV shows. While those stories are crucial to the discussion, we seek to expand and shift the conversation around science fiction to include more literature, especially recent literature. We highlight some stories, principally books released in the last decade, and the ways they might be relevant to roboticists today in how they portray the themes of empathy, embodiment, and the place of robots in a human world.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2b_04">
             11:00-11:10, Paper WO2B.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('162'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Power of Atmosphere: LLM-Based Social Task Generation of Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388461" title="Click to go to the Author Index">
             Lee, Hanna
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence (KIRO)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388462" title="Click to go to the Author Index">
             LYM, Hyo Jeong
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence (KIRO)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#242487" title="Click to go to the Author Index">
             Kim, Da-Young
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Technology Convergence(KIRO)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#158370" title="Click to go to the Author Index">
             Kim, Min-Gyu
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab162" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#robotic_systems_architectures_and_programming" title="Click to go to the Keyword Index">
               Robotic Systems Architectures and Programming
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In Human-Robot Interaction (HRI), the ability of robots to understand social atmosphere is essential for the quality improvement of the interaction. However, robots have limitations in recognizing and performing appropriate behaviors for social atmosphere that are not externally revealed. In this study, based on Large Language Models (LLMs), we aimed to investigate whether adding atmosphere elements to robots' social behavior generation could enable robots to generate and perform more appropriate behaviors in social contexts. A total of 50 participants participated in the experiment. As a result of the experiment, a robot scenario that incorporates atmospheric elements showed significantly higher differences in terms of the robot's social behavior, sociability, and human-robot interaction, compared to a robot scenario without atmospheric elements. This study provides new insights into the importance of considering social context when setting prompting elements for LLM-based social robots. By adding social atmospheric elements, it is expected that robots will be able to improve human’s understanding of social context and perform more natural and effective interactions.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo2c">
             <b>
              WO2C
             </b>
            </a>
           </td>
           <td class="r">
            KC 907
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo2c" title="Click to go to the Program at a Glance">
             <b>
              Multisensor Data Fusion
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#231831" title="Click to go to the Author Index">
             Yumbla, Francisco
            </a>
           </td>
           <td class="r">
            ESPOL Polytechnic University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#223911" title="Click to go to the Author Index">
             Hong, Seonghun
            </a>
           </td>
           <td class="r">
            Keimyung University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2c_01">
             10:30-10:40, Paper WO2C.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('39'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Robot Cooperative Localization with Single UWB Error Correction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#358827" title="Click to go to the Author Index">
             Marsim, Kevin Christiansen
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#269372" title="Click to go to the Author Index">
             Choi, Junho
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#286867" title="Click to go to the Author Index">
             Jeong, Myeongwoo
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386877" title="Click to go to the Author Index">
             Ryoo, Kihwan
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386912" title="Click to go to the Author Index">
             Kim, Jeewon
            </a>
           </td>
           <td class="r">
            School of Electrical Engineering, KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386899" title="Click to go to the Author Index">
             Kim, Taeyun
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#104490" title="Click to go to the Author Index">
             Myung, Hyun
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab39" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deployment of multiple robots in real-world scenarios requires simultaneous information exchange from all platforms to ensure effective task performance. The robot’s relative position to its peers is an important data needed to predict collision between robots or task distribution. This paper introduces a robust and simple method for achieving cooperative localization among multiple robots, utilizing a single ultra-wideband (UWB) sensor for each platform. Each robot uses a visual-inertial odometry (VIO) system to track its own trajectory. Given the inherent drift associated with VIO systems, we leverage UWB data to estimate and correct this drift, enhancing each robot’s localization accuracy. Our approach substantially improves the result compared to other cooperative localization methods and can even correct the VIO ego-motion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2c_02">
             10:40-10:50, Paper WO2C.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('38'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Terrain-Based Place Recognition for Quadruped Robots with Limited Field-Of-View LiDAR
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386696" title="Click to go to the Author Index">
             Lee, Roun
            </a>
           </td>
           <td class="r">
            Keimyung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#223911" title="Click to go to the Author Index">
             Hong, Seonghun
            </a>
           </td>
           <td class="r">
            Keimyung University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#405551" title="Click to go to the Author Index">
             Yoon, Sukmin
            </a>
           </td>
           <td class="r">
            Hanwha Systems
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab38" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Scientific and engineering applications of solid-state light detection and ranging (LiDAR) sensors with no rotating mechanisms and a limited field of view have attracted research attention in recent years because of their cost-effectiveness and durability. However, it is challenging to perform place recognition, which is one of the most important problems in simultaneous localization and mapping (SLAM), using limited field-of-view measurements. Considering a terrestrial SLAM framework for quadruped robots with limited field-of-view LiDAR sensors, this study proposes a terrain-based place recognition algorithm that uses a set of foot contact information for quadruped robots. The practical feasibility of the proposed approach is demonstrated through experimental results using a quadruped robot system with a limited field-of-view LiDAR sensor.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2c_03">
             10:50-11:00, Paper WO2C.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Quantifying the Accuracy of Collaborative IoT and Robot Sensing in Indoor Settings of Rigid Objects
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#332505" title="Click to go to the Author Index">
             Sørensen, Sune Lundø
            </a>
           </td>
           <td class="r">
            University of Southern Denmark
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#296610" title="Click to go to the Author Index">
             Mikkel, Kjærgaard
            </a>
           </td>
           <td class="r">
            University of Southern Denmark
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#world_modelling" title="Click to go to the Keyword Index">
               World Modelling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Perceptual anchoring traditionally relies on data from sensors mounted on a mobile robot. This allows the sensors to be close to objects in the environment, making it possible to acquire details with high accuracy. IoT sensors are becoming more and more ubiquitous, and are found in both private and public buildings. IoT cameras are often mounted on ceilings or walls, allowing them to observe a larger part of the environment than robot-mounted sensors, but making them unsuitable for acquiring detailed visual information. They often have a lower sampling rate, keeping them cost-effective. We hypothesize that IoT and robot sensors can be combined in a way that exploits the details of the robot sensors and the immediately high overview of the IoT sensors by embracing ubiquitous sensing. In this work, we evaluate and compare different methods for associating IoT and robot sensing data, including a novel context-based similarity measure and a simple geometric baseline. The results support our hypothesis and we find that all methods outperform the baseline method in most scenarios. Using context-similarity is most beneficial for the affinity propagation clustering algorithm for setups with 16 and 12 objects. These results can serve as a guideline for designing anchoring or world modeling systems, using IoT and robot sensing data.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2c_04">
             11:00-11:10, Paper WO2C.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('145'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MSCKF-DVIO: Multi-State Constraint Kalman Filter Based RGB-D Visual-Inertial Odometry with Spline Interpolation and Nonholonomic Constraint
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387036" title="Click to go to the Author Index">
             Jung, KwangYik
            </a>
           </td>
           <td class="r">
            TWINNY
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387057" title="Click to go to the Author Index">
             Song, Jaebong
            </a>
           </td>
           <td class="r">
            TWINNY Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387059" title="Click to go to the Author Index">
             Seong, Samwoo
            </a>
           </td>
           <td class="r">
            Twinny
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#104490" title="Click to go to the Author Index">
             Myung, Hyun
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab145" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study presents MSCKF-DVIO (Multi-State Constraint Kalman Filter - Depth-aided Visual Inertial Odometry) as an innovative approach to address the limitations of existing methods. MSCKF-DVIO leverages RGB-D images and low-cost IMU measurements to enhance the accuracy and efficiency of visual-inertial odometry systems. The proposed framework jointly optimizes RGB-D images and low-cost IMU measurements, enabling robust and precise state estimation.
             <p>
              By reducing the number of state variables compared to the existing MSCKF-VIO, the efficiency of state augmentation and covariance computations is significantly enhanced. Notably, the prediction of the future state is achieved through the interpolation of past keyframe viewpoint poses from the image and short-time IMU integration. The proposed MSCKF-DVIO improves the accuracy of estimation by applying nonholonomic constraints (NHCs) of a differential-wheeled mobile robot and utilizing the zero velocity update (ZUPT) based on stationary state determination. The efficacy of MSCKF-DVIO is evaluated through the Absolute Pose Error (APE) using 3D LiDAR-based localization as the ground truth. The evaluation includes pose estimation outcomes and performance evaluations in comparison to other algorithms. The results demonstrate significant promise of MSCKF-DVIO for improving the performance and reliability of visual-inertial odometry systems.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2c_05">
             11:10-11:20, Paper WO2C.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('47'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring Image Fusion Techniques for Off-Road Semantic Segmentation in Harsh Lighting Conditions. a Multispectral Imagery Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#324602" title="Click to go to the Author Index">
             deoli, Pankaj
            </a>
           </td>
           <td class="r">
            Technical University of Kaiserslautern
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386971" title="Click to go to the Author Index">
             deshpande, shubham
            </a>
           </td>
           <td class="r">
            University of Kaiserslautern-Landau
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#245419" title="Click to go to the Author Index">
             Vierling, Axel
            </a>
           </td>
           <td class="r">
            TU Kaiserslautern
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#255546" title="Click to go to the Author Index">
             Berns, Karsten
            </a>
           </td>
           <td class="r">
            University of Kaiserslautern
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab47" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#robotics_in_hazardous_applications" title="Click to go to the Keyword Index">
               Robotics in Hazardous Applications
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, we have witnessed significant progress in the field of autonomous mobility. However, these advancements have been highly limited to urban environments. Autonomous mobility in off-road environments is more challenging because of diverse environments, illumination conditions, lack of distinct features, among others. This paper delves into the issue of semantic segmentation for off-road conditions, focusing on the fusion of RGB (Red-Green-Blue) &amp; NIR (Near Infrared) under intense lighting conditions. Given the huge variability associated with off-road environments, the existing datasets fail to capture strong illumination variations in the environment and therefore, to address this, we present "RPTU-Forest dataset" with 285 RGB images along with their respective multi-spectral images including (RGB, NIR, GREEN, RED and Red Edge (REG)) channel images. The paper explores the different fusion approaches for multispectral semantic segmentation documented in literature such as concatenation, Variational autoencoders, dual branch and DenseFuse fusion and provides a thorough analysis of each approach for the task of semantic segmentation. Two state-of-the-art semantic segmentation networks (UNet &amp; DeepLab V3) are additionally compared for our use-case and the best among them is selected. The paper concludes with a qualitative and quantitative analysis. This work represents a significant contribution to the ongoing research in off-road autonomous mobility. The code is publicly available at (https://github.com/ShubhamAbhayDeshpande/RobustSemanticSegmentationWithSensorFusion)
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2c_06">
             11:20-11:30, Paper WO2C.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('141'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards an Intuitive Virtual Reality Interface Using Cable-Driven Parallel Robots for Space Exploration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#317470" title="Click to go to the Author Index">
             Kassai, Nathan
            </a>
           </td>
           <td class="r">
            University of Nevada, Las Vegas
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#317471" title="Click to go to the Author Index">
             Castrejon, Zahir
            </a>
           </td>
           <td class="r">
            University of Nevada Las Vegas
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#100181" title="Click to go to the Author Index">
             Oh, Paul Y.
            </a>
           </td>
           <td class="r">
            University of Nevada, Las Vegas (UNLV)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab141" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#telerobotics" title="Click to go to the Keyword Index">
               Telerobotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Space Exploration is a continuously flourishing field of research, as NASA has a plethora of ongoing missions to be achieved over the next few years. With the advent of many robotic platforms dedicated for space exploration such as NASA's Dragonfly, their Mars Perserverance Rover, and many more, it is evident that these types of robots will continue to play a key role. Despite their success, the limited man power for such specialized operators, reliability concerns with Unmanned Aerial Vehicles (UAVs or drones) in such harsh environments, and the limited battery life justify the consideration of different approaches. This paper presents work towards a suspended Cable-Driven Parallel Robot (CDPR), paired with an intuitive Virtual Reality interface designed for space exploration. Real-time 3D Point Cloud visualization can potentially grant the operator a greater sense of immersion, and can allow any operator to view the environment around the CDPR. Along with the benefits of a CDPR, an immersive VR interface gives operators intuitive control through rigorous tasks.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo2d">
             <b>
              WO2D
             </b>
            </a>
           </td>
           <td class="r">
            KC 909
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo2d" title="Click to go to the Program at a Glance">
             <b>
              Object Recognition
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#103260" title="Click to go to the Author Index">
             Martinson, Eric
            </a>
           </td>
           <td class="r">
            Lawrence Technological University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#102633" title="Click to go to the Author Index">
             Lim, Gi Hyun
            </a>
           </td>
           <td class="r">
            Wonkwang University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2d_01">
             10:30-10:40, Paper WO2D.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('4'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Deep Learning-Based Wildfire Smoke Detection Using Uncrewed Aircraft System Imagery
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#385404" title="Click to go to the Author Index">
             Mahmud, Khan Raqib
            </a>
           </td>
           <td class="r">
            Louisiana Tech University, Ruston, LA 71272
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#270151" title="Click to go to the Author Index">
             Wang, Lingxiao
            </a>
           </td>
           <td class="r">
            Louisiana Tech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#385516" title="Click to go to the Author Index">
             Liu, Xiyuan
            </a>
           </td>
           <td class="r">
            Louisiana Tech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#385483" title="Click to go to the Author Index">
             Li, Jiahao
            </a>
           </td>
           <td class="r">
            Louisiana Tech University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#278824" title="Click to go to the Author Index">
             Hassan, Sunzid
            </a>
           </td>
           <td class="r">
            Louisiana Tech University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab4" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent years have seen notable advancements in wildfire smoke detection, particularly in Uncrewed Aircraft Systems (UAS)-based detection employing diverse deep learning (DL) approaches. Despite the promise exhibited by these approaches, the task of detecting smoke from UAS imagery remains challenging due to difficulties in differentiating smoke from similar phenomena such as clouds and water. This work introduces a novel DL-based method for smoke detection from UAS visual observations. The core idea involves segregating forest areas from non-forest regions, such as sky and lake, and exclusively applying smoke detection to forested areas, thus eliminating the chance of misidentifying clouds and water as smoke. Specifically, we utilized a Mask Region-Based Convolutional Neural Network (Mask R-CNN) for semantic segmentation to remove non-forest regions (e.g., sky and lake): Subsequently, a customized You Only Look Once-version 7 (YOLOv7) model was trained to detect smoke within the forest areas. The proposed method was validated on an image dataset collected from our previous prescribed burn experiment, where we extracted 246 images to train both MASK R-CNN and YOLOv7 models. Additionally, we extract another 128 images to validate and confirm the efficacy of our enhanced wildfire smoke detection approach. The test results demonstrate that our proposed approach, employing MASK R-CNN and YOLOv7 models, outperforms the YOLOv7-only model by 25.3% in precision, 18.7% in recall, and 45% in mean Average Precision (mAP). The datasets are available at: https://github. com/khanRmahmud/wildfire-smoke-detection.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2d_02">
             10:40-10:50, Paper WO2D.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('43'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Interactive, Privacy-Aware Semantic Mapping for Homes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#103260" title="Click to go to the Author Index">
             Martinson, Eric
            </a>
           </td>
           <td class="r">
            Lawrence Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#272001" title="Click to go to the Author Index">
             Alladkani, Fadi
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab43" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#roboethics" title="Click to go to the Keyword Index">
               Roboethics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#world_modelling" title="Click to go to the Keyword Index">
               World Modelling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Semantic mapping is computationally expensive, requiring either large GPUs on the robot, or significant numbers of uploaded images to the cloud. Neither solution is appropriate for home robots, where the hardware must be inexpensive, and privacy is a real concern. Instead of resorting fully to hand-labeled maps to address privacy concerns, where label noise can be a big problem depending on the quality of the input interface, we propose an interactive solution integrating hand drawn boxes with robot exploration data. Specifically, nonlinear optimization is conducted on each user-submitted proposal based on the bounding boxes and detection information collected by the robot, generating higher quality estimates quickly for human review as part of an interaction. In this manner, images are processed once on the robot with cost effective algorithms, and then discarded, minimizing the risk of exposing sensitive information. This privacy-aware approach leads to an improvement in map and object quality compared to using hand-labeled maps directly, even when working with user proposals that have up to 50% label noise.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2d_03">
             10:50-11:00, Paper WO2D.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('86'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Comparison of Approaches for Human Detection with Low-Resolution Infrared Data Sets Using Deep Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387101" title="Click to go to the Author Index">
             Läufer, Damian
            </a>
           </td>
           <td class="r">
            Offenburg University of Applied Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387106" title="Click to go to the Author Index">
             Braun, Simone
            </a>
           </td>
           <td class="r">
            Offenburg University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349919" title="Click to go to the Author Index">
             Süme, Sinan
            </a>
           </td>
           <td class="r">
            Hochschule Offenburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#219332" title="Click to go to the Author Index">
             Himmelsbach, Urban B.
            </a>
           </td>
           <td class="r">
            Universtiy of Applied Sciences Offenburg
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab86" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#intelligent_robotic_vehicles" title="Click to go to the Keyword Index">
               Intelligent Robotic Vehicles
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human-machine interaction can be supported by the detection of humans through the simultaneous localization and distinction from non-human objects. This paper compares modern object detection algorithms (Damo-YOLO, YOLOv6, YOLOv7 and YOLOv8) in combination with Transfer Learning and Super Resolution in different scenarios to achieve human detection on low resolution infrared images. The data set created for this purpose includes images of an empty room, images of warm coffee cups, and images of people in various scenarios and at distances ranging from two to six meters. The Average Precision AP@50 and AP@50:95 values achieved across all scenarios reach up to 98.02 % and 66.99% respectively.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2d_04">
             11:00-11:10, Paper WO2D.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('115'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Meaningful Change Detection in Indoor Environments Using CLIP Models and NeRF-Based Image Synthesis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#103260" title="Click to go to the Author Index">
             Martinson, Eric
            </a>
           </td>
           <td class="r">
            Lawrence Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387198" title="Click to go to the Author Index">
             Lauren, Paula
            </a>
           </td>
           <td class="r">
            Lawrence Technological University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab115" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#robot_surveillance_and_security" title="Click to go to the Keyword Index">
               Robot Surveillance and Security
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Security operations are all about detecting change. Looking for out of place or suspicious things or people is the job, so the first step is to learn what is normal and then recognize what is not. Change detection in robotics, however, has focused on the big picture – extracting mask images of new buildings or construction to support autonomous cars, or correcting semantic maps. If we want robots to help patrol a facility, a different type of change detection is required that can be quickly adapted for working with humans to address new security concerns. To this end, we propose a highly dynamic change detection system based on Contrastive Language-Image Pre-Training (CLIP) and Neural Radiance Fields (NeRF). NeRF is used to generate images from the viewpoint that are high quality indoor reconstructions, while CLIP-based segmentation allows a security guard to search for a variety of potential threats using natural language queries. The resulting robotic system is demonstrated to be effective on an office environment with no additional manual annotation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2d_05">
             11:10-11:20, Paper WO2D.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('130'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Semantic Segmentation for Robotic Apple Harvesting: A Deep Learning Approach Leveraging U-Net, Synthetic Data, and Domain Adaptation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387224" title="Click to go to the Author Index">
             Selvaraj, Ghokulji
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#160893" title="Click to go to the Author Index">
             Farzan, Siavash
            </a>
           </td>
           <td class="r">
            California Polytechnic State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab130" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a deep learning-based semantic segmentation framework tailored for robotic apple harvesting, leveraging synthetic data generated within a 3D simulated apple orchard. The proposed simulation environment replicates real-world scenarios, encompassing challenges such as occlusion, variety in apple types, and changes in lighting conditions. This approach eliminates the extensive costs and complexities associated with collecting real-world datasets, particularly in unpredictable agricultural settings. The synthetic dataset, rendered from perspectives consistent with a robotic harvester's camera in the Gazebo physics engine, provides a comprehensive range of scenarios for robust model training. For validation, we deploy U-Net, a fully convolutional neural network, emphasizing its adaptability to domain shifts between synthetic and real-world data. By integrating strategies such as domain adaptation, data augmentation, and the inclusion of pre-trained ResNet-50 encoders in the U-Net framework, we demonstrate superior performance in detecting and segmenting apples in diverse real-world conditions compared to standard U-Net models and traditional computer vision techniques. The results highlight the potential of synthetic data in deep learning-based semantic segmentation, offering a cost-effective and scalable solution when real-world data is limited or hard to collect.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2d_06">
             11:20-11:30, Paper WO2D.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('171'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Labelling a Stereo Event Dataset in Indoor Scenes for Segmentation Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#102633" title="Click to go to the Author Index">
             Lim, Gi Hyun
            </a>
           </td>
           <td class="r">
            Wonkwang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386939" title="Click to go to the Author Index">
             Lee, Se Hyun
            </a>
           </td>
           <td class="r">
            Wonkwang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab171" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#learning_from_humans" title="Click to go to the Keyword Index">
               Learning From Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Lots of well-labelled data are needed to utilize and understand new types of sensors such as event camera systems. To reduce the effort for labelling dataset, we utilized a Swin transformer as a backbone and a transformer decoder with mask attention for segmentation tasks. More and more labelled data has been collected by just selecting well-labelled data and fine-tuning from the collected ones iteratively. Stereo event datasets are being built by non-experts by labelling them via fine-tuning on Swin transformer backbone and a pre-trained transformer decoder. So far one-fifth of the images collected by a traditional camera aligned with the stereo event camera system have been accepted as properly labelled data.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo2e">
             <b>
              WO2E
             </b>
            </a>
           </td>
           <td class="r">
            KC 912
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo2e" title="Click to go to the Program at a Glance">
             <b>
              Humanoid Robots
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#150747" title="Click to go to the Author Index">
             Kim, Joohyung
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#305757" title="Click to go to the Author Index">
             Wen, Lu
            </a>
           </td>
           <td class="r">
            University of Michigan, Ann Arbor
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2e_01">
             10:30-10:40, Paper WO2E.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('68'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Minimizing Wrist Displacement for Drum Stroke Spinova of Humanoid
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#177871" title="Click to go to the Author Index">
             Cho, Jungsoo
            </a>
           </td>
           <td class="r">
            Sogang Univ
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#122553" title="Click to go to the Author Index">
             Yim, Sehyuk
            </a>
           </td>
           <td class="r">
            KIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab68" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#performance_evaluation_and_optimization" title="Click to go to the Keyword Index">
               Performance Evaluation and Optimization
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#humanoids" title="Click to go to the Keyword Index">
               Humanoids
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper focuses on implementing Spinova, a drum stroke often discovered when drummers hit a drum over a large distance in a limited amount of time, by minimizing wrist displacement. In order to execute the minimization at the right time, a triggering condition is introduced by appropriately quantifying the drum distance and stroke time length. The method is implemented on MOFFETT, a drumming humanoid platform capable of autonomous drumming based on a given drum score. Finally, the impact of Spinova on energy consumption and reaction torque on shoulder is analyzed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2e_02">
             10:40-10:50, Paper WO2E.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('69'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Door-Opening with a Dual-Arm Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#285119" title="Click to go to the Author Index">
             Shin, Kazuki
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#355042" title="Click to go to the Author Index">
             Mineyev, Roman
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#232738" title="Click to go to the Author Index">
             Hong, Jooyoung
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#150747" title="Click to go to the Author Index">
             Kim, Joohyung
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab69" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#humanoids" title="Click to go to the Keyword Index">
               Humanoids
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents the development and evaluation of specialized perception and manipulation methodologies for a dual-arm robot system. The goal of this research is to enhance the adaptability and performance of this system for intricate real-world bimanual tasks. The proposed approach is validated through a key real-world task - autonomous door-opening. This particular task provides an excellent test bed, as it requires robust perception, coordinated manipulation, and context-aware decision-making in a human-centric environment. To tackle these challenges, we employ methods such as camera-based object recognition and localization, task-specific motion planning algorithms, and integrated force feedback mechanisms. The proposed strategies for door-opening have been successfully implemented, leading to improvements in the overall performance, robustness, and adaptability of the dual-arm robot system under varied conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2e_03">
             10:50-11:00, Paper WO2E.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('113'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Preliminary Study and Analysis of MUAT: Modularized Ultralight Arm Tracker for Humanoid Teleoperation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386408" title="Click to go to the Author Index">
             Kim, Dongjun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386409" title="Click to go to the Author Index">
             Kim, Juhyun
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#302751" title="Click to go to the Author Index">
             You, Seungbin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#294364" title="Click to go to the Author Index">
             Sung, Eunho
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#101200" title="Click to go to the Author Index">
             Park, Jaeheung
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab113" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#telerobotics" title="Click to go to the Keyword Index">
               Telerobotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#mechanism_and_design" title="Click to go to the Keyword Index">
               Mechanism and Design
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#humanoids" title="Click to go to the Keyword Index">
               Humanoids
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This research addresses the need for advanced teleoperation in challenging environments where human access is limited, introducing MUAT (Modularized Ultralight Arm Tracker). Leveraging advancements in humanoid robotics and motion retargeting, MUAT creatively overcomes the limitations of previous teleoperation methods, including IMU-based and optical sensor-based systems. These methods often struggled with issues related to measurement accuracy, space requirements, and operational constraints. MUAT employs a position sensor-based exoskeleton approach, offering a novel solution. The design of MUAT is characterized by its lightweight and modular construction, allowing it to be easily attached to an operator's forearm. This design minimizes interference with movement and grants unrestricted use of the hands. To assess the feasibility and effectiveness of MUAT, a preliminary study version, referred to as p-MUAT (preliminary MUAT), was developed in this study. Through rigorous testing with p-MUAT, we successfully validated the device's precision in tracking the operator's wrist position, utilizing a motion capture system for validation. Additionally, this study demonstrated the device's capability to effectively teleoperate a humanoid robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2e_04">
             11:00-11:10, Paper WO2E.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('118'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Mixed Reality Interface for Whole-Body Balancing and Manipulation of Humanoid Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#282506" title="Click to go to the Author Index">
             Song, Hyunjong
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387209" title="Click to go to the Author Index">
             Bronfman, Gabriel
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387210" title="Click to go to the Author Index">
             Zhang, Yunxiang
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387211" title="Click to go to the Author Index">
             Sun, Qi
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#120137" title="Click to go to the Author Index">
             Kim, Joo H.
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab118" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#humanoids" title="Click to go to the Keyword Index">
               Humanoids
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#telerobotics" title="Click to go to the Keyword Index">
               Telerobotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The complexity of the control and operation is one of the roadblocks of widespread utilization of humanoid robots. In this study, we introduce a novel approach to humanoid robot control by leveraging a mixed reality (MR) interface for whole-body balancing and manipulation. This interface system uses an MR headset to track the operator’s movement and provide the operator with useful visual information for the control. The robot mimics the operator’s movement through a motion retargeting method based on linear scaling and inverse kinematics. The operator obtains visual access to the robot’s perspective view augmented with fiducial detection and perceives the current stability of the robot by evaluating the robot’s center-of-mass state in real-time against the precomputed balanced state basin. In experimental demonstrations, the operator successfully controlled the robot to grasp and lift an object without falling. The common issues in teleoperation with virtual reality headsets, motion sickness and unawareness of their surroundings, are reduced to a low level by using the MR headset with transparent glasses. This study demonstrates the potential of MR in teleoperation with a motion retargeting and stability monitoring method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo2e_05">
             11:10-11:20, Paper WO2E.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('133'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design and Development of the Linear Actuator for Enhanced Agility in Humanoid Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#348988" title="Click to go to the Author Index">
             Won, Junhee
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#347279" title="Click to go to the Author Index">
             Kang, Gihun
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#385971" title="Click to go to the Author Index">
             Jee, SunHyuk
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#188172" title="Click to go to the Author Index">
             Ahn, Min Sung
            </a>
           </td>
           <td class="r">
            University of California, Los Angeles
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#143376" title="Click to go to the Author Index">
             Han, Jeakweon
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab133" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#actuation_and_actuators" title="Click to go to the Keyword Index">
               Actuation and Actuators
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#humanoids" title="Click to go to the Keyword Index">
               Humanoids
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we will discuss the development of linear actuators for agile humanoid robots. Most linear actuators have greater rigidity than rotary actuators and are configured to be used in high-load environments using large deceleration ratios or hydraulic systems. However, to apply the linear actuator to a very dynamic humanoid robot, in addition to the large rigidity capable of withstanding impact, it must satisfy requirements such as accurate speed control, low impedance force control, and high power density. In particular, for high-bandwidth force control, mechanical impedance in the joint stage must be minimized. To compensate for these shortcomings, this paper proposes a linear actuator that minimizes mechanical impedance and ultimately has excellent back-drive performance by directly fastening only ball screws with large screw gaps to high power density BLDC motors. Taking into account these characteristics, a linear actuator controller was designed and the performance and function of the controller were verified on the actual actuator. Finally, we demonstrate that it is optimized for humanoid robots with agile motor performance by applying linear actuators to the leg of humanoid robots for actual bipedal walking.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo4a">
             <b>
              WO4A
             </b>
            </a>
           </td>
           <td class="r">
            Rosenthal
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo4a" title="Click to go to the Program at a Glance">
             <b>
              ISR Journal Session
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#102527" title="Click to go to the Author Index">
             Chong, Nak Young
            </a>
           </td>
           <td class="r">
            Japan Advanced Institute of Science and Technology
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo4b">
             <b>
              WO4B
             </b>
            </a>
           </td>
           <td class="r">
            KC 905
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo4b" title="Click to go to the Program at a Glance">
             <b>
              Social Human-Robot Interaction of Human-Care Service Robots
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#111352" title="Click to go to the Author Index">
             Ahn, Ho Seok
            </a>
           </td>
           <td class="r">
            The University of Auckland, Auckland
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#115484" title="Click to go to the Author Index">
             Jang, Minsu
            </a>
           </td>
           <td class="r">
            Electronics &amp; Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td>
            Organizer:
            <a href="UR24_AuthorIndexWeb.html#115484" title="Click to go to the Author Index">
             Jang, Minsu
            </a>
           </td>
           <td class="r">
            Electronics &amp; Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td>
            Organizer:
            <a href="UR24_AuthorIndexWeb.html#103688" title="Click to go to the Author Index">
             Choi, Jongsuk
            </a>
           </td>
           <td class="r">
            Korea Inst. of Sci. and Tech
           </td>
          </tr>
          <tr>
           <td>
            Organizer:
            <a href="UR24_AuthorIndexWeb.html#111352" title="Click to go to the Author Index">
             Ahn, Ho Seok
            </a>
           </td>
           <td class="r">
            The University of Auckland, Auckland
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4b_01">
             14:20-14:30, Paper WO4B.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('10'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Emotional Talking Face Generation with a Single Image (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386022" title="Click to go to the Author Index">
             Kim, Gayeon
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386023" title="Click to go to the Author Index">
             Hong, Yugyeong
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#216665" title="Click to go to the Author Index">
             Lim, JongYoon
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#339296" title="Click to go to the Author Index">
             Gee, Trevor
            </a>
           </td>
           <td class="r">
            The University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#107803" title="Click to go to the Author Index">
             MacDonald, Bruce
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#111352" title="Click to go to the Author Index">
             Ahn, Ho Seok
            </a>
           </td>
           <td class="r">
            The University of Auckland, Auckland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab10" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The primary goal of our project is to generate artificial humanoid avatars, specifically talking faces from a single image and a text for enhanced human-robotic interaction. We put a specific emphasis on avatars that exhibit precise lip motion, head movement, and dynamic facial expressions. We believe that these attributes are essential components, making avatars significantly more engaging to human users. Contrary to traditional 3D modelling techniques that are commonly used in many modern state-of-the-art systems, our project aims to build avatars from machine-learned image augmentations. While numerous studies have been conducted on talking face generation systems, most have explored lip-motion in isolation from emotional facial shifts. Additionally, many methods depend heavily on audio or video inputs. In this paper, we propose an emotional talking face generation, called EmoFaceGen, which generates realistic talking face videos with emotions. Our system is unique in that it creates emotional talking face videos taking a single facial image and a text as inputs, then producing a talking face video with emotions as an output. The text input is converted to an audio source using the Text-To-Speech method for our project. Based on our findings, EmoFaceGen provides a more realistic talking face representation compared to other open-source models, highlighting a positive direction in overcoming present challenges in this area especially when considering the memory and hardware limitations associated with conventional 3D graphics methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4b_02">
             14:30-14:40, Paper WO4B.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('12'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Human-Robot Interaction: Integrating ASL Recognition and LLM-Driven Co-Speech Gestures in Pepper Robot with a Compact Neural Network (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#216665" title="Click to go to the Author Index">
             Lim, JongYoon
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#116755" title="Click to go to the Author Index">
             Sa, Inkyu
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#107803" title="Click to go to the Author Index">
             MacDonald, Bruce
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#111352" title="Click to go to the Author Index">
             Ahn, Ho Seok
            </a>
           </td>
           <td class="r">
            The University of Auckland, Auckland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab12" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This research investigates the use of compact deep neural network designs to teach the humanoid robot Pepper to comprehend American Sign Language (ASL), enhancing non-verbal interactions between humans and robots. Initially, we developed a streamlined and powerful model specifically for ASL interpretation, optimized for embedded systems. This model ensures swift sign language recognition while minimizing computational demands. Furthermore, we incorporate large language models (LLMs) to enhance the robot's interactive abilities. By carefully crafting prompts, Pepper can produce Co-Speech Gesture responses, fostering more fluid and realistic human-robot conversations. We also introduce a comprehensive software framework that encapsulates these advancements in a socially conscious AI interaction model. Utilizing Pepper's capabilities, we showcase the practicality and impact of our method in actual scenarios. Our findings underscore the significant possibilities for improving human-robot interactions through non-verbal means, bridging communication barriers, and making technology more accessible and understandable.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4b_03">
             14:40-14:50, Paper WO4B.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('17'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Beyond Words: Enhancing Natural Interaction by Recognizing Social Conversation Contexts in HRI (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#222353" title="Click to go to the Author Index">
             jang, jaeyoon
            </a>
           </td>
           <td class="r">
            ETRI
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#130175" title="Click to go to the Author Index">
             Yoon, Youngwoo
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab17" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#ai_reasoning_methods_for_robotics" title="Click to go to the Keyword Index">
               AI Reasoning Methods for Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the ongoing advancements in AI technology, human-robot interactions have become increasingly prevalent, extending across diverse domains such as AI speakers and service robots. Despite the progress, users often perceive interactions with robots as lacking naturalness. One factor contributing to this perception is the improper involvement of robots in specific situations. To address these issues, this paper proposes a method for defining and recognizing social conversation contexts. Furthermore, the paper outlines plan for constructing a database to assess the performance of the defined problem. By enabling robots to recognize social conversational situations based on the speaker and addressee and generate context-aware actions, we envision achieving more natural interactions. Through the newly proposed situation definition and problem-solving approach, we anticipate alleviating some of the unnatural interaction elements in Human-Robot Interaction (HRI) scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4b_04">
             14:50-15:00, Paper WO4B.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('126'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multimodal Personality Prediction: A Real-Time Recognition System for Social Robots with Data Acquisition (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#254311" title="Click to go to the Author Index">
             Bhin, Hyeonuk
            </a>
           </td>
           <td class="r">
            Korea Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#109322" title="Click to go to the Author Index">
             Lim, Yoonseob
            </a>
           </td>
           <td class="r">
            Korea Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#103688" title="Click to go to the Author Index">
             Choi, Jongsuk
            </a>
           </td>
           <td class="r">
            Korea Inst. of Sci. and Tech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab126" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#foundations_of_sensing_and_estimation" title="Click to go to the Keyword Index">
               Foundations of Sensing and Estimation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we propose a new real-time recognition system that predicts the Big Five personality traits - extroversion, agreeableness, conscientiousness, neuroticism, and openness. This system continuously evaluates these traits over time and across various context. By treating each moment individually to predict personality scores, we have implemented and compared various multimodal approaches to enhance the accuracy of these predictions. Our framework has shown the capability to obtain robust personality predictions extrapolated from complex information. Additionally, we have successfully implemented this framework in a real robot, confirming its potential applicability in the realm of social robotics. Based on these research findings, our personality prediction model is expected to operate stably in a wide range of environments, contributing to social interactions and applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4b_05">
             15:00-15:10, Paper WO4B.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('176'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Simple Baseline for Uncertainty-Aware Language-Oriented Task Planner for Embodied Agents (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388425" title="Click to go to the Author Index">
             Ong, Hyobin
            </a>
           </td>
           <td class="r">
            University of Science and Technology(UST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#130175" title="Click to go to the Author Index">
             Yoon, Youngwoo
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#360138" title="Click to go to the Author Index">
             Choi, JaeWoo
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute (ETRI)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#115484" title="Click to go to the Author Index">
             Jang, Minsu
            </a>
           </td>
           <td class="r">
            Electronics &amp; Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab176" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#performance_evaluation_and_optimization" title="Click to go to the Keyword Index">
               Performance Evaluation and Optimization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Our research presents an improvement to task planning using Large Language Models (LLMs) by incorporat- ing a simple approach to consider uncertainty in planning. This strategy, which differs from standard LLM-based planners, emphasizes quantifying uncertainty and exploring alternative paths for task execution. By establishing a method to measure uncertainty by setting appropriate thresholds on probabilities in skill selection, our planner is more capable at selecting a better path for carrying out tasks. Through our experiments in high- level planning within the ALFRED task domain, we observed an improvement in plan execution success rates by 0.96–2.41 percent points over conventional LLM-based task planners. These results demonstrate that uncertainty-aware strategies can lead to more precise and effective task planning.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo4c">
             <b>
              WO4C
             </b>
            </a>
           </td>
           <td class="r">
            KC 907
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo4c" title="Click to go to the Program at a Glance">
             <b>
              Underwater Robotics
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#146612" title="Click to go to the Author Index">
             Wang, Long
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#154213" title="Click to go to the Author Index">
             Huang, Shouren
            </a>
           </td>
           <td class="r">
            Tokyo University of Science
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4c_01">
             14:20-14:30, Paper WO4C.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('99'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Bimanual Teleoperation Framework for Light Duty Underwater Vehicle-Manipulator Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#307180" title="Click to go to the Author Index">
             Sitler, Justin L.
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386983" title="Click to go to the Author Index">
             Sowrirajan, Srikarran
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#123501" title="Click to go to the Author Index">
             Englot, Brendan
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#146612" title="Click to go to the Author Index">
             Wang, Long
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab99" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#underwater_robotics" title="Click to go to the Keyword Index">
               Underwater Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#telerobotics" title="Click to go to the Keyword Index">
               Telerobotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In an effort to lower the barrier to entry in underwater manipulation, this paper presents an open-source, user-friendly framework for bimanual teleoperation of a light- duty underwater vehicle-manipulator system (UVMS). This framework allows for the control of the vehicle along with two manipulators and their end-effectors using two low-cost haptic devices.
             <p>
              The UVMS kinematics are derived in order to create an independent resolved motion rate controller for each manipulator, which optimally controls the joint positions to achieve a desired end-effector pose. This desired pose is computed in real-time using a teleoperation controller developed to process the dual haptic device input from the user. A physics-based simulation environment is used to implement this framework for two example tasks as well as provide data for error analysis of user commands. The first task illustrates the functionality of the framework through motion control of the vehicle and manipulators using only the haptic devices. The second task is to grasp an object using both manipulators simultaneously, demonstrating precision and coordination using the framework. The framework code is available at https://github.com/stevens-armlab/uvms_bimanual_sim.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4c_02">
             14:30-14:40, Paper WO4C.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('120'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Preliminary Results on Cooperative Operation of ASV-AUV Using Acoustic Based Relative Localization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#104653" title="Click to go to the Author Index">
             Choi, Jinwoo
            </a>
           </td>
           <td class="r">
            KRISO, Korea Research Institute of Ships &amp; Ocean Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#319846" title="Click to go to the Author Index">
             Kang, Minju
            </a>
           </td>
           <td class="r">
            Korea Research Institute of Ships &amp; Ocean Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#130223" title="Click to go to the Author Index">
             Choi, Hyun-Taek
            </a>
           </td>
           <td class="r">
            Korea Research Institute of Ships and Oceans Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#171826" title="Click to go to the Author Index">
             Park, Jeonghong
            </a>
           </td>
           <td class="r">
            KRISO
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab120" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#underwater_robotics" title="Click to go to the Keyword Index">
               Underwater Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multiple autonomous marine vehicles can be used for complementary cooperative operation to perform given marine tasks. This paper presents a method of cooperative positioning system for ASV-AUV. ASVs can perform accurate localization based on GNSS. On the other hand, AUVs suffer from lack of absolute localization and the accumulation of inertial sensor based localization error. The proposed method is developed to perform reliable localization of both ASV and AUV by cooperative localization system. The proposed cooperative positioning system utilizes relative geometric information between vehicles acquired by acoustic sensors. AUV can correct its own location by using the relative geometric information. The proposed method is implemented by Kalman filter based estimation for the cooperation of single ASV and single AUV case. Simulation results verify the performance of the proposed cooperative location method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4c_03">
             14:40-14:50, Paper WO4C.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('157'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Segmentation of Respiratory Bubbles in Underwater Diver Image Using Pixel Coordinate Information and K-Means Clustering
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388330" title="Click to go to the Author Index">
             Jeon, Mingyu
            </a>
           </td>
           <td class="r">
            Kongju National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#104577" title="Click to go to the Author Index">
             Lee, Sejin
            </a>
           </td>
           <td class="r">
            Kongju National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab157" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#underwater_robotics" title="Click to go to the Keyword Index">
               Underwater Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#ai_reasoning_methods_for_robotics" title="Click to go to the Keyword Index">
               AI Reasoning Methods for Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In underwater diving monitoring, camera sensors capture respiratory bubbles, which are useful for tracking breathing cycles, assessing breath volume, and identifying anomalies. However, segmenting these dynamic and irregular bubbles is challenging. Supervised deep learning offers high accuracy but demands significant training data. To overcome this, we used unsupervised K-means clustering, combining RGB color space with HSV color space images with relative coordinates to extract bubbles from diver images. Relative coordinates address spatial issues in clustering. We applied Contrast Limited Adaptive Histogram Equalization to balance color channels affected by the underwater environment. This validated our ability to segment respiratory bubbles by suppressing seabed reflections using cluster-based ensembles. We evaluate the result of the algorithm with the mean IoU score. And we also compared the impact of pre-processing on the result.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo4d">
             <b>
              WO4D
             </b>
            </a>
           </td>
           <td class="r">
            KC 909
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo4d" title="Click to go to the Program at a Glance">
             <b>
              Motion Planning and Obstacle Avoidance
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#103527" title="Click to go to the Author Index">
             Lee, Kiju
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#192667" title="Click to go to the Author Index">
             Suresh, Aamodh
            </a>
           </td>
           <td class="r">
            US Army Research Laboratory
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4d_01">
             14:20-14:30, Paper WO4D.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('50'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Model Predictive Control under Hard Collision Avoidance Constraints for a Robotic Arm
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386987" title="Click to go to the Author Index">
             Haffemayer, Arthur
            </a>
           </td>
           <td class="r">
            LAAS-CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#296707" title="Click to go to the Author Index">
             Jordana, Armand
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#250698" title="Click to go to the Author Index">
             Fourmy, Mederic
            </a>
           </td>
           <td class="r">
            CIIRC, CVUT
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386988" title="Click to go to the Author Index">
             Wojciechowski, Krzysztof
            </a>
           </td>
           <td class="r">
            LAAS-CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#191946" title="Click to go to the Author Index">
             Saurel, Guilhem
            </a>
           </td>
           <td class="r">
            LAAS-CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#171924" title="Click to go to the Author Index">
             Petrik, Vladimir
            </a>
           </td>
           <td class="r">
            Czech Technical University in Prague
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#101933" title="Click to go to the Author Index">
             Lamiraux, Florent
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#103132" title="Click to go to the Author Index">
             Mansard, Nicolas
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab50" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We design a method to control the motion of a manipulator robot while strictly enforcing collision avoidance in a dynamic obstacle field. We rely on model predictive control while formulating collision avoidance as a hard constraint. We express the constraint as the requirement for a signed distance function to be positive between pairs of strictly convex objects. Among various formulations, we provide a suitable definition for this signed distance and for the analytical derivatives needed by the numerical solver to enforce the constraint. The method is completely implemented on a manipulator "Panda" robot, and the efficient open-source implementation is provided along with the paper. We experimentally demonstrate the efficiency of our approach by performing dynamic tasks in an obstacle field while reacting to non-modeled perturbations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4d_02">
             14:30-14:40, Paper WO4D.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('89'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Field Navigation of Mobile Robots for Agricultural Crop Monitoring
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#306167" title="Click to go to the Author Index">
             Wei, Yuan
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#298836" title="Click to go to the Author Index">
             Lee, Kangneoung
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#103527" title="Click to go to the Author Index">
             Lee, Kiju
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab89" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#wheeled_mobile_robots" title="Click to go to the Keyword Index">
               Wheeled Mobile Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a mobile ground robot designed for autonomous navigation and data collection in agricultural fields, utilizing precise localization through an extended Kalman filter (EKF) that integrates data from GPS, an inertial measurement unit (IMU), and wheel encoders. We propose a novel method based on an artificial electric potential field (AEPF) for reliable and autonomous navigation in these robots. Implemented on a four-wheeled robot interfaced, our experiments showed that AEPF-based navigation processed data more quickly than the traditional Nav2 local path planner. Additionally, the robot reliably collected RGB and depth images while navigating crop rows, highlighting the method's effectiveness and its potential for extensive applications in autonomous crop monitoring. Additionally, a graphical user interface was developed to enable users to define target areas, assign tasks, and monitor the robot's performance in real time.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4d_03">
             14:40-14:50, Paper WO4D.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('94'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dual-Type Discriminator Adversarial Reservoir Computing for Robust Autonomous Navigation in a Snowy Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#371391" title="Click to go to the Author Index">
             Li, Fangzheng
            </a>
           </td>
           <td class="r">
            Japan Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#179460" title="Click to go to the Author Index">
             Ji, Yonghoon
            </a>
           </td>
           <td class="r">
            JAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab94" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#intelligent_robotic_vehicles" title="Click to go to the Keyword Index">
               Intelligent Robotic Vehicles
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#wheeled_mobile_robots" title="Click to go to the Keyword Index">
               Wheeled Mobile Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In winter, snowfall has a risk of impairing the autonomous navigation capabilities of mobile robots by obscuring road lane markings and causing sensor noise. This problem complicates the development of safe and efficient snow removal robots. In our research, we propose a novel supervised machine learning method for autonomous robot navigation in snowy environments based on Dual-Type Discriminator Adversarial Reservoir Computing (DDARC) which integrates Reservoir Computing (RC) with Generative Adversarial Networks (GANs). Utilizing depth and thermal imagery as inputs, our method can generate reliable control values for the robot's movement. Experiments in simulated environments have demonstrated that our method significantly improves the autonomous navigation capabilities of mobile robots, even in substantial environmental noise from snowfall.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4d_04">
             14:50-15:00, Paper WO4D.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('109'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reactive Robot Navigation Using Behavioral Risk Perception for Uncertain Dynamic Obstacle
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#192667" title="Click to go to the Author Index">
             Suresh, Aamodh
            </a>
           </td>
           <td class="r">
            US Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#103595" title="Click to go to the Author Index">
             Nieto-Granda, Carlos
            </a>
           </td>
           <td class="r">
            U.S. Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab109" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#behavior_based_systems" title="Click to go to the Keyword Index">
               Behavior-Based Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Successful robotic deployment in challenging environments requires diverse reasoning and reactive control techniques to deal with uncertainty and risk. In this work, we propose a novel behavioral control framework to navigate in such environments with static and dynamic sources of risks. Different agent behaviors can create distinct environment assessments, leading to a variety of reactions while dealing with uncertain and risky situations. We construct a class of perceived risk functions to capture these different behaviors by taking inspiration from behavioral decision making models from Cumulative Prospect Theory (CPT). We then incorporate these perceived risks via local costmaps into a Model Predictive Controller (MPC) framework. Specifically, we use Model Predictive Path Integral (MPPI) Control framework that is capable of handling more general cost functions like our proposed perceived risks. Using this framework, we generate reactive control policies for any given behavioral profile, resulting in a diverse AI for reactive controls. We then illustrate the proposed algorithm in virtual experiments conducted in a high fidelity indoor ROS-Unity environment embedded with static and dynamic sources of risk. We show that our proposed framework is capable of producing a larger range of reactive behaviors leading to a more successful robot deployment.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4d_05">
             15:00-15:10, Paper WO4D.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('143'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Unified Safety-Critical Motion Planning for Connected Non-Holonomic Agents Using an Adaptive A* and Hybrid A* Integration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#357399" title="Click to go to the Author Index">
             Vashi, Harin
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387265" title="Click to go to the Author Index">
             Shanbhag, Sumeet
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#160893" title="Click to go to the Author Index">
             Farzan, Siavash
            </a>
           </td>
           <td class="r">
            California Polytechnic State University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab143" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a unified approach to safety-critical, multi-agent motion planning for connected autonomous robotic systems, seamlessly integrating the kinematic, dynamic, and safety constraints of individual agents, while reducing computational expense to ensure real-time applicability. By integrating Voronoi Cells with an adaptive blend of A* and Hybrid A* algorithms, the proposed combinational planner ensures the generation of feasible and executable trajectories, guaranteeing efficient and collision-free navigation of multiple agents in dynamically complex environments. An additional deadlock avoidance strategy is proposed to further enhance the safety layer. We demonstrate the effectiveness and robustness of our approach in terms of efficiency, collision avoidance, and deadlock resolution through simulations in diverse, randomly generated environments. The results show that the proposed method outperforms existing methods in terms of dynamic considerations and obstacle avoidance, making it a practical real-time motion planning approach for connected non-holonomic agents in complex environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4d_06">
             15:10-15:20, Paper WO4D.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('105'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Navigation with Route Opening Capability Based on Deep Reinforcement Learning by Material Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#339785" title="Click to go to the Author Index">
             Lu, Jiaheng
            </a>
           </td>
           <td class="r">
            Japan Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#179460" title="Click to go to the Author Index">
             Ji, Yonghoon
            </a>
           </td>
           <td class="r">
            JAIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab105" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#intelligent_robotic_vehicles" title="Click to go to the Keyword Index">
               Intelligent Robotic Vehicles
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#wheeled_mobile_robots" title="Click to go to the Keyword Index">
               Wheeled Mobile Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In situations following catastrophic events such as fires and earthquakes, the deployment of autonomous robotic technology plays a crucial role in ensuring the success of exploration and assessment tasks. Despite significant progress in robot navigation, there remains a critical need for autonomous navigation systems adept at executing adaptive motion strategies, particularly in complex environments with obstacles of varying material properties. This study proposes an autonomous navigation system for disaster response scenarios, utilizing advanced deep reinforcement learning techniques. We develop a novel route opening policy that enhances the robot's ability to interact with and navigate around obstacles, thereby improving its route opening capabilities. Our method distinguishes between general navigation and collision-pushing scenarios to identify optimal routes. Experiments demonstrate the system's effectiveness in navigating and opening routes, as well as in locating victims, utilizing both the range and intensity data provided by Light Detection and Ranging (LiDAR) sensors.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wo4e">
             <b>
              WO4E
             </b>
            </a>
           </td>
           <td class="r">
            KC 912
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wo4e" title="Click to go to the Program at a Glance">
             <b>
              Legged Robots
             </b>
            </a>
           </td>
           <td class="r">
            Regular
           </td>
          </tr>
          <tr>
           <td>
            Chair:
            <a href="UR24_AuthorIndexWeb.html#104490" title="Click to go to the Author Index">
             Myung, Hyun
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td>
            Co-Chair:
            <a href="UR24_AuthorIndexWeb.html#266110" title="Click to go to the Author Index">
             Bratta, Angelo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4e_01">
             14:20-14:30, Paper WO4E.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('16'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic Analysis and Verification of the Robot Leg Employing the Water-Based Electro-Hydraulic Actuator (EHA)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#242841" title="Click to go to the Author Index">
             Lim, Dongwon
            </a>
           </td>
           <td class="r">
            University of Suwon
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab16" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#actuation_and_actuators" title="Click to go to the Keyword Index">
               Actuation and Actuators
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The aim of this paper is to apply the water-based Electro-Hydraulic Actuator (EHA) to a robotic leg device. To assess its applicability, kinematic and dynamic analyses have been performed. The EHA is a linear actuator where the power cylinder is driven by hydraulic pressure from a pump. In this study, one robotic leg was considered, and its motion was simulated by exoskeletal parallel actuation, mimicking a human configuration. Water was used as the working fluid for the EHA in this study, as conventional oil hydraulics have drawbacks such as contamination, cost, and fire risk. Therefore, a new type of actuator was tested and verified for a robotic leg. The Euler-Lagrange equation was employed to derive the dynamic equation, using an inverted pendulum as a simplified model for the robotic leg. An experimental set-up was constructed, and pressure measurements of the EHA were compared with calculations from the dynamics, incorporating measured angle and angular acceleration values. The experimental results indicated that the pressure percentage difference averaged ~22%. The graph of the calculated pressure appeared smoother than the measured curve, as the dynamics calculation utilized measurements from the mechanical system. In the future, more rigorous dynamics will be considered, and a feedback controller will be designed for improved robot operation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4e_02">
             14:30-14:40, Paper WO4E.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('23'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              3D LiDAR Map-Based Robust Localization System Leveraging Pose Divergence Detection and Relocalization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#319663" title="Click to go to the Author Index">
             Lee, Seungjae
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#295392" title="Click to go to the Author Index">
             Oh, Minho
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#310811" title="Click to go to the Author Index">
             Nahrendra, I Made Aswin
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#279170" title="Click to go to the Author Index">
             Song, Wonho
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#274099" title="Click to go to the Author Index">
             Yu, Byeongho
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#358827" title="Click to go to the Author Index">
             Marsim, Kevin Christiansen
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386993" title="Click to go to the Author Index">
             Kang, DongWan
            </a>
           </td>
           <td class="r">
            Hanwhaaerospace
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#104490" title="Click to go to the Author Index">
             Myung, Hyun
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab23" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, various studies on simultaneous localization and mapping~(SLAM) have achieved outstanding performance in terms of accuracy. Accordingly, various SLAM methods can generate a precise 3D map of the surroundings in usual environments and estimate the pose accurately on a pre-built map. However, the localization should be not only accurate but also robust in various situations to achieve a fully autonomous navigation system. Unfortunately, existing localization algorithms are not robust in some cases. For example, the aggressive walking motion of quadruped robots frequently causes a divergence of the odometry algorithm, leading to a catastrophic failure of a fully autonomous system. In this study, we propose a robust localization system leveraging a pose divergence manager, which is applicable to various odometry algorithms. The localization system integrates a pose divergence manager with a 3D LiDAR map-based global localizer that estimates the global pose of the robot on the pre-built 3D LiDAR map. We conducted real-world experiments using a quadruped robot and verified that our proposed method is accurate and robust in indoor and outdoor environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4e_03">
             14:40-14:50, Paper WO4E.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('103'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ContactNet: Online Multi-Contact Planning for Acyclic Legged Robot Locomotion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#266110" title="Click to go to the Author Index">
             Bratta, Angelo
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#257265" title="Click to go to the Author Index">
             Meduri, Avadesh
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#131859" title="Click to go to the Author Index">
             Focchi, Michele
            </a>
           </td>
           <td class="r">
            Università Di Trento
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#107817" title="Click to go to the Author Index">
             Righetti, Ludovic
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#108792" title="Click to go to the Author Index">
             Semini, Claudio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab103" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#ai_reasoning_methods_for_robotics" title="Click to go to the Keyword Index">
               AI Reasoning Methods for Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The field of legged robots has seen tremendous progress in the last few years. Locomotion trajectories are commonly generated by optimization algorithms in a Model Predictive Control (MPC) loop. To achieve online trajectory optimization, the locomotion community generally makes use of heuristic-based contact planners due to their low computation times and high replanning frequencies. In this work, we propose ContactNet, a fast acyclic contact planner based on a multi-output regression neural network. ContactNet ranks discretized stepping locations, allowing to quickly choose the best feasible solution, even in complex environments. The low computation time, in the order of 1 ms, enables the execution of the contact planner concurrently with a trajectory optimizer in a MPC fashion. In addition, the computational time does not scale up with the configuration of the terrain. We demonstrate the effectiveness of the approach in simulation in different scenarios with the quadruped robot Solo12. To the best knowledge of the authors, this is the first time a contact planner is presented that does not exhibit an increasing computational time on irregular terrains with an increasing number of gaps.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4e_04">
             14:50-15:00, Paper WO4E.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('107'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Impedance Matching: Enabling an RL-Based Running Jump in a Quadruped Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387175" title="Click to go to the Author Index">
             Guan, Neil
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#341431" title="Click to go to the Author Index">
             yu, Shangqun
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#286851" title="Click to go to the Author Index">
             Zhu, Shifan
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#168080" title="Click to go to the Author Index">
             Kim, Donghyun
            </a>
           </td>
           <td class="r">
            University of Massachusetts Amherst
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab107" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Replicating the remarkable athleticism seen in animals has long been a challenge in robotics control. Although Reinforcement Learning (RL) has demonstrated significant progress in dynamic legged locomotion control, the substantial sim-to-real gap often hinders the real-world demonstration of truly dynamic movements. We propose a new framework to mitigate this gap through frequency-domain analysis-based impedance matching between simulated and real robots. Our framework offers a structured guideline for parameter selection and the range for dynamics randomization in simulation, thus facilitating a safe sim-to-real transfer. The learned policy using our framework enabled jumps across distances of 55 cm and heights of 38 cm. The results are, to the best of our knowledge, one of the highest and longest running jumps demonstrated by an RL-based control policy in a real quadruped robot. Note that the achieved jumping height is approximately 85% of that obtained from a state-of-the-art trajectory optimization method, which can be seen as the physical limit for the given robot hardware. In addition, our control policy accomplished stable walking at speeds up to 2 m/s in the forward and backward directions, and 1 m/s in the sideway direction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wo4e_05">
             15:00-15:10, Paper WO4E.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('119'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Panoptic-SLAM: Visual SLAM in Dynamic Environments Using Panoptic Segmentation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#342624" title="Click to go to the Author Index">
             Fischer Abati, Gabriel
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#253766" title="Click to go to the Author Index">
             Soares, João Carlos Virgolino
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#272207" title="Click to go to the Author Index">
             Suzano Medeiros, Vivian
            </a>
           </td>
           <td class="r">
            University of São Paulo
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#114634" title="Click to go to the Author Index">
             Meggiolaro, Marco Antonio
            </a>
           </td>
           <td class="r">
            Pontifical Catholic University of Rio De Janeiro
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#108792" title="Click to go to the Author Index">
             Semini, Claudio
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab119" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The majority of visual SLAM systems are not robust in dynamic scenarios. The ones that deal with dynamic objects in the scenes usually rely on deep-learning-based methods to detect and filter these objects. However, these methods cannot deal with unknown moving objects. This work presents Panoptic-SLAM, a visual SLAM system robust to dynamic environments, even in the presence of unknown objects. It uses panoptic segmentation to filter dynamic objects from the scene during the state estimation process. Panoptic-SLAM is based on ORB-SLAM3, a state-of-the-art SLAM system for static environments. The implementation was tested using real-world datasets and compared with several state-of-the-art systems from the literature, including DynaSLAM, DS-SLAM, SaD-SLAM, PVO and FusingPanoptic. For example, Panoptic-SLAM is on average four times more accurate than PVO, the most recent panoptic-based approach for visual SLAM. Also, experiments were performed using a quadruped robot with an RGB-D camera to test the applicability of our method in real-world scenarios. The tests were validated by a ground-truth created with a motion capture system.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="wi5a">
             <b>
              WI5A
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="UR24_ProgramAtAGlanceWeb.html#wi5a" title="Click to go to the Program at a Glance">
             <b>
              Poster Sesssion II
             </b>
            </a>
           </td>
           <td class="r">
            Interactive
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_01">
             15:20-16:30, Paper WI5A.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('67'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Study on Motion Control of Robot System for Non-Destructive Testing of Steel Structures
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349827" title="Click to go to the Author Index">
             Jeong, Myeongsu
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349713" title="Click to go to the Author Index">
             Kim, Seolha
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics Technology Covergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387047" title="Click to go to the Author Index">
             Jang, Minwoo
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349814" title="Click to go to the Author Index">
             Lee, Eun-Bi
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349783" title="Click to go to the Author Index">
             Lee, Jae Youl
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab67" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#mechanism_and_design" title="Click to go to the Keyword Index">
               Mechanism and Design
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Steel structures can pose potential risks, including damage and collapse due to aging. Safety inspections can help you determine the condition of a structure, identify problems, and develop a maintenance and reinforcement plan. This paper proposes the design of a permanent magnet-based attachment mechanism for Non-Destructive Testing of steel structures and a walking linkage structure for overcoming various obstacles. The applicability of the designed robotic system was verified by selecting the drive part specification and motion verification through dynamics and motion analysis.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_02">
             15:20-16:30, Paper WI5A.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('30'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Analysis and Classification of Car Door Torque Profile for Hybrid Haptic Device Development
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#363812" title="Click to go to the Author Index">
             Kim, Ji-Sung
            </a>
           </td>
           <td class="r">
            KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#244519" title="Click to go to the Author Index">
             Ma, Jihyeong
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#118310" title="Click to go to the Author Index">
             Kyung, Ki-Uk
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science &amp; Technology (KAIST)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab30" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#haptics" title="Click to go to the Keyword Index">
               Haptics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#mechanism_and_design" title="Click to go to the Keyword Index">
               Mechanism and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Designing car doors to provide optimal haptic sensations is essential for enhancing user experience. Consequently, research has focused on simulating these haptic sensations using haptic devices during the design phase. However, the virtual implementation of car door mechanisms presents unique challenges, including significant resistive torque, self-opening and closing behavior, and variable torque profiles during the opening and closing phases. Therefore, analyzing the characteristics of the torque components that constitute the car door torque profile and developing suitable devices for their rendering is necessary. This paper presents the measurement of the car door torque profile and introduces a method that classifies the torque into active and passive components, based on whether they aid or impede rotation. By employing each torque component as an active actuator (motor) and a passive actuator (brake), we will design a suitable hybrid haptic device and can realistically implement the haptic feeling of a car door.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_03">
             15:20-16:30, Paper WI5A.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('73'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Posture Maintenance and Locomotion of a Quadruped Robot on a Marine Motion Platform Using Reinforcement Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349539" title="Click to go to the Author Index">
             Choi, Seunghyuk
            </a>
           </td>
           <td class="r">
            Chungnam National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#350011" title="Click to go to the Author Index">
             Park, Kwang-Phil
            </a>
           </td>
           <td class="r">
            Chungnam National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386897" title="Click to go to the Author Index">
             Ku, Bonseok
            </a>
           </td>
           <td class="r">
            Chungnam National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#131396" title="Click to go to the Author Index">
             Jung, Jongdae
            </a>
           </td>
           <td class="r">
            Chungnam National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab73" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this study, we focused on developing a controller of quadruped robots using a Proximal Policy Optimization (PPO) algorithm to achieve stable and effective locomotion of the robot in a moving ship. In the initial stage, the basic performance of the PPO algorithm was checked by performing simulation-based training in OpenAI Gym, targeting the Unitree Go1 platform. Then, the Gazebo simulator was used to verify the performance of the algorithm in a situation mimicking the inside of the moving ship hull in marine environments. The data generated through parallel learning reduced computational efforts in the learning process. Through the realistic gazebo simulations, we developed a quadruped control algorithm that can be applied to the various ship operation scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_04">
             15:20-16:30, Paper WI5A.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('78'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Preliminary Design of Maritime Visualization Framework for Cyber Physical Operating Systems (CPOS)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#173356" title="Click to go to the Author Index">
             Lee, Yeongjun
            </a>
           </td>
           <td class="r">
            Korea Research Institute of Ships and Ocean Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#237923" title="Click to go to the Author Index">
             Han, Jong-Boo
            </a>
           </td>
           <td class="r">
            Korea Institute of Ships and Ocean Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#156200" title="Click to go to the Author Index">
             Park, Daegil
            </a>
           </td>
           <td class="r">
            Korea Research Institute of Ships &amp; Ocean Engineering (KRISO)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#331715" title="Click to go to the Author Index">
             Kim, Seongsoon
            </a>
           </td>
           <td class="r">
            Korea Research Institute of Ships &amp; Ocean Engineering (KRISO)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#175841" title="Click to go to the Author Index">
             Yeu, Tae-Kyeong
            </a>
           </td>
           <td class="r">
            KRISO (Korea Research Institute of Ships &amp; Ocean Engineering)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab78" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#underwater_robotics" title="Click to go to the Keyword Index">
               Underwater Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper present a preliminary research on designing and applying a maritime visualization framework for CPOS technology in ocean environments. We developed a virtual underwater environment by mapping the seafloor and then created a virtual reality setting to make this environment visible to people. To assess its effectiveness, we built a test-bed for testing, verification, and analysis.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_05">
             15:20-16:30, Paper WI5A.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('84'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Demonstration through the Virtual Environment: Acquisition of Intrinsic Task Skills
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#334485" title="Click to go to the Author Index">
             Kim, Donghyeon
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology (KAIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#360015" title="Click to go to the Author Index">
             Park, Seong-Su
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#196716" title="Click to go to the Author Index">
             Lee, Kwang-Hyun
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#102369" title="Click to go to the Author Index">
             Ryu, Jee-Hwan
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab84" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#learning_from_humans" title="Click to go to the Keyword Index">
               Learning From Humans
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#robotic_systems_architectures_and_programming" title="Click to go to the Keyword Index">
               Robotic Systems Architectures and Programming
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#telerobotics" title="Click to go to the Keyword Index">
               Telerobotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning from Demonstration has garnered considerable attention for its ability to teach human-performed skills to robots. The method of providing demonstrations can limit the tasks that can be performed and significantly impact the learning outcomes. Therefore, we propose a demonstration method that is unrestricted by the task, which is the demonstration through the virtual environment. The demonstrations performed in virtual settings allow the operator to fully concentrate on the demonstration itself, extracting only the pure task skills of the operator. The proposed method has demonstrated its efficacy experimentally in the case of tight peg-in-hole tasks through CHAI3D simulator.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_06">
             15:20-16:30, Paper WI5A.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('88'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Implementation of a Person Following Algorithm Based on a LiDAR for a Smart Mobility Scooter
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#110874" title="Click to go to the Author Index">
             Jung, Eui-Jung
            </a>
           </td>
           <td class="r">
            Korea Institute of Robot and Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#242934" title="Click to go to the Author Index">
             KIM, Yongkuk
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Technology Convergence (kiro)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#186681" title="Click to go to the Author Index">
             Jeon, Kwang Woo
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#302765" title="Click to go to the Author Index">
             KIM, JUHYUN
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Techonlogy Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#237927" title="Click to go to the Author Index">
             Lee, Ye Jun
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Technology Convergence(kiro)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#158370" title="Click to go to the Author Index">
             Kim, Min-Gyu
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab88" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#wheeled_mobile_robots" title="Click to go to the Keyword Index">
               Wheeled Mobile Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#social_and_socially_assistive_robotics" title="Click to go to the Keyword Index">
               Social and Socially Assistive Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#intelligent_robotic_vehicles" title="Click to go to the Keyword Index">
               Intelligent Robotic Vehicles
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this study, we introduce an electric mobility scooter with autonomous driving algorithms, with particular emphasis on caregiver tracking capabilities to address mobility challenges faced by older adults in Korea. Utilizing a LiDAR-based person tracking algorithm, the scooter successfully recognizes and follows the user's guardian. Integration of various sensors ensures functional autonomy and safety. The experimental results demonstrated the effectiveness of the follow-the-guardian algorithm in a controlled environment and highlighted that further validation in crowded spaces is needed for successful commercialization. This solution has the potential to provide a safe and convenient transportation option for our aging population.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_07">
             15:20-16:30, Paper WI5A.7
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('92'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Self-Supervised Visual Odometry from Monocular Thermal Images: Exploration and Discussion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#241049" title="Click to go to the Author Index">
             Shin, Ukcheol
            </a>
           </td>
           <td class="r">
            CMU(Carnegie Mellon University)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387016" title="Click to go to the Author Index">
             park, seho
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#179403" title="Click to go to the Author Index">
             Oh, Jean
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab92" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robust spatial perception is one fundamental requirement for a safe and reliable autonomous driving system against adverse weather and lighting conditions, such as rain, fog, haze, snow, and low-light environments. However, the RGB sensor, the most widely used sensor, has a critical vulnerability to lighting and weather conditions. On the other hand, thermal sensors provide clear visibility and robustness under harsh rain, snow, fog, smoke, and light conditions. Therefore, this paper investigates the feasibility of a visual odometry method from a monocular thermal image as a potential rescue for a safe and reliable autonomous driving system. Due to the difficulty of ground-truth data collection in harsh environments, we propose a self-supervised learning method that trains a dense correspondence matching network only from sequential thermal images. After that, the odometry is estimated by RANSAC and the 5-point algorithm from the dense correspondence. Lastly, we discuss the current challenges of thermal visual odometry by comparing it with RGB image results.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_08">
             15:20-16:30, Paper WI5A.8
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('104'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Features Characterizing Safe Aerial-Aquatic Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#345094" title="Click to go to the Author Index">
             Giordano, Andrea
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#373269" title="Click to go to the Author Index">
             Romanello, Luca
            </a>
           </td>
           <td class="r">
            TUM
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#386955" title="Click to go to the Author Index">
             Perez Gonzalez, Diego
            </a>
           </td>
           <td class="r">
            Technical University of Munich (TUM)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#103152" title="Click to go to the Author Index">
             Kovac, Mirko
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#196333" title="Click to go to the Author Index">
             Armanini, Sophie Franziska
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab104" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#aerial_and_flying_robots" title="Click to go to the Keyword Index">
               Aerial and Flying Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#mechanism_and_design" title="Click to go to the Keyword Index">
               Mechanism and Design
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#biomimetic_and_bioinspired_robots" title="Click to go to the Keyword Index">
               Biomimetic and Bioinspired Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper underscores the importance of environmental monitoring, and specifically of freshwater ecosystems, which play a critical role in sustaining life and global economy. Despite their importance, insufficient data availability prevents a comprehensive understanding of these ecosystems, thereby impeding informed decision-making concerning their preservation. Aerial-aquatic robots are identified as effective tools for freshwater sensing, offering rapid deployment and avoiding the need of using ships and manned teams. To advance the field of aerial aquatic robots, this paper conducts a comprehensive review of air-water transitions focusing on the water entry strategy of existing prototypes. This analysis also highlights the safety risks associated with each transition and proposes a set of design requirements relating to robots' tasks, mission objectives, and safety measures. To further explore the proposed design requirements, we present a novel robot with VTOL capability, enabling seamless air water transitions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_09">
             15:20-16:30, Paper WI5A.9
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('124'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Motion Constraint-Based Contact Skill Segmentation to Extract the Interaction Skill Primitives
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#196716" title="Click to go to the Author Index">
             Lee, Kwang-Hyun
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#334485" title="Click to go to the Author Index">
             Kim, Donghyeon
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology (KAIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#360015" title="Click to go to the Author Index">
             Park, Seong-Su
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#102369" title="Click to go to the Author Index">
             Ryu, Jee-Hwan
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab124" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#telerobotics" title="Click to go to the Keyword Index">
               Telerobotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#learning_from_humans" title="Click to go to the Keyword Index">
               Learning From Humans
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present a novel motion constraint-based segmentation method to extract the interaction skill primitives in robotic interaction tasks. The proposed approach effectively leverages motion constraints to segment complex tasks into distinct sub-motions. Our experiments, conducted with a robot possessing 3 degrees of freedom in translation, demonstrated successful segmentation performance during interaction tasks. Demonstrations were provided by a remote operator relying solely on force feedback, simulating conditions where the robot operates without visual information. The segmented sub-motions accurately captured the exploration, confirmation, and insertion phases of the task, showcasing the method's efficacy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_10">
             15:20-16:30, Paper WI5A.10
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Efficient Task Assignment for Multiple Tethered Autonomous Underwater Vehicles to Prevent Entanglement
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#310650" title="Click to go to the Author Index">
             Patil, Abhishek
            </a>
           </td>
           <td class="r">
            Michigan Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#270575" title="Click to go to the Author Index">
             Park, Myoungkuk
            </a>
           </td>
           <td class="r">
            Michigan Technological University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#101173" title="Click to go to the Author Index">
             Bae, Jungyun
            </a>
           </td>
           <td class="r">
            Michigan Technological University
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_11">
             15:20-16:30, Paper WI5A.11
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('132'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              On Force Control of the Variable Topology Truss System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#191032" title="Click to go to the Author Index">
             Bae, Jangho
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#273265" title="Click to go to the Author Index">
             choi, myeongjin
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#101787" title="Click to go to the Author Index">
             Yim, Mark
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#140339" title="Click to go to the Author Index">
             Seo, TaeWon
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab132" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#modular_robots" title="Click to go to the Keyword Index">
               Modular Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#actuation_and_actuators" title="Click to go to the Keyword Index">
               Actuation and Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a preliminary study for applying force control to the Variable Topology Truss (VTT) system. The elements of the VTT system have so far only used position control as the main prismatic actuator (spiral zipper) is very difficult to model including friction, cogging, and other non-linear characteristics. An end-effector load cell provides a means for closed-loop control.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_12">
             15:20-16:30, Paper WI5A.12
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('136'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Framework for Learning and Reusing Robotic Skills
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#283495" title="Click to go to the Author Index">
             Hertel, Brendan
            </a>
           </td>
           <td class="r">
            University of Masssachusetts Lowell
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387206" title="Click to go to the Author Index">
             Tran, Nhu
            </a>
           </td>
           <td class="r">
            University of Massachusetts Lowell
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387216" title="Click to go to the Author Index">
             Elkoudi, Meriem
            </a>
           </td>
           <td class="r">
            Umass Lowell
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#160355" title="Click to go to the Author Index">
             Azadeh, Reza
            </a>
           </td>
           <td class="r">
            University of Massachusetts Lowell
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab136" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present our work in progress towards creating a library of motion primitives. This library facilitates easier and more intuitive learning and reusing of robotic skills. Users can teach robots complex skills through Learning from Demonstration, which is automatically segmented into primitives and stored in clusters of similar skills. We propose a novel multimodal segmentation method as well as a novel clustering method. Then, when needed for reuse we transform primitives into new environments using trajectory editing. We present simulated results for our framework with demonstrations taken on real-world robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_13">
             15:20-16:30, Paper WI5A.13
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('139'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dynamic MSCKF-3D VIO: Robust Filter Based Visual Inertial Odometry in Dynamic Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387059" title="Click to go to the Author Index">
             Seong, Samwoo
            </a>
           </td>
           <td class="r">
            Twinny
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387036" title="Click to go to the Author Index">
             Jung, KwangYik
            </a>
           </td>
           <td class="r">
            TWINNY
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387057" title="Click to go to the Author Index">
             Song, Jaebong
            </a>
           </td>
           <td class="r">
            TWINNY Corporation
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#104490" title="Click to go to the Author Index">
             Myung, Hyun
            </a>
           </td>
           <td class="r">
            KAIST (Korea Advanced Institute of Science and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab139" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, filter-based visual-inertial odometry (VIO) systems have demonstrated remarkable accuracy and robustness in static environments. However, the failure of these methods to account for dynamic objects renders them impractical in real-world scenarios, particularly in indoor environments with high pedestrian presence. To address this limitation, this paper proposes a filter-based state estimation method tailored for dynamic scenes, termed Dynamic MSCKF-3D VIO. The Dynamic MSCKF-3D VIO system comprises three primary components. Firstly, it involves the detection and segmentation of potential dynamic objects within the scene. Subsequently, feature tracking is conducted, excluding features associated with moving objects. Finally, the tracked features are utilized to execute the MSCKF-3D algorithm. To evaluate the efficacy of the proposed system, real-world data collected by the Twinny Deohago 60 is employed. The results demonstrate a notable improvement compared to the performance of the MSCKF-3D VIO system without dynamic object feature handling.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_14">
             15:20-16:30, Paper WI5A.14
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('142'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Development of a Thin Three-Axis Force Sensor Based on Sensitivity Amplification Mechanism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387251" title="Click to go to the Author Index">
             Lee, Seran
            </a>
           </td>
           <td class="r">
            Ajou University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#180577" title="Click to go to the Author Index">
             Jung, Dawoon
            </a>
           </td>
           <td class="r">
            Ajou Unversity
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387264" title="Click to go to the Author Index">
             Hwang, jinhak
            </a>
           </td>
           <td class="r">
            Ajou University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#150528" title="Click to go to the Author Index">
             Kim, Uikyum
            </a>
           </td>
           <td class="r">
            Ajou University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab142" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#performance_evaluation_and_optimization" title="Click to go to the Keyword Index">
               Performance Evaluation and Optimization
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#modeling__identification__calibration" title="Click to go to the Keyword Index">
               Modeling, Identification, Calibration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, research focus on force sensing for delicate tasks in robotic applications has grown. This has led to increased demands for force sensors with improved sensitivity, expanded measurement ranges, and more compact designs. The study introduces a compact three-axis force sensor with an enhanced sensitivity amplification mechanism for high sensitivity and capacity measurements. The mechanism, shaped like Wi-Fi, utilizes eccentric structures placed 120^{circ} apart between electrodes for three-axis force detection. The sensor comprises three main components: a top plate, sensing part, and bottom part. The Wi-Fi-shaped electrode structures are situated in the sensing part, where the capacitance information of the electrodes is measured. With high sensitivity (2.72 mN), a wide measurement range (100 N), and compactness (thickness: 6.6 mm), the proposed sensor offers the advantages of low cost and easy manufacture. The sensor's performance was validated through various experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_15">
             15:20-16:30, Paper WI5A.15
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('147'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Floor Plan Generation Via Ceiling Segmentation in Indoor Environment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349591" title="Click to go to the Author Index">
             Maeng, Jemo
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology(GIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#274398" title="Click to go to the Author Index">
             Lee, Seongju
            </a>
           </td>
           <td class="r">
            Gwangju Institue of Science and Technology (GIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#100825" title="Click to go to the Author Index">
             Lee, Kyoobin
            </a>
           </td>
           <td class="r">
            Gwangju Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab147" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#modeling__identification__calibration" title="Click to go to the Keyword Index">
               Modeling, Identification, Calibration
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#wheeled_mobile_robots" title="Click to go to the Keyword Index">
               Wheeled Mobile Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The 3D structure of indoor environments is essential for comprehensive robot perception and scene analysis. Current floor plan generation methods predominantly rely on 360-degree cameras or point cloud-based techniques, which are unsuitable for devices with limited sensor capabilities, such as robot vacuums that only have monocular cameras and wheel odometry sensors. Addressing this gap, our paper introduces a new method for floor plan creation compatible with these simpler devices. Our approach utilizes partially observed monocular images combined with wheel odometry data to detect and assemble mask images. The process involves segmenting the ceiling area from these images, overlaying the segmented masks, and applying homography transformations guided by the wheel odometry data. The methodology includes the development of two deep learning models: one for segmenting the ceiling from partially visible images, and another for refining the mask for enhanced processing. The segmentation model is designed to improve performance through the use of continuous image sequences, while the refinement model focuses on the hierarchical prediction of vertices, edges, and surfaces of the ceiling. This approach offers a practical solution for floor plan generation in settings where advanced sensory equipment is not available.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_16">
             15:20-16:30, Paper WI5A.16
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('149'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning-Based Orientation Estimation Using Continuous Representation for SO(3)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387220" title="Click to go to the Author Index">
             Seo, Youngrang
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387222" title="Click to go to the Author Index">
             Kim, Hajun
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#324358" title="Click to go to the Author Index">
             Kang, Dongyun
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#219098" title="Click to go to the Author Index">
             Kim, Joon-Ha
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology(KAIST)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#138298" title="Click to go to the Author Index">
             Park, Hae-Won
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab149" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#foundations_of_sensing_and_estimation" title="Click to go to the Keyword Index">
               Foundations of Sensing and Estimation
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study proposes a learning-based approach for estimating the orientation of a 6D rigid body object. The framework estimates the orientation in SO(3) by utilizing only the sensor values from Inertial Measurement Units. We validate the performance of the proposed orientation estimator by comparing it with the Invariant extended Kalman filter in the simulation of a 6D rigid body object moving in various poses.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_17">
             15:20-16:30, Paper WI5A.17
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('150'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Magnetic Legged-Robot Foot Design for Diverse Ferromagnetic Terrains with Differential Mechanisms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387203" title="Click to go to the Author Index">
             Kim, Hyunseok
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387427" title="Click to go to the Author Index">
             Um, Yong
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#313945" title="Click to go to the Author Index">
             Kim, Gijeong
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology, KAIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#138298" title="Click to go to the Author Index">
             Park, Hae-Won
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab150" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#mechanism_and_design" title="Click to go to the Keyword Index">
               Mechanism and Design
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#search_and_rescue_robotics" title="Click to go to the Keyword Index">
               Search and Rescue Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes design of a magnetic legged robot foot designed to adhere to various ferromagnetic terrains, including uneven, stepped, and curved surfaces. For the robot to navigate these challenging environments, we introduce a novel magnetic foot equipped with multiple miniaturized electropermanent magnet toes (MME) and a differential ankle mechanism (DAM). The MME comprises eight coin-sized electropermanent magnets (EPMs) that can adaptively adhere to surfaces characterized by unevenness, steps, and curves. DAM seamlessly integrates the MME and evenly distributes the force among the eight EPMs using a pulley differential mechanism. Consequently, our study demonstrates that multiple EPMs can be adaptively attached to curved and uneven surfaces.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_18">
             15:20-16:30, Paper WI5A.18
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('154'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Research and Scenario Experiments on Computer Vision for Enhancing the Performance of AI-Powered Prosthetic Hand
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#350223" title="Click to go to the Author Index">
             Kang, Jeon-Seong
            </a>
           </td>
           <td class="r">
            KIRO
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#221207" title="Click to go to the Author Index">
             Beom-Joon, Park
            </a>
           </td>
           <td class="r">
            KIRO
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387054" title="Click to go to the Author Index">
             Yoon, Junwon
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence (KIRO)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387056" title="Click to go to the Author Index">
             Song, Ha-Yoon
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#305510" title="Click to go to the Author Index">
             Kim, Jungjun
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#170598" title="Click to go to the Author Index">
             Chung, Hyun-Joon
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab154" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#computer_vision_and_visual_servoing" title="Click to go to the Keyword Index">
               Computer Vision and Visual Servoing
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#robotic_hands" title="Click to go to the Keyword Index">
               Robotic Hands
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Capturing the relationship and position of individual joints of a real human hand is a critical task in the study of AI-Powered Prosthetic Hands. Computer Vision technology aids in acquiring training datasets for this research and understanding the relationship between joints according to actions. In this paper, we research Computer Vision for AI-Powered Prosthetic Hands and conduct experiments applying CV algorithms to various scenarios. Based on the experimental results, we identify current limitations and problems, and propose directions for future research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_19">
             15:20-16:30, Paper WI5A.19
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('159'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Shape Memory Alloy-Driven Finger Haptic Device
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#319779" title="Click to go to the Author Index">
             Kang, Beomchan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#107881" title="Click to go to the Author Index">
             Majidi, Carmel
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab159" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#haptics" title="Click to go to the Keyword Index">
               Haptics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#actuation_and_actuators" title="Click to go to the Keyword Index">
               Actuation and Actuators
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#soft_robotics" title="Click to go to the Keyword Index">
               Soft Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In response to the escalating demand for immersive engagements in Virtual Reality (VR) and Augmented Reality (AR), the imperative for advanced haptic feedback systems, particularly for finger interactions, has grown significantly. This study introduces a novel Shape Memory Alloy (SMA)-driven finger haptic device tailored to deliver skin stretch feedback. The SMA actuator configuration, comprising four serpentine structures, enables individual or simultaneous control with diverse actuation patterns. Integrated seamlessly into a 3D printed flexible guide structure and an elastic cover, the proposed device effectively mitigates the challenges of bulkiness and weight associated with traditional designs. Emphasizing its streamlined and lightweight nature, the study delves into the dynamic performances of the SMA-driven finger haptic device on a fingertip. This haptic device with SMA actuators showcased remarkable achievements, realizing a total of eight distinct motions in both tangential and diagonal directions. Furthermore, this system excels in user-finger compatibility, seamlessly integrating into daily activities. Experimental results not only underscore its thin and compact system but also affirm its potential applications in VR and AR environments, marking a significant stride in the evolution of haptic technology.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_20">
             15:20-16:30, Paper WI5A.20
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('161'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Detection of Thruster Fault of an Unmanned Surface Vehicle
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#253183" title="Click to go to the Author Index">
             Ko, Nak Yong
            </a>
           </td>
           <td class="r">
            Chosun University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388424" title="Click to go to the Author Index">
             Song, Gyeongsub
            </a>
           </td>
           <td class="r">
            Chosun Univerity
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#130223" title="Click to go to the Author Index">
             Choi, Hyun-Taek
            </a>
           </td>
           <td class="r">
            Korea Research Institute of Ships and Oceans Engineering
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#134382" title="Click to go to the Author Index">
             Sur, Joono
            </a>
           </td>
           <td class="r">
            Korea Naval Academy
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab161" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#underwater_robotics" title="Click to go to the Keyword Index">
               Underwater Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#actuation_and_actuators" title="Click to go to the Keyword Index">
               Actuation and Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a method that detects the faults in the operation of the thruster of an unmanned surface vehicle (USV). The method uses the dynamic model of the USV, and detects the fault in real-time. The dynamic model describes the USV velocity as a function of the thruster rotation speed. The proposed method compares the measured speed of the USV and the speed estimated using the dynamic model. The proposed method is verified by offshore experiments using a USV.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_21">
             15:20-16:30, Paper WI5A.21
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('173'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Study on Point Cloud Map Matching Positioning of AGVs Using LiDAR and IMU Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388523" title="Click to go to the Author Index">
             Jang, Jae-Hun
            </a>
           </td>
           <td class="r">
            Pukyong National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388565" title="Click to go to the Author Index">
             Lee, Min Su
            </a>
           </td>
           <td class="r">
            Pukyong National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#130236" title="Click to go to the Author Index">
             Lee, Kyung-Chang
            </a>
           </td>
           <td class="r">
            Pukyong National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab173" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#range__sonar__gps_and_inertial_sensing" title="Click to go to the Keyword Index">
               Range, Sonar, GPS and Inertial Sensing
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study is about positioning an AGV in the open air through LiDAR and IMU fusion. In this study, we estimate the position of an AGV through a proposed two-stage matching using LiDAR and IMU on the AGV
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_22">
             15:20-16:30, Paper WI5A.22
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('179'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Robot Autonomous Exploration and Mapping under Localization Uncertainty Via Reinforcement Learning on Graphs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#277429" title="Click to go to the Author Index">
             Huang, Yewei
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#270493" title="Click to go to the Author Index">
             Lin, Xi
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#123501" title="Click to go to the Author Index">
             Englot, Brendan
            </a>
           </td>
           <td class="r">
            Stevens Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab179" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#ai_reasoning_methods_for_robotics" title="Click to go to the Keyword Index">
               AI Reasoning Methods for Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a Deep Reinforcement Learning (DRL) based autonomous exploration algorithm designed for distributed multi-robot teams, which takes into account map and localization uncertainties of range-sensing mobile robots. An exploration graph, incorporating current SLAM pose estimation and potential future actions, is introduced to characterize the robot state at each iteration. A Graph Neural Network (GNN) is integrated into DRL agents to enhance their understanding of the topology within the exploration graph. The results of our experiments demonstrate the algorithm’s capacity to strike a balance between ensuring map uncertainty and achieving efficient exploration with a multi-robot team.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_23">
             15:20-16:30, Paper WI5A.23
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('185'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Visual SLAM through Manipulator for Unexplored Areas Tracking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387009" title="Click to go to the Author Index">
             Hong, Hyeonwook
            </a>
           </td>
           <td class="r">
            Jeonbuk National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#104627" title="Click to go to the Author Index">
             Park, Jaebyung
            </a>
           </td>
           <td class="r">
            Jeonbuk National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab185" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#simultaneous_localization_and_mapping__slam_" title="Click to go to the Keyword Index">
               Simultaneous Localization and Mapping (SLAM)
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile manipulators are widely used and are expected to see even broader applications due to the increasing demand for automation. These mobile manipulators perceive their surroundings through Simultaneous Localization and Mapping (SLAM). However, the integration of mobile robots and manipulators for unified SLAM has not been extensively studied. The visual SLAM of a mobile robot with a statically linked camera required rotation to change the camera orientation unrelated to the robot's position movement due to its limited field of view (FOV), which resulted in an inefficient driving path decision. "In this work, we address the previously mentioned problem by tracking unexplored areas with a sensor-equipped manipulator. The manipulator autonomously adjusts the camera orientation without rotating the base and scans unexplored areas identified through point cloud density. Areas where scanning is impossible due to obstacles always maintain a low point cloud density, which can cause the manipulator to become trapped. These shadowed areas are identified through vertical point cloud density and excluded from the target angle. The experiment was conducted through gazebo simulation. The experiment showed that manipulator assistance SLAM is possible with a more efficient driving path than the statically linked camera only method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_24">
             15:20-16:30, Paper WI5A.24
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('187'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Study on the FishBack Dataset with Fish Dorsal Images for Fish Cage Farm Monitoring Using ROV
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388611" title="Click to go to the Author Index">
             Kang, Jung-Ho
            </a>
           </td>
           <td class="r">
            Pukyong National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388610" title="Click to go to the Author Index">
             Keruzel, Tatiana
            </a>
           </td>
           <td class="r">
            Pukyong National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#130236" title="Click to go to the Author Index">
             Lee, Kyung-Chang
            </a>
           </td>
           <td class="r">
            Pukyong National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab187" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#underwater_robotics" title="Click to go to the Keyword Index">
               Underwater Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Various methods have been proposed to develop smart aquaculture, but fish detection models designed for automated fish monitoring suffer from problems such as illumination changes, low contrast, high noise, fish deformation, frequent occlusion, and dynamic background. In this study, a dataset containing images of fish backs was developed to solve the problem that models trained with existing fish dataset models cannot detect the backs of fish. These limitations arise when monitoring fish in cage farms using ROVs. By evaluating the results of training the fish detection model using the proposed dataset, an mAP of 96.7% was achieved for the test dataset. Additionally, the model successfully detected fish even when the fish were not visible due to lack of contrast with the net.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_25">
             15:20-16:30, Paper WI5A.25
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('188'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Training Quadrotor PID Controller Using Particle Swarm Optimization for Collaborative Navigation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#388593" title="Click to go to the Author Index">
             Rodriguez, Eric
            </a>
           </td>
           <td class="r">
            The University of Texas at Rio Grande Valley
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#189075" title="Click to go to the Author Index">
             Lu, Qi
            </a>
           </td>
           <td class="r">
            The University of Texas Rio Grande Valley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab188" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#aerial_and_flying_robots" title="Click to go to the Keyword Index">
               Aerial and Flying Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Energy expenditure for quadrotor control has a likelihood of being costly given parameter-dependent controllers that are less than optimal. The cost can grow proportionally when applied to multiple quadrotors for tracking and collaborative navigation tasks. This research aims to establish a basic approach to tuning PID (Proportional-Integral-Derivative) parameters for a simulated quadrotor drone. Implementing a PID controller for autonomy provides a straightforward method for correcting robotic movement based on its current state. However, applying a PID system to a flight controller poses challenges with an inherently under-actuated system, which includes the likelihood of large overshoots and lengthy adjustment times. To address this, we propose utilizing Particle Swarm Optimization (PSO) for tuning PID parameters in a simulated quadrotor using Webots. The PSO algorithm is employed to find optimal PID values for thrust, yaw, and translational PIDs for x- and y-positions by identifying converging values across randomly created particles. The results demonstrate converging properties for particles that achieve minimal fitness scores, particularly in reducing overshoot. The results indicate that the optimized PID controller outperforms the default PID controller without optimization. We also present a proposed application for transferring our PSO implementation to find optimal gains for a physical quadrotor, including carrying found parameters into multiple physical robots for collaborative navigation and tracking.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_26">
             15:20-16:30, Paper WI5A.26
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('207'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Quantifying Physical Burden Using Muscle Activity of Caregivers: Care Robot-Aided Transfer vs Manual Transfer
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401112" title="Click to go to the Author Index">
             Park, So Seul
            </a>
           </td>
           <td class="r">
            Hanyang University, Seoul
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401036" title="Click to go to the Author Index">
             Shin, Yong Soon
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401058" title="Click to go to the Author Index">
             YOUNG A, LEE
            </a>
           </td>
           <td class="r">
            Hanyang Uriversity, Seoul, Korea
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401109" title="Click to go to the Author Index">
             KIM, MI YOUNG
            </a>
           </td>
           <td class="r">
            HANYANG UNIVERSITY
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401055" title="Click to go to the Author Index">
             Jang, Hye-Young
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab207" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#rehabilitation_and_healthcare_robotics" title="Click to go to the Keyword Index">
               Rehabilitation and Healthcare Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this study, we quantitatively compared muscle activity during manual care and robot-assisted care when a caregiver assisted a caregiver in transferring from a bed to a wheelchair. The muscle activity measurement and comparison system can be used to quantitatively evaluate workload reduction by applying various manual care systems and robot-based care systems to elderly care tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_27">
             15:20-16:30, Paper WI5A.27
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('208'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Development of a Work Analysis Model for Nursing Care Using Robotic Technology Based on the Job Strain Index
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401058" title="Click to go to the Author Index">
             YOUNG A, LEE
            </a>
           </td>
           <td class="r">
            Hanyang Uriversity, Seoul, Korea
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401036" title="Click to go to the Author Index">
             Shin, Yong Soon
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401109" title="Click to go to the Author Index">
             KIM, MI YOUNG
            </a>
           </td>
           <td class="r">
            HANYANG UNIVERSITY
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401055" title="Click to go to the Author Index">
             Jang, Hye-Young
            </a>
           </td>
           <td class="r">
            Hanyang University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401112" title="Click to go to the Author Index">
             Park, So Seul
            </a>
           </td>
           <td class="r">
            Hanyang University, Seoul
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab208" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#rehabilitation_and_healthcare_robotics" title="Click to go to the Keyword Index">
               Rehabilitation and Healthcare Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Most of care works are known to be major harmful risk factors for musculoskeletal disorders.	Transferring care involves a high proportion of physical labor and causes physical strain to caregivers. To evaluate transfer care using ergonomic tool, a work analysis model based on Job Strain Index was developed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_28">
             15:20-16:30, Paper WI5A.28
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('209'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Advancing Parking Robot Systems Enhanced Perception and Localization Utilizing Sensor Fusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401028" title="Click to go to the Author Index">
             IN, SUNGUK
            </a>
           </td>
           <td class="r">
            HL Mando
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401878" title="Click to go to the Author Index">
             Lim, Joonhoo
            </a>
           </td>
           <td class="r">
            HL Mando
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401501" title="Click to go to the Author Index">
             LIM, HEEJEONG
            </a>
           </td>
           <td class="r">
            HL Mando
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#276490" title="Click to go to the Author Index">
             Kim, Kyuwon
            </a>
           </td>
           <td class="r">
            Konkuk University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401678" title="Click to go to the Author Index">
             Jeong, woojae
            </a>
           </td>
           <td class="r">
            HL Mando
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#350006" title="Click to go to the Author Index">
             Cho, Youngha
            </a>
           </td>
           <td class="r">
            HL Mando
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab209" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents an innovative parking robot system designed to alleviate urban parking congestion. The system incorporates advanced recognition and localization methodologies crucial to its parking function. Empirical operations conducted in a real parking lot environment validate the effectiveness of our approach, particularly in ensuring the smooth operation of parking robot systems. Using the proposed method can be offer a promising solution to the persistent issue of urban parking congestion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_29">
             15:20-16:30, Paper WI5A.29
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('210'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Research on Generating Elevation Maps for Quadruped Robots in Smoky Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401240" title="Click to go to the Author Index">
             Park, Min Cheol
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401242" title="Click to go to the Author Index">
             Lee, Han-Wool
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#401243" title="Click to go to the Author Index">
             Choi, Young Joo
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#115550" title="Click to go to the Author Index">
             Hwang, Jung-Hoon
            </a>
           </td>
           <td class="r">
            Korea Eletronics Technology Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab210" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#multisensor_data_fusion" title="Click to go to the Keyword Index">
               Multisensor Data Fusion
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#legged_robots" title="Click to go to the Keyword Index">
               Legged Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#robotics_in_hazardous_applications" title="Click to go to the Keyword Index">
               Robotics in Hazardous Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this study, we proposed a method to remove fog and generate an elevation map to ensure effective terrain recognition even in dense fog or smoky environments, and verified the proposed method through experiments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_30">
             15:20-16:30, Paper WI5A.30
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('213'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Linear Motion Guide Fault Diagnosis in Complex Motion Conditions Using Multi-Modal 1D-CNN
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#402597" title="Click to go to the Author Index">
             Oh, Kyoung-whan
            </a>
           </td>
           <td class="r">
            Samsung Electronics
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#402599" title="Click to go to the Author Index">
             Lee, JeeHyong
            </a>
           </td>
           <td class="r">
            Sungkyunkwan University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#402585" title="Click to go to the Author Index">
             Choi, Yeon-Woo
            </a>
           </td>
           <td class="r">
            Samsung Electronics
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab213" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#foundations_of_sensing_and_estimation" title="Click to go to the Keyword Index">
               Foundations of Sensing and Estimation
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#contact__modeling__sensing_and_control_" title="Click to go to the Keyword Index">
               Contact: Modeling, Sensing and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#actuation_and_actuators" title="Click to go to the Keyword Index">
               Actuation and Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, linear motion (LM) guides have been used in high-tech facilities or robots that require high precision or high-speed operations. In the event of a sudden failure of an LM guide, an automated factory can experience production delays. The LM guide includes high-speed motion and complex motion, which can reduce the diagnostic accuracy of the existing fault diagnosis methodology based on single input of torque ripple. To overcome this difficulty, this paper proposes a method to increase diagnostic accuracy by weighting motion information to concatenate failure information. The proposed method concatenates three images: the noise-removed speed signal containing motion information, the speed spectrogram, and the torque spectrogram, which contain fault information. To address this issue, the Multi-Modal 1D-CNN methodology is utilized to integrate and extract features from heterogeneous signals for fault detection. Validation was conducted with data concerning LM guide failures in actual industrial settings. The proposed method was confirmed to enhance diagnostic accuracy.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_31">
             15:20-16:30, Paper WI5A.31
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('214'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Development of Map-Based Task Configuration Method and Control Technology for a High Wall Painting Mobile Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#110874" title="Click to go to the Author Index">
             Jung, Eui-Jung
            </a>
           </td>
           <td class="r">
            Korea Institute of Robot and Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#310303" title="Click to go to the Author Index">
             Kang, Minseok
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#186014" title="Click to go to the Author Index">
             SHIN, JUSEONG
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#237808" title="Click to go to the Author Index">
             Park, Sang Hyun
            </a>
           </td>
           <td class="r">
            Korea Institute of Robot and Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#302765" title="Click to go to the Author Index">
             KIM, JUHYUN
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Techonlogy Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#114738" title="Click to go to the Author Index">
             Kim, Murim
            </a>
           </td>
           <td class="r">
            Korea Institute of Robot and Convergence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab214" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#wheeled_mobile_robots" title="Click to go to the Keyword Index">
               Wheeled Mobile Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#robotics_in_hazardous_applications" title="Click to go to the Keyword Index">
               Robotics in Hazardous Applications
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#intelligent_robotic_vehicles" title="Click to go to the Keyword Index">
               Intelligent Robotic Vehicles
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a robot that enhances worker safety, increases efficiency, and improves the quality of painting work in high-height painting operations. Sophisticated navigation and control technology has been developed to automatically complete these painting tasks. Previous mural robots had limited movement capabilities and were mainly suitable for low working heights. The key to the efficiency of high wall painting robots lies in the implementation of map-based task organization and control technology. This paper aims to address specific challenges associated with high mural operations by developing innovative map-based task organization methods and control techniques, with the goals of improving safety, increasing efficiency, and improving overall performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_32">
             15:20-16:30, Paper WI5A.32
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('215'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Accelerated Gradient Descent for High Frequency Model Predictive Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#378550" title="Click to go to the Author Index">
             Zhang, Jianghan
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#296707" title="Click to go to the Author Index">
             Jordana, Armand
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#107817" title="Click to go to the Author Index">
             Righetti, Ludovic
            </a>
           </td>
           <td class="r">
            New York University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab215" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#manipulation_planning_and_control" title="Click to go to the Keyword Index">
               Manipulation Planning and Control
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#dynamics_and_control" title="Click to go to the Keyword Index">
               Dynamics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The recent promises of Model Predictive Control in robotics have motivated the development of tailored second-order methods to solve optimal control problems efficiently. While those methods benefit from strong convergence properties, tailored efficient implementations are challenging to derive. In this work, we study the potential effectiveness of first-order methods and show on a torque controlled manipulator that they can equal the performances of second-order methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_33">
             15:20-16:30, Paper WI5A.33
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('216'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Reinforcement Learning-Based Modification Structure Behavior Tree (RLMS-BT) for Task Planning in a Patrolling Mission
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#221207" title="Click to go to the Author Index">
             Beom-Joon, Park
            </a>
           </td>
           <td class="r">
            KIRO
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#350223" title="Click to go to the Author Index">
             Kang, Jeon-Seong
            </a>
           </td>
           <td class="r">
            KIRO
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387054" title="Click to go to the Author Index">
             Yoon, Junwon
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence (KIRO)
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#387056" title="Click to go to the Author Index">
             Song, Ha-Yoon
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics &amp; Technology Convergence
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#170598" title="Click to go to the Author Index">
             Chung, Hyun-Joon
            </a>
           </td>
           <td class="r">
            Korea Institute of Robotics and Technology Convergence
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab216" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#behavior_based_systems" title="Click to go to the Keyword Index">
               Behavior-Based Systems
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#robotic_systems_architectures_and_programming" title="Click to go to the Keyword Index">
               Robotic Systems Architectures and Programming
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#motion_planning_and_obstacle_avoidance" title="Click to go to the Keyword Index">
               Motion Planning and Obstacle Avoidance
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Behavior Trees (BTs) are an effective method for organizing autonomous agent tasks in robotics and AI. However, conventional BTs' rigid design limits flexibility and exhibits limitations in designing intelligent agents with adaptable behaviors. This paper proposes a Reinforcement Learning-based Modification Structure of Behavior Tree (RLMS-BT), which is a modified design of a conventional BT structure aimed at applying RL algorithms. In this design, Sub-nodes are transformed into RL nodes through hierarchical restructuring, which can enhance overall performance by optimizing task sequence. Furthermore, a task planning framework is presented, which enables the optimization of the modified BT structure using various RL algorithms. In the simulation of task planning within an obstacle environment, the RLMS-BT structure exhibits superior adaptability, surpassing conventional BT from a time-cost perspective. This demonstrates its potential for implementing complex behavioral systems in autonomous agents.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_34">
             15:20-16:30, Paper WI5A.34
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('217'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              OPC UA-Based System Architecture for Robot Manipulators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#318419" title="Click to go to the Author Index">
             Cho, Chang Nho
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#154622" title="Click to go to the Author Index">
             Jung, Byung-jin
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#154839" title="Click to go to the Author Index">
             Kim, Tae-Keun
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#115550" title="Click to go to the Author Index">
             Hwang, Jung-Hoon
            </a>
           </td>
           <td class="r">
            Korea Eletronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#102369" title="Click to go to the Author Index">
             Ryu, Jee-Hwan
            </a>
           </td>
           <td class="r">
            Korea Advanced Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab217" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#robotic_systems_architectures_and_programming" title="Click to go to the Keyword Index">
               Robotic Systems Architectures and Programming
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In order to achieve intelligent manufacturing system capable of high-mix low-volume production, it is essential to integrate multiple systems including sensors, robot manipulators, mobile robots and other systems. Open Platform Communications Unified Architecture (OPC UA) is one of the most promising methods for such integration. In this study, an OPC UA-based control architecture for robot manipulators is proposed. The OPC UA client acts as the main process controller while each device, such as robot manipulators or sensors, contains OPC UA servers. This configuration allows the main controller to effectively control multiple devices by sending requests. Furthermore, this configuration enables easy modularization of each device, which is particularly helpful for high-mix low-volume manufacturing. To illustrate the feasibility of the proposed control architecture, the proposed architecture is applied to a system consists of a robot manipulator and a camera.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_35">
             15:20-16:30, Paper WI5A.35
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('218'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Integrated Control System for Multiple Heterogeneous Drones Based on oneM2M IoT Platform with Human-Centered User Interface
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349680" title="Click to go to the Author Index">
             Ahn, Il-Yeop
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349675" title="Click to go to the Author Index">
             Lee, Jiho
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349676" title="Click to go to the Author Index">
             Park, Jong-Hong
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab218" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#aerial_and_flying_robots" title="Click to go to the Keyword Index">
               Aerial and Flying Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Unmanned Aerial Vehicles (UAVs), commonly called drones, are a promising and revolutionary technology with the potential to impact industry and the living environment. We present an integrated and intuitive human-centered collaborative management system for multiple drones based on oneM2M IoT platform. The system consists of CGCS (Central Ground Control System), FGCS (Field Ground Control System), and XR (eXtended Reality) platform to support the management and operation of multiple drones in the field. By adapting user-centered factors such as voice controlling and hand gesture interfaces, we expect to adapt human-centered factors in the UAV management environment to achieve intuitive aerial operations in the future.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_36">
             15:20-16:30, Paper WI5A.36
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('219'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ground Control System with Communication Quality Information for Unmanned Aerial Vehicles
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349676" title="Click to go to the Author Index">
             Park, Jong-Hong
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349675" title="Click to go to the Author Index">
             Lee, Jiho
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#349680" title="Click to go to the Author Index">
             Ahn, Il-Yeop
            </a>
           </td>
           <td class="r">
            Korea Electronics Technology Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab219" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#aerial_and_flying_robots" title="Click to go to the Keyword Index">
               Aerial and Flying Robots
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper is about the development of a ground control system to support flight path setting in terms of communication connection stability for unmanned aerial vehicles. The proposed ground control system aims to build an environment that allows to select the flight path by utilizing communication quality data obtained through actual measurements. We describe the structure of an global IoT standard-based server platform for storing and managing measured communication quality data. As a result, we propose a ground control system that links actual measurement data with the system and allows the operator to set the route in terms of communication when setting the flight path.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_37">
             15:20-16:30, Paper WI5A.37
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('220'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Class-Wise Confidence Thresholding for OOD Detection in Robot Vision-Based Applications
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#403479" title="Click to go to the Author Index">
             Jihyun, Hwang
            </a>
           </td>
           <td class="r">
            Electronics and Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#115484" title="Click to go to the Author Index">
             Jang, Minsu
            </a>
           </td>
           <td class="r">
            Electronics &amp; Telecommunications Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab220" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#ai_reasoning_methods_for_robotics" title="Click to go to the Keyword Index">
               AI Reasoning Methods for Robotics
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#object_recognition" title="Click to go to the Keyword Index">
               Object Recognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study proposes a novel method for detecting Out-Of-Distribution (OOD) data in image classification tasks, aimed at improving the trustworthiness of robot vision tasks e.g. surveillance or safety inspection. The proposed approach utilizes class-wise confidence thresholds, determined analytically through a grid search, to effectively identify data that falls outside the model's training distribution. Experimental results demonstrate that the proposed method achieves competitive performance across various OOD detection scenarios, with significant improvements in Area Under the Receiver Operating Characteristic (AUROC) curve and False Positive Rate at 95% True Positive Rate (FPR95) compared to existing research. By accurately detecting model uncertainty, this study contributes to expanding the scope of indoor safety check robots, enhancing system reliability, safety, and efficiency. The proposed method's feasibility in improving the trustworthiness of robot vision intelligence highlights its potential for real-world applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_38">
             15:20-16:30, Paper WI5A.38
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('221'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Trajectory Analysis for Collision Detection in Foraging Robot Swarms
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#403521" title="Click to go to the Author Index">
             Gonzalez, Arturo
            </a>
           </td>
           <td class="r">
            University of Texas at Rio Grande Valley
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#189075" title="Click to go to the Author Index">
             Lu, Qi
            </a>
           </td>
           <td class="r">
            The University of Texas Rio Grande Valley
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab221" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot swarms exhibit optimal efficiency when operating with a small number of robots. However, as the size of a robot swarm increases, the collective performance on tasks such as foraging tends to decline due to heightened competition rather than cooperation among individual robots. A primary obstacle encountered during swarm navigation within search arenas is the occurrence of collisions among robots. Our objective is to enhance the scalability of robot swarms, enabling them to efficiently execute foraging tasks even with a large number of robots. To achieve this goal, we introduce a novel trajectory analysis method designed to predict potential collisions for each robot. By collecting and analyzing 2-dimensional trajectory data from a simulated environment featuring 16 robots engaged in foraging tasks, we categorized the data into congested and normal states, subsequently evaluating the accuracy of this classification. Utilizing the predictions generated by our method, robots can adjust their behaviors proactively to mitigate the likelihood of collisions, thereby enhancing foraging performance and overall swarm scalability. Furthermore, our preliminary results underscore the potential of trajectory analysis to identify anomalies within robot swarms, offering promising prospects for future research in anomaly detection methodologies.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_39">
             15:20-16:30, Paper WI5A.39
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('222'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Modal Tactile Sensors Based on Design-Optimized Carbon Nanocomposites for Wearable Applications
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#403545" title="Click to go to the Author Index">
             Park, Young-Bin
            </a>
           </td>
           <td class="r">
            Ulsan National Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#403546" title="Click to go to the Author Index">
             Jeong, Changyoon
            </a>
           </td>
           <td class="r">
            Yeongnam University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab222" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Tactile sensors for wearable applications, e.g., electronic skins, robotic manipulators, etc., often require multi-modal sensing ability that enables combinations of pressure, shear, and bending stress/strain measurements. Carbon nanomaterials (CNMs) and CNM-derived polymer composites offer a high degree of freedom for sensor design and performance tunability. This paper presents recent studies on CNM-based, multi-modal tactile sensors, namely: (1) pressure-shear multi-modal sensor based on carbon nanocomposite pillar arrays; (2) multi-layered, micro-patterned nanocomposite pressure sensor with modulus gradient for wide-range sensitivity; and (3) bio-inspired, carbon nanotube(CNT)-coated hexagonal micro-columnar arrays with interlocking structure for pressure, shear and bending sensing.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="wi5a_40">
             15:20-16:30, Paper WI5A.40
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('9'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Investigating Electrodermal Activity for Trust Assessment in Industrial Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#310229" title="Click to go to the Author Index">
             Campagna, Giulio
            </a>
           </td>
           <td class="r">
            Aalborg University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#108136" title="Click to go to the Author Index">
             Chrysostomou, Dimitrios
            </a>
           </td>
           <td class="r">
            Aalborg University
           </td>
          </tr>
          <tr>
           <td>
            <a href="UR24_AuthorIndexWeb.html#158245" title="Click to go to the Author Index">
             Rehm, Matthias
            </a>
           </td>
           <td class="r">
            Aalborg University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab9" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="UR24_KeywordIndexWeb.html#physical_and_cognitive_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical and Cognitive Human-Robot Interaction
              </a>
              ,
              <a href="UR24_KeywordIndexWeb.html#behavior_based_systems" title="Click to go to the Keyword Index">
               Behavior-Based Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the Industry 5.0 framework, due to the close collaboration between humans and robots, providing a safe environment and balance workload becomes an essential requirement. In this context, evaluating the trustworthiness of robots from a human-centric perspective is essential as trust impacts the interaction in human-robot collaborations. Numerous researchers in the literature have delved into physiological responses as indicators of user trust in robots. In this research endeavor, multiple machine learning models were employed, leveraging skin conductance response (SCR) to classify the trust level of the human operator. A chemical industry scenario was developed, where a collaborative robot supported a human operator by handing over a beaker used for the pouring of chemicals. The machine learning models achieved a moderate accuracy rate of 68.99% and AUC of 0.73 for the handover task. Nonetheless, this study underscores the importance of sensor fusion techniques to improve the accuracy of trust assessment within the context of human-robot collaborations.
            </div>
           </td>
          </tr>
         </table>
        </div>
        <p>
         <br/>
        </p>
        <p>
         <br/>
        </p>
        <p>
         <p>
         </p>
        </p>
       </td>
       <td height="100%" style="background-color:#2E5286;" width="5">
       </td>
      </tr>
      <tr>
       <td alt="" border="0" colspan="4" height="16" style="background-color:#2E5286;" valign="center" width="100%">
        <p align="center">
         <span style="font-size:8pt;line-height:10pt;color:#ffffff;">
          Technical Content © IEEE Robotics &amp; Automation Society
         </span>
        </p>
       </td>
      </tr>
      <tr>
       <td colspan="4" width="100%">
        <p align="right">
         <span style="text-decoration:none;">
          <img align="right" border="0" src="/images/pc_logo_small.png" style="margin-left: 10px; margin-right: 10px"/>
          This site is protected
by copyright and trademark laws under US and International law.
          <br/>
          All rights
reserved. © 2002-2024 PaperCept, Inc.
          <br/>
          Page generated 2024-07-01  01:22:01 PST
          <a href="" onclick="window.open('/conferences/scripts/about.pl','tc','width=1000,scrollbars=yes'); return false">
           Terms of use
          </a>
         </span>
        </p>
       </td>
      </tr>
     </table>
    </body>
   </div>
  </form>
 </body>
</html>
