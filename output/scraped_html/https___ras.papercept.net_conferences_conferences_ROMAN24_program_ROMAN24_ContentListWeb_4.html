<!DOCTYPE HTML>
<html>
 <head>
  <meta content="en-us" http-equiv="Content-Language"/>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="width=device-width" name="viewport"/>
  <script src="https://ras.papercept.net/conferences/scripts/dom-drag.js" type="text/javascript">
  </script>
  <script src="jquery-1.11.1.min.js">
  </script>
  <title>
   RO-MAN 2024 Program | Thursday August 29, 2024
  </title>
  <style type="text/css">
   body, table, td, th{
	Font-Family : sans-serif;
	Font-Size : 10pt;
}
.r {text-align: right}
.blue {color: #0000FF;}
td {vertical-align: top; text-align: left}
.c {text-align: center}
table.s {
	border-collapse:collapse;
	border-width: 1px;
}
table.s td{
	border-width: 1px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
table.t {
	border-collapse: collapse;
	border-width: 0px;
}
table.t td{
	border-width: 0px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
.dots {
    background:url('./images/dot.gif') repeat-x center;
}
.field {
    background-color: #FFFFFF;
}
#pTitle { /* Page title */
   font-size: 14pt;
   line-height: 1.5em;
}
#pSubTitle { /* Page subtitle */
   color: #909090;
   font-size: 10pt; 
   line-height: 1.5em;
}
#container {
	position: absolute;
	width: 100%;
	margin-top: 2px;
/*	overflow: hidden; */
}

.sHdr {   /* Session header Content list */
   background-color: #F0E68C
}
      
.sSHdr {   /* Subsession header Content list */
   background-color: #f8f3c6 
}
      
table.trk { /* Track table Content list */
   border-collapse: collapse;
   border-width: 0px;
   margin: auto;
/**   width: 640px; **/
   width: 720px;
}
table.trk td{
   border-width: 0px;
   padding: 4px;
   border-style: solid;
   border-color: gray;
 }
      
.pHdr {  /* Paper header Content list */
   background-color: #E6E6FA;
   color: black;
}
hr.thin { /* Horizontal rule content list */
   border: 0px; 
   height: .8px; 
   background-color: #8888FF;
}
      
.pTtl {  /* Paper title Content list */
   font-size: 11pt;
   font-style: italic;
}
      
.ssHdr {  /* Subsession header container session Content list */
   background-color: #DDDDDD;
   color: black;
}
      
.ssTtl {  /* Subsession title container session Content list */
   font-size: 10pt;
   font-style: normal;
   font-weight: bold;
}
  </style>
  <script language="JavaScript">
   function initXMLHttp(){
   var oRequest = false;
   try {
      oRequest = new XMLHttpRequest();
   }  catch (trymicrosoft) {
      try {
         oRequest = new ActiveXObject("Msxml2.XMLHTTP");
      }  catch (othermicrosoft) {
         try {
            oRequest = new ActiveXObject("Microsoft.XMLHTTP");
         }  catch (failed) {
            oRequest = false;
         }
      }
   }
   if (!oRequest){
      alert("Error initializing XMLHttpRequest! Your browser does not support AJAX");
   }
   return oRequest;
}
function modify(number,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'Add';
   }
   else{
      action = 'Delete';
   }
   
//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=431&' + action + number;
//   window.open(url,'myprogrampage');

   modifyItem("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","431",action,number)

}


function modifyItem(url,ConfID,action,number){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&Number=' + number;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

var iIntervalId;  // Global variable
function modsession(id,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'AddSession';
   }
   else{
      action = 'DelSession';
   }

//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=431&' + action + id;
//   window.open(url,'myprogrampage');

   modifySession("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","431",action,id)

}

function modifySession(url,ConfID,action,id){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&ID=' + id;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

function getCookie(sName){
   var sRE = "(?:; )?" + sName + "=([^;]*);?";   
   var oRE = new RegExp(sRE);
   if (oRE.test(document.cookie)){
      return decodeURIComponent(RegExp["$1"]);}
   else{
      return null;
   }
}
function loadprogram(){
   var list = getCookie("ROMAN24");
   if (list){
      var List = list.split(",");
      for (var i=0; i<List.length; i++){
         var names = document.getElementsByName('modify' + List[i]);
         if (names.length){
            for (var j=0; j<names.length; j++){
               names[j].checked = true;
            }
         }
      }
   }
}
function reset(){

   // Uncheck all modify and addsession checkboxes

   var ins = document.getElementsByTagName('input');
   for (var i=0; i<ins.length; i++){
      if (ins[i].type == 'checkbox' && ins[i].id && ins[i].id.substring(0,3) == 'mod'){
         ins[i].checked = false;
      }
   }
   
   // Reload the program
   
   loadprogram();
}
function startreset(){
   iIntervalId = setInterval(reset,2000);
}
function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
var uhash;
var pColor;
$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){
   
      // Mark the session
   
      pColor = $('#' + uhash).parent().css('backgroundColor');
      $('#' + uhash).parent().css('backgroundColor','#FF8888');
   }
});


$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){

      // Set the widths
      
      setwidth();
      
      // If claasical view is required then return

      if (!ghit){return;} 
      
      // Reset left margin for FF

      document.getElementById('container').scrollLeft = 0;;

      // Discover the table and the block and determine the block Id
   
      var rt = $('#' + uhash);
      var done = false;
      while (!done){
         rt = rt.parent();    
         var etype = rt.get(0).tagName;  
         if (rt.is("table")){      
            done = true;
         }
      }
      rt = rt.parent().parent().parent();
      var iid = rt.attr('id')

      // Show the block

      initialize();
      $('#' + iid).show();
      $( '#A' + iid ).focus();
      var ypos = $('#' + iid).offset().top;      
      window.scrollTo(0,ypos);

      // Cancel the scroll to uhash

      var url = location.href;
      url += '_';
      location.href = url;
      
      // Scroll into view

      var leftPosition = $('#' + uhash).parent().position().left;
      var topOffset = $('#' + uhash).parent().offset().top;
      var divOffset = $('#' + iid).find('div').offset().top;
      var topPosition = topOffset-divOffset;
      $('#' + iid).find('div').scrollLeft(leftPosition);
      $('#' + iid).find('div').scrollTop(topPosition);
   }
   else{
      setwidth();
      initialize();
   }
});

var ghit = false;
function setwidth(){
   var viewportwidth = $( window ).width();
   var viewportheight = $( window ).height();
   var sdiv = $( ".sdiv" );
   for (var i=0; i<sdiv.length; i++){
      $(sdiv[i]).css({width: .98*viewportwidth + 'px'});
      $(sdiv[i]).css("height", .9*viewportheight-50 + 'px');      
   }

   // Detect horizontal overflow on any of the divs
   
   var divs = document.getElementsByTagName('div');
   for (var i=0; i<divs.length; i++){
      if (divs[i].id && divs[i].id.substring(0,3) == 'div'){
         if (divs[i].scrollWidth > divs[i].clientWidth){
            ghit = true;
            break;
         }
      }
   }
   if (!ghit){
      for (var i=0; i<divs.length; i++){
         divs[i].style.height = 'auto';
      }
   }
}

function selfollowing(hsh){
   $('#' + uhash).parent().css('backgroundColor',pColor);
   setwidth();
   initialize();
   if (hsh == 'TheTop'){
      var ypos = $('#container').offset().top;
      window.scrollTo(0,ypos)
   }
   else{
      $('#' + hsh).show();
      $( '#A' + hsh ).focus();
      var ypos = $('#' + hsh).offset().top;
      window.scrollTo(0,ypos)
   }
}

function initialize(){

   // Show all day blocks
   
   var blcks = $('.blck');
   for (var i=0; i<blcks.length; i++){
      blcks[i].style.display = 'block';
   }

   // Detect horizontal overflow on any of the divs
   
   var hit = false;
   var divs = document.getElementsByTagName('div');
   for (var i=0; i<divs.length; i++){
      if (divs[i].id && divs[i].id.substring(0,3) == 'div'){
         if (divs[i].scrollWidth > divs[i].clientWidth || divs[i].scrollHeight > divs[i].clientHeight){
            hit = true;
            break;
         }
      }
   }
   if (hit){
   
      // Set overflow hidden on body. This will prevent it from scrolling
      
      $("body").css("overflow", "hidden");
      document.getElementById('start').style.display = 'inline';
      
      // Hide all day blocks
   
      var blcks = $('.blck');
      for (var i=0; i<blcks.length; i++){
         blcks[i].style.display = 'none';
      }
      var scrlis = $('.scrlis');
      for (var i=0; i<scrlis.length; i++){
         scrlis.show();
      }
   }
   else{
      $("body").css("overflow", "auto");
      document.getElementById('start').style.display = 'none';
      var blcks = $('.sdiv');
      for (var i=0; i<blcks.length; i++){
        blcks[i].style.height = 'auto';
      }
      var scrlis = $('.scrlis');
      for (var i=0; i<scrlis.length; i++){
         scrlis.hide();
      }
   }
   return;
}
  </script>
 </head>
 <body onresize="setwidth(); initialize()">
  <form action="https://ras.papercept.net/conferences/scripts/myprogram.pl" name="myprogram">
   <div id="container">
    <body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0">
     <link href="https://fonts.googleapis.com" rel="preconnect"/>
     <link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/>
     <link href="https://fonts.googleapis.com/css2?family=Antonio:wght@500;700;900&amp;display=swap" rel="stylesheet"/>
     <table border="0" cellpadding="0" cellspacing="0" width="100%">
      <tr>
       <td alt="" height="140;" nowrap="" style="background-color:#000;" width="100%">
        <img alt="" border="0" height="140" src="/images/roman/roman24.png" style="position:absolute;top:0px;left:0px;"/>
        <span style="font-size: 34px; font-family: Antonio, sans-serif; font-weight: 900; color: #fff;line-height:40px;transform:scale(1.25,1);
text-align: left; position: absolute; top: 4px; left: 238px; text-shadow: 0px 0px 0px #ffffff, 0px 0px 0px #ffffff;">
         IEEE RO-MAN 2024
         <br/>
        </span>
        <span style="font-size: 18px; font-family: Antonio, sans-serif; font-weight: 900; color: #fff;line-height:40px;
text-align: left; position: absolute; top: 40px; left: 212px; text-shadow: 0px 0px 0px #ffffff, 0px 0px 0px #ffffff;">
         33RD IEEE INTERNATIONAL CONFERENCE ON ROBOT AND HUMAN INTERACTIVE COMMUNICATION
        </span>
        <span style="font-size: 16px; font-family: Antonio, sans-serif; font-weight: 900; color: #F5BB77;line-height:40px;
text-align: left; position: absolute; top: 80px; left: 212px; text-shadow: 0px 0px 0px #ffffff, 0px 0px 0px #ffffff;">
         EMBRACING HUMAN-CENTERED HRI
        </span>
        <span style="font-size: 12px; font-family: Antonio, sans-serif; font-weight: 900; color: #fff;line-height:40px;
text-align: left; position: absolute; top: 100px; left: 212px; text-shadow: 0px 0px 0px #ffffff, 0px 0px 0px #ffffff;">
         AUGUST 26-30, 2024 - PASADENA - CALIFORNIA, USA
        </span>
       </td>
      </tr>
      <tr>
       <td alt="" border="0" height="1" style="background-color:#000;" width="100%">
       </td>
      </tr>
     </table>
     <table border="0" cellpadding="0" cellspacing="0" height="80%" width="100%">
      <tr>
       <td height="100%" style="background-color:#000;" width="5">
       </td>
       <td width="5">
       </td>
       <td height="100%" valign="top" width="100%">
        <br/>
        <div class="c" id="TheTop">
         <span id="pTitle">
          <a href="http://www.ro-man2024.org" target="_blank">
           <b>
            2024 33rd IEEE International Conference on Robot and Human Interactive Communication (ROMAN)
           </b>
          </a>
          <br/>
         </span>
         <span id="pSubTitle">
          <b>
           August 26-30, 2024, Pasadena, CA, USA
          </b>
         </span>
         <br/>
         <br/>
        </div>
        <div class="c" style="position: relative">
         <a href="ROMAN24_ProgramAtAGlanceWeb.html">
          Program at a Glance
         </a>
         <a href="ROMAN24_ContentListWeb_1.html">
          Monday
         </a>
         <a href="ROMAN24_ContentListWeb_2.html">
          Tuesday
         </a>
         <a href="ROMAN24_ContentListWeb_3.html">
          Wednesday
         </a>
         <a href="ROMAN24_ContentListWeb_4.html">
          Thursday
         </a>
         <a href="ROMAN24_ContentListWeb_5.html">
          Friday
         </a>
         <a href="ROMAN24_AuthorIndexWeb.html">
          Author Index
         </a>
         <a href="ROMAN24_KeywordIndexWeb.html">
          Keyword Index
         </a>
        </div>
        <div class="c">
         <p style="color: gray">
          Last updated on July 31, 2024. This conference program is tentative and subject to change
         </p>
        </div>
        <div class="c">
         <h3>
          Technical Program for Thursday August 29, 2024
         </h3>
        </div>
        <p class="c">
        </p>
        <div class="c">
         <span style="color:gray ">
          To show or hide the keywords and abstract (text summary) of a paper (if available), click on the paper title
         </span>
         <br/>
         <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">
          Open all abstracts
         </a>
         <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">
          Close all abstracts
         </a>
        </div>
        <div class="c">
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpl">
             <b>
              ThPL
             </b>
            </a>
           </td>
           <td class="r">
            Room T9 - Ballroom
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thpl" title="Click to go to the Program at a Glance">
             <b>
              Plenary Talk - Ayanna Howard
             </b>
            </a>
           </td>
           <td class="r">
            Plenary Session
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tham1_br">
             <b>
              ThAM1_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tham1_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break - TI
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that1">
             <b>
              ThAT1
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#that1" title="Click to go to the Program at a Glance">
             <b>
              Human Modeling for Adaptive Interactions and Robot Autonomy I
             </b>
            </a>
           </td>
           <td class="r">
            Special Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_01">
             10:15-10:30, Paper ThAT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('156'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Human-Variability-Respecting Optimal Control for Physical Human-Machine Interaction (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#322035" title="Click to go to the Author Index">
             Kille, Sean
            </a>
            (Karlsruhe Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#399767" title="Click to go to the Author Index">
             Leibold, Paul
            </a>
            (Karlsruhe Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#318672" title="Click to go to the Author Index">
             Karg, Philipp
            </a>
            (Karlsruhe Institute of Technology (KIT)),
            <a href="ROMAN24_AuthorIndexWeb.html#244980" title="Click to go to the Author Index">
             Varga, Balint
            </a>
            (Karlsruhe Institute of Technology (KIT), Campus South),
            <a href="ROMAN24_AuthorIndexWeb.html#245275" title="Click to go to the Author Index">
             Hohmann, Sören
            </a>
            (Institute of Control Systems, Karlsruhe Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab156" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Physical Human-Machine Interaction plays a pivotal role in facilitating collaboration across various domains. When designing appropriate model-based controllers to assist a human in the interaction, the accuracy of the human model is crucial for the resulting overall behavior of the coupled system. When looking at state-of-the-art control approaches, most methods rely on a deterministic model or no model at all of the human behavior. This poses a gap to the current neuroscientific standard regarding human movement modeling, which uses stochastic optimal control models that include signal-dependent noise processes and therefore describe the human behavior much more accurate than the deterministic counterparts. To close this gap by including these stochastic human models in the control design, we introduce a novel design methodology resulting in a Human-Variability-Respecting Optimal Control that explicitly incorporates the human noise processes and their influence on the mean and variability behavior of a physically coupled human-machine system. Our approach results in an improved overall system performance, i.e. higher accuracy and lower variability in target point reaching, while allowing to shape the joint variability, for example to preserve human natural variability patterns.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_02">
             10:30-10:45, Paper ThAT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('328'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Experimental Evaluation of ROS-Causal in Real-World Human-Robot Spatial Interaction Scenarios (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#321567" title="Click to go to the Author Index">
             Castri, Luca
            </a>
            (University of Lincoln),
            <a href="ROMAN24_AuthorIndexWeb.html#219638" title="Click to go to the Author Index">
             Beraldo, Gloria
            </a>
            (National Research Council of Italy),
            <a href="ROMAN24_AuthorIndexWeb.html#199572" title="Click to go to the Author Index">
             Mghames, Sariah
            </a>
            (University of Lincoln),
            <a href="ROMAN24_AuthorIndexWeb.html#104948" title="Click to go to the Author Index">
             Hanheide, Marc
            </a>
            (University of Lincoln),
            <a href="ROMAN24_AuthorIndexWeb.html#101540" title="Click to go to the Author Index">
             Bellotto, Nicola
            </a>
            (University of Padua)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab328" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Deploying robots in human-shared environments requires a deep understanding of how nearby agents and objects interact. Employing causal inference to model cause-and-effect relationships facilitates the prediction of human behaviours and enables the anticipation of robot interventions. However, a significant challenge arises due to the absence of implementation of existing causal discovery methods within the ROS ecosystem, the standard de-facto framework in robotics, hindering effective utilisation on real robots. To bridge this gap, in our previous work we proposed ROS-Causal, a ROS-based framework designed for onboard data collection and causal discovery in human-robot spatial interactions. In this work, we present an experimental evaluation of ROS-Causal both in simulation and on a new dataset of human-robot spatial interactions in a lab scenario, to assess its performance and effectiveness. Our analysis demonstrates the efficacy of this approach, showcasing how causal models can be extracted directly onboard by robots during data collection. The online causal models generated from the simulation are consistent with those from lab experiments. These findings can help researchers to enhance the performance of robotic systems in shared environments, firstly by studying the causal relations between variables in simulation without real people, and then facilitating the actual robot deployment in real human environments.
             <br/>
             ROS-Causal: https://lcastri.github.io/roscausal
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_03">
             10:45-11:00, Paper ThAT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('211'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Kinesthetic Teaching in Robotics: A Mixed Reality Approach (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#309292" title="Click to go to the Author Index">
             Macciò, Simone
            </a>
            (University of Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#352883" title="Click to go to the Author Index">
             Shaaban, Mohamad
            </a>
            (University of Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#213447" title="Click to go to the Author Index">
             Carfì, Alessandro
            </a>
            (University of Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#108313" title="Click to go to the Author Index">
             Mastrogiovanni, Fulvio
            </a>
            (University of Genoa)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab211" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As collaborative robots become more common in manufacturing scenarios and adopted in hybrid human-robot teams, we should develop new interaction and communication strategies to ensure smooth collaboration between agents. In this paper, we propose a novel communicative interface that uses Mixed Reality as a medium to perform Kinesthetic Teaching (KT) on any robotic platform. We evaluate our proposed approach in a user study involving multiple subjects and two different robots, comparing traditional physical KT with holographic-based KT through user experience questionnaires and task-related metrics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that1_04">
             11:00-11:15, Paper ThAT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('241'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Challenges in Detecting and Analyzing EEG Error-Related Potentials: Lessons from a Case Study in HRI (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399718" title="Click to go to the Author Index">
             fava, alessandra
            </a>
            (University of Modena and Reggio Emilia),
            <a href="ROMAN24_AuthorIndexWeb.html#400074" title="Click to go to the Author Index">
             lucchese, adriana
            </a>
            (University of Modena and Reggio Emilia),
            <a href="ROMAN24_AuthorIndexWeb.html#173992" title="Click to go to the Author Index">
             Meattini, Roberto
            </a>
            (University of Bologna),
            <a href="ROMAN24_AuthorIndexWeb.html#103582" title="Click to go to the Author Index">
             Palli, Gianluca
            </a>
            (University of Bologna),
            <a href="ROMAN24_AuthorIndexWeb.html#192312" title="Click to go to the Author Index">
             Villani, Valeria
            </a>
            (University of Modena and Reggio Emilia),
            <a href="ROMAN24_AuthorIndexWeb.html#122186" title="Click to go to the Author Index">
             Sabattini, Lorenzo
            </a>
            (University of Modena and Reggio Emilia)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab241" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recently, electroencephalographic (EEG) signals have been used to enhance Human-Robot Interaction (HRI). In particular, Error-Related Potentials (ErrPs) have been exploited since very few years. These potentials are evoked when there is a mismatch between the command given by the subject and the movements of the robot, or if the user’s expectation is different from the robot or other human behavior. These signals can be used to improve and customize the robot system, as feedback to better adapt the robot to human needs. This work aims to investigate and detect the ErrPs during different interaction tasks. We set up an experiment divided into five different tasks, where every task has 120 events with a 25%- 35% probability of error. The robot used in the experiment is a Baxter robot and the commands from the subject to the robot are sent in two different ways: with a keyboard or with a motion capture device. This work aims to reproduce a simplified teleoperated pick and place task. However, the achieved results do not allow to correctly identify the ErrPs, but exhibit only some minor differences between trials with and without errors. Hence, we here analyze the reasons behind such negative results, focusing on the challenges of the structure and the setup of the experiment. We analyze the possible problems and provide some recommendations to overcome them in similar use cases.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that2">
             <b>
              ThAT2
             </b>
            </a>
           </td>
           <td class="r">
            Room T2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#that2" title="Click to go to the Program at a Glance">
             <b>
              Digital Health Applications of Social Robots
             </b>
            </a>
           </td>
           <td class="r">
            Special Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_01">
             10:15-10:30, Paper ThAT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('347'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Ins and Outs of Socially Assistive Robots: Sensors and Behaviors of a Therapeutic Robot for Depression Management (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#300253" title="Click to go to the Author Index">
             Collins, Sawyer
            </a>
            (Indiana University Bloomington),
            <a href="ROMAN24_AuthorIndexWeb.html#133284" title="Click to go to the Author Index">
             Henkel, Zachary
            </a>
            (Mississippi State University),
            <a href="ROMAN24_AuthorIndexWeb.html#327764" title="Click to go to the Author Index">
             Henkel, Kenna Baugus
            </a>
            (Mississippi State University),
            <a href="ROMAN24_AuthorIndexWeb.html#171202" title="Click to go to the Author Index">
             Bennett, Casey C.
            </a>
            (DePaul University),
            <a href="ROMAN24_AuthorIndexWeb.html#390309" title="Click to go to the Author Index">
             Stanojevic, Cedomir
            </a>
            (Clemson University),
            <a href="ROMAN24_AuthorIndexWeb.html#320269" title="Click to go to the Author Index">
             Piatt, Jennifer
            </a>
            (Indiana University-Bloomington),
            <a href="ROMAN24_AuthorIndexWeb.html#114415" title="Click to go to the Author Index">
             Bethel, Cindy L.
            </a>
            (Mississippi State University),
            <a href="ROMAN24_AuthorIndexWeb.html#116082" title="Click to go to the Author Index">
             Sabanovic, Selma
            </a>
            (Indiana University Bloomington)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab347" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Using socially assistive robots (SARs) as specialized companions for those living with depression to manage symptoms provides a unique opportunity for exploration of robotic systems as comfort objects. Moreover, the robotic components allow for specialized behavioral responses to particular stimuli, as preferred by the user. We have conducted semi-structured interviews with 10 participants about the zoomorphic robot’s Therabot™ desired behaviors and focus groups with five additional participants regarding the preferred sensors within the Therabot™ system. In this paper, using the data from interviews and focus groups, we explore SAR input and output for depression management. While participants overall expected the robot to respond in much similar ways as a well-trained service animal, they expressed interest in the robot understanding unique information about the environment and the user, such as when the user might need interaction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_02">
             10:30-10:45, Paper ThAT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('234'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Exploratory Analysis of Trust in Socially Assistive Robot Interactions with Unpaid Carers of Older Adults (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#358719" title="Click to go to the Author Index">
             Gul, Aisha
            </a>
            (Cardiff University),
            <a href="ROMAN24_AuthorIndexWeb.html#358715" title="Click to go to the Author Index">
             Turner, Liam
            </a>
            (Cardiff University),
            <a href="ROMAN24_AuthorIndexWeb.html#358717" title="Click to go to the Author Index">
             Fuentes, Carolina
            </a>
            (Cardiff University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab234" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The global aging population is growing, leading to an increased need for unpaid carers to support older adults. Socially Assistive Robots (SARs) can play an important role in supporting unpaid carers. However, as with any socio-technical system, building and maintaining trust is critical for achieving full potential and adoption of a SAR. Despite the growing interest in robotics across various domains, there has been limited research on time-dependant patterns of trust during Human Robot Interactions (HRI) and how unpaid carers perceive SARs as a support tool for care. We present a study in which we a) investigate the level of trust that unpaid carers have in using SARs for taking care of older adults, b) qualitatively explore the factors that influence trust considering the dynamic and context dependant nature of care. To explore SARs as a support tool from the perspective of unpaid carers we recruited 15 caregivers, we conducted co-design sessions, interviews and carers interacted with a Pepper robot evaluating trust before and after interaction. We discuss how the level of trust of unpaid carers in SARs increases after interaction and how unpaid carers trust in SARs is impacted by socially and culturally intelligent robots. Our work contributes to identify factors influencing trust that are important for carer’s while interacting with SARs, offering insights for the development and implementation of future SAR-based as a support tool that can assist them in taking care of older adults.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_03">
             10:45-11:00, Paper ThAT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('246'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Alpha Mini Social Robot As a Fitness Trainer at Home (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#281779" title="Click to go to the Author Index">
             De Carolis, Berardina Nadja
            </a>
            (University of Bari),
            <a href="ROMAN24_AuthorIndexWeb.html#221285" title="Click to go to the Author Index">
             Palestra, Giuseppe
            </a>
            (University of Bari),
            <a href="ROMAN24_AuthorIndexWeb.html#400092" title="Click to go to the Author Index">
             Bochicchio, Mario
            </a>
            (University of Bari A. Moro),
            <a href="ROMAN24_AuthorIndexWeb.html#101766" title="Click to go to the Author Index">
             Mazzoleni, Stefano
            </a>
            (Polytechnic University of Bari)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab246" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Engaging in regular exercise offers numerous mental and physical health benefits. Many people, after the Covid-19 pandemic, started to work out at home. However, performing exercises incorrectly can result in injuries. For this reason, technology can be an effective tool for encouraging physical activity at home, and social robots have emerged as a potential solution for providing fitness training in a safe and engaging manner. A physical robot is often perceived as more socially attractive compared to virtual agents. Despite the growing popularity and existing literature on social robotics-based fitness solutions, there is a lack of research examining the accuracy of these systems and interventions at home during physical exercise recognition.In this paper, we propose a novel approach utilizing the Ubtech alpha mini social robot, which possesses motor capabilities to demonstrate exercises. We endowed the robot with the capability to recognize exercise correctness and motivate users to engage in physical activities. In this project, first of all, a new dataset of physical exercises was created, including 1,500 videos of physical exercises. Then, a new approach to recognize physical exercises from image sequences was presented. This method was implemented in a personal social robot to recognize physical exercises using a deep learning model. The results of the study validate the effectiveness of the system in recognizing physical exercises through the camera onboard a personal social robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that2_04">
             11:00-11:15, Paper ThAT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('369'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multimodal and Multi-Lingual Deep Neural Network for Interactive Behavior Style Recognition from Uncontrolled Video-Logs of Children with Autism (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#361680" title="Click to go to the Author Index">
             Zhao, Zhenhao
            </a>
            (George Washington University),
            <a href="ROMAN24_AuthorIndexWeb.html#400351" title="Click to go to the Author Index">
             Chung, Eunsun
            </a>
            (Yonsei University),
            <a href="ROMAN24_AuthorIndexWeb.html#364175" title="Click to go to the Author Index">
             Lee, Myungeun
            </a>
            (George Washington University),
            <a href="ROMAN24_AuthorIndexWeb.html#400354" title="Click to go to the Author Index">
             Kyong-Mee, Chung
            </a>
            (Yonsei University),
            <a href="ROMAN24_AuthorIndexWeb.html#109904" title="Click to go to the Author Index">
             Park, Chung Hyuk
            </a>
            (George Washington University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab369" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#computational_architectures" title="Click to go to the Keyword Index">
               Computational Architectures
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             With the increase of prevalence in autism, the need for efficient public health support has been amplified. Socially-assistive robots (SARs) have been found effective in engaging and interacting with autistic children, however, the perception intelligence during interaction still needs more domain-specific knowledge in terms of understanding children's behaviors. The Family Observation Schedule-Second Version (FOS-II) is one of the key methods in assessing parent-child interactions in developmental disabilities, yet its manual annotation demands considerable time and effort. This study proposes a multi-modal artificial intelligence (AI) model using video and audio inputs for automated FOS-II annotation. Utilizing advanced deep learning for behavior recognition, this method offers rapid, cost-effective FOS-II scaling. It will thus enhance the capability of socially assistive robots to understand human behaviors and support the advancement of digital health research for children with autism. The visual perception in home settings are most likely based on uncontrolled environments, so it is crucial to develop algorithms that can robustly work with video-log data with uncontrolled quality. Ultimately, it aims to ease the burden on parents and caregivers, streamlining the monitoring and treatment of challenging behaviors in autism.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that3">
             <b>
              ThAT3
             </b>
            </a>
           </td>
           <td class="r">
            Room T3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#that3" title="Click to go to the Program at a Glance">
             <b>
              Child-Robot Interaction I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_01">
             10:15-10:30, Paper ThAT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('45'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotising Psychometrics: Validating Wellbeing Assessment Tools in Child-Robot Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#327842" title="Click to go to the Author Index">
             Abbasi, Nida Itrat
            </a>
            (University of Cambridge),
            <a href="ROMAN24_AuthorIndexWeb.html#388650" title="Click to go to the Author Index">
             Laban, Guy
            </a>
            (University of Cambridge),
            <a href="ROMAN24_AuthorIndexWeb.html#327851" title="Click to go to the Author Index">
             Ford, Tamsin
            </a>
            (University of Cambridge),
            <a href="ROMAN24_AuthorIndexWeb.html#327854" title="Click to go to the Author Index">
             Jones, Peter B.
            </a>
            (University of Cambridge),
            <a href="ROMAN24_AuthorIndexWeb.html#151713" title="Click to go to the Author Index">
             Gunes, Hatice
            </a>
            (University of Cambridge)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab45" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The interdisciplinary nature of Child-Robot Interaction (CRI) fosters incorporating measures and methodologies from many established domains. However, when employing CRI approaches to sensitive avenues of health and wellbeing, caution is critical in adapting metrics to retain their safety standards and ensure accurate utilisation. We conducted a secondary analysis to previous empirical work, investigating the reliability and construct validity of established psychological questionnaires such as the Short Moods and Feelings Questionnaire (SMFQ) and three subscales (generalised anxiety, panic and low mood) of the Revised Child Anxiety and Depression Scale (RCADS) within a CRI setting for the assessment of mental wellbeing. Through confirmatory principal component analysis, we have observed that these measures are reliable and valid in the context of CRI. Furthermore, our analysis revealed that scales communicated by a robot demonstrated a better fit than when self-reported, underscoring the efficiency and effectiveness of robot-mediated psychological assessments in these settings. Nevertheless, we have also observed variations in item contributions to the main factor, suggesting potential areas of examination and revision (e.g., relating to physiological changes, inactivity and cognitive demands) when used in CRI. Our findings highlight the importance of verifying the reliability and validity of standardised metrics and assessment tools when employed in CRI settings, thus, aiming to avoid any misinterpretations and misrepresentations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_02">
             10:30-10:45, Paper ThAT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('331'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Acceptability of a Home-Based Humanoid Robot for Deaf Children with Cochlear Implants and Their Family: A First Step towards a Remote Speech-Language Training Tool
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#363181" title="Click to go to the Author Index">
             Stiti, Sabrina
            </a>
            (University of Toulouse III Paul Sabatier),
            <a href="ROMAN24_AuthorIndexWeb.html#364284" title="Click to go to the Author Index">
             Caroux, Loïc
            </a>
            (Clle-Csc Umr 5263 Ut2j-Cnrs),
            <a href="ROMAN24_AuthorIndexWeb.html#364285" title="Click to go to the Author Index">
             Gaillard, Pascal
            </a>
            (Clle Umr 5263 Ut2j-Cnrs),
            <a href="ROMAN24_AuthorIndexWeb.html#182378" title="Click to go to the Author Index">
             Paubel, Pierre-Vincent
            </a>
            (CLLE-LTC UMR 5263 UT2J-CNRS, Toulouse),
            <a href="ROMAN24_AuthorIndexWeb.html#400251" title="Click to go to the Author Index">
             Barone, Pascal
            </a>
            (Cerco Umr 5549 Cnrs-Ut3),
            <a href="ROMAN24_AuthorIndexWeb.html#364286" title="Click to go to the Author Index">
             Deguine, Olivier
            </a>
            (Service D’oto-Rhino-Laryngologie, Oto-Neurologie Et O.R.L Pédiat)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab331" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study explores the acceptability of a humanoid robot in the homes of deaf children with cochlear implants. Traditional audiologic rehabilitation, usually conducted at hospitals or clinics, significantly improves communication but geographical distance and parental availability pose challenges for families. Introducing home training with a humanoid robot could provide greater equity in care distribution, aid in the child's progress, and offer an ecological approach. Here, ten families (11 cochlear-implanted (CI) children aged 7–12 years and their parents) participated in a study evaluating the acceptability of the humanoid robot (called Pepper) in their homes for one month. We identified two distinct clusters among the 11 CI children. The first cluster ("younger CI Children") demonstrated better and higher scores for acceptability, usability, usefulness, emotion, and fun, and used the robot more than children in the second cluster ("older CI children"). We found a decrease in both duration of use and number of applications launched by CI children each week when interacting with the robot. Parents showed similar results to their children, with parents of younger CI children demonstrating greater acceptance of the robot. Various technical issues (transcription errors, robot non-responsiveness, and comprehension difficulties) may have affected the robot's acceptability among both CI children and their parents. Both CI children and parents expected humanoid robots to have more advanced capabilities, including high social intelligence, advanced artificial intelligence, voice assistant features, and Natural Language Processing integration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_03">
             10:45-11:00, Paper ThAT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('370'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Bow Ties &amp; Colorful Eyes: Centering Youth Designs of Social Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#335679" title="Click to go to the Author Index">
             Levinson, Leigh
            </a>
            (Indiana University),
            <a href="ROMAN24_AuthorIndexWeb.html#116318" title="Click to go to the Author Index">
             Gomez, Randy
            </a>
            (Honda Research Institute Japan Co., Ltd),
            <a href="ROMAN24_AuthorIndexWeb.html#116082" title="Click to go to the Author Index">
             Sabanovic, Selma
            </a>
            (Indiana University Bloomington)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab370" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#innovative_robot_designs" title="Click to go to the Keyword Index">
               Innovative Robot Designs
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Under UNICEF’s Policy guidance on AI for children, child-centered AI should always ‘ensure inclusion of and for children.’ To spotlight youth visions for robots, we led co-design workshops with children between 5-14 years old. Youth designs were expressive, customized, relatable, and approachable. Based on 54 drawings and descriptions of the social robot Haru, we suggest that future child-centered robots should 1) be expressive across verbal and non-verbal channels of communication, 2) allow for customization to give children more agency when interacting with the robot, 3) adapt to children’s style and hobbies to make them feel seen, and 4) aesthetically keep proportions of robot faces consistent and cartoon-like to make robots more approachable.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that3_04">
             11:00-11:15, Paper ThAT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('96'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              “If a Robot Was Teaching, Then Everybody Would Definitely Like School Better”: An Analysis of Grade 3-5 Children’s Perceptions of Learning STEM Vocabulary with a Social Educational Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#154259" title="Click to go to the Author Index">
             Louie, Wing-Yue Geoffrey
            </a>
            (Oakland University),
            <a href="ROMAN24_AuthorIndexWeb.html#399354" title="Click to go to the Author Index">
             Christ, Tanya
            </a>
            (Oakland University),
            <a href="ROMAN24_AuthorIndexWeb.html#399355" title="Click to go to the Author Index">
             Wowra, Amanda
            </a>
            (Oakland University),
            <a href="ROMAN24_AuthorIndexWeb.html#399358" title="Click to go to the Author Index">
             Alexander, Danielle
            </a>
            (Oakland University),
            <a href="ROMAN24_AuthorIndexWeb.html#355534" title="Click to go to the Author Index">
             Bakhoda, Iman
            </a>
            (Intelligent Robotics Laboratory, Oakland University, Michigan),
            <a href="ROMAN24_AuthorIndexWeb.html#293127" title="Click to go to the Author Index">
             Shahverdi, Pourya
            </a>
            (Oakland University, Michigan, USA)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab96" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This qualitative study explored 20 grade 3-5 children’s perceptions of learning STEM vocabulary with an educational social robot. A semi-structured interview protocol was used to elicit children’s perceptions. Interviews were recorded and transcribed into a database reflecting one talk-turn per row (911 talk-turns total). Two coders used emergent coding and constant comparative method to identify talk-turns that reflected children’s perceptions of the assets and drawbacks of learning STEM vocabulary with the robot. Findings identified new assets and drawbacks about the robot’s instruction, and new drawbacks about the robot’s speech, which were not explored in previous research. Findings suggest design implications, including designing robots with the capacity for more individualization of instruction and adjustable movement and speech features based on learners’ preferences.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that4">
             <b>
              ThAT4
             </b>
            </a>
           </td>
           <td class="r">
            Room T4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#that4" title="Click to go to the Program at a Glance">
             <b>
              Motion Planning and Navigation in Human-Centered Environments III
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_01">
             10:15-10:30, Paper ThAT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('310'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robots on the Road - Investigating Potentials of eHMI-Concepts for HRI to Tackle Critical Situations in Public Spaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400188" title="Click to go to the Author Index">
             Turriziani, Lea
            </a>
            (Ulm University),
            <a href="ROMAN24_AuthorIndexWeb.html#400199" title="Click to go to the Author Index">
             Kraus, Johannes
            </a>
            (Johannes-Gutenberg University of Mainz),
            <a href="ROMAN24_AuthorIndexWeb.html#400206" title="Click to go to the Author Index">
             Zeng, Zhe
            </a>
            (Ulm University),
            <a href="ROMAN24_AuthorIndexWeb.html#400200" title="Click to go to the Author Index">
             Ruess, Stephanie
            </a>
            (Ulm University),
            <a href="ROMAN24_AuthorIndexWeb.html#226469" title="Click to go to the Author Index">
             Kannan, Shyam Sundar
            </a>
            (Purdue University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab310" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots in public spaces need to communicate with lay persons who are not directly involved in the robot task to coordinate their movements and resolve critical situations. Hereby, this communication aims at salience and clarity and at the same time needs to be unobtrusive. While in automated cars, communication with uninvolved road members has been investigated with the label external human-machine interface (eHMI) in human-robot interaction (HRI) this has not been systematically discussed. This study investigates some of the mainly discussed eHMI concepts (blinker lights, beep, and speech) for solving critical situations in HRI. Six critical situations were presented together with five communication strategies (presented as videos) in an online study with N = 175 participants. Mainly, criticality and trust were measured as dependent variables. Overall, situations including visually or hearing-impaired persons were perceived as most critical. For all situations, criticality was reduced with added interaction modalities. The combination of blinker lights and voice was ranked as the most preferred strategy for five situations and led to a reduction in criticality of all situations and higher trust in the robot. The relation between perceived criticality and trust was partially mediated by predictability and transparency. Design recommendations for solving critical situations through robots’ communication strategies in the public are discussed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_02">
             10:30-10:45, Paper ThAT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('255'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Semantic Map Based Robot Navigation with Natural Language Input
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#268801" title="Click to go to the Author Index">
             YANG, Guang
            </a>
            (Stevens Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#321641" title="Click to go to the Author Index">
             huang, xinchi
            </a>
            (Stevens Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#105621" title="Click to go to the Author Index">
             Guo, Yi
            </a>
            (Stevens Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab255" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a new semantic map based robot navigation system in the paper. The system takes human voice input, processes multi-modal data including natural languages and RGB-D images, and generates semantic maps for robot navigation. Making use of recent development in image segmentation tools, we integrate robot mapping and localization with a customized real-time object detection model, so that the semantic and navigation layers are efficiently combined for robot navigation purpose. We demonstrate the performance of our developed algorithms in both simulation and real robot experiments. Compared with existing works, we demonstrate applicability to real robot system and superior performance in terms of success rate.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_03">
             10:45-11:00, Paper ThAT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('125'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Improving UV Disinfection of Objects by a Robot Using Human Feedback
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#251166" title="Click to go to the Author Index">
             Sanchez, Alan Giovanny
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#370844" title="Click to go to the Author Index">
             Bernhart, Nash
            </a>
            (Oregon State University),
            <a href="ROMAN24_AuthorIndexWeb.html#104078" title="Click to go to the Author Index">
             Smart, William
            </a>
            (Oregon State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab125" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#programming_by_demonstration" title="Click to go to the Keyword Index">
               Programming by Demonstration
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#medical_and_surgical_applications" title="Click to go to the Keyword Index">
               Medical and Surgical Applications
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Ultraviolet-C (UV-C) robot irradiation is a promising approach for disinfecting surfaces contaminated by pathogens in healthcare settings. However, limitations exist with current UV disinfection robots, including coverage for complex surface geometries. This research presents a system for human-guided robotic UV disinfection that uses empirical sensor measurements rather than relying on high-accurate models for UV map coverage. Human guidance is integrated into the methodology to enhance disinfection, aiding in addressing complex shaped objects and topologies. Further, a validation test confirmed that our estimation approach reliably underestimates the UV exposure, which is beneficial for ensuring thorough disinfection. Initial user studies demonstrated that while autonomous disinfection was effective for simple objects like tabletops, human-guided disinfection, especially with feedback, improved coverage and speed for complex shapes like mugs. Combining human intuition with autonomy shows potential for enhancing robotic disinfection effectiveness.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that4_04">
             11:00-11:15, Paper ThAT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('209'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Novel Social Navigation Approach Based on Model Predictive Control and Social Force Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399926" title="Click to go to the Author Index">
             Sacco, Federico
            </a>
            (Università Di Modena E Reggio Emilia),
            <a href="ROMAN24_AuthorIndexWeb.html#155018" title="Click to go to the Author Index">
             Recchiuto, Carmine Tommaso
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#156960" title="Click to go to the Author Index">
             Mårtensson, Jonas
            </a>
            (KTH Royal Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab209" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the future, eventually, robots will become extremely widespread also in urban environments, and perhaps, us humans will need to learn how to interact and live with them. Social navigation accounts for the problem of having a safe and efficient navigation among objects and pedestrians, which can be considered as sentient road users and, for this reason, more special considerations need be taken into account when dealing with them. The goal of any social navigation software stack is to make the robotic agent behave as similarly as possible to a pedestrian, which is used to abide to many social rules that has learnt throughout all of their life. In this way, humans will not need to learn new ”robotic” rules for navigating an environment: they would only need to apply the same rules that also robots will follow. Many social navigation approaches rely on sociological-psychological studies in which the pedestrian motion has been modeled in deep details. In this work a novel approach is presented, leveraging the predictivity of Model Predictive Control and the reactivity of Social Force Model, which will model the pedestrian motion.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that5">
             <b>
              ThAT5
             </b>
            </a>
           </td>
           <td class="r">
            Room T5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#that5" title="Click to go to the Program at a Glance">
             <b>
              Multimodal Interaction and Conversational Skills I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_01">
             10:15-10:30, Paper ThAT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('119'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Little Chit-Chat Goes a Long Way: Design and Evaluation of Task and Person-Oriented Styles for Social Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#244933" title="Click to go to the Author Index">
             Saad, Elie
            </a>
            (Delft University of Technology - TUDelft; Holy Spirit University),
            <a href="ROMAN24_AuthorIndexWeb.html#154893" title="Click to go to the Author Index">
             broekens, joost
            </a>
            (Leiden University),
            <a href="ROMAN24_AuthorIndexWeb.html#145192" title="Click to go to the Author Index">
             Neerincx, Mark
            </a>
            (TNO)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab119" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Whereas the reception task is a promising application domain for social robots, knowledge is lacking about how to design the appropriate re-usable communication styles for a reception robot. This paper presents the use and evaluation of an iterative interaction-design (ID) method with which task- and person-oriented multi-modal communication styles have been designed for such a robot. First, we report on an evaluation study of the ID-method with Industrial Design students (N = 13) who designed these two communication styles for a Pepper robot. This provided a set of distinct designs of the two styles, for which the differences in design parameters were in line with social science theory. The task-oriented style showed a more formal, shorter and less chatty communication. Second, we present findings from a Mechanical Turk study conducted to evaluate the perception of these style designs. Participants (N = 301) were presented with videos showing the robot acting as a receptionist and were asked to rate their perception of the robot, the service experience and the orientation of the designs. Overall, the interaction with the robot was appreciated well. The robot with a person-oriented style was perceived to be more animate and likeable. Analysis showed that chit-chat was the main contributor to the perceived difference between the person-oriented and task-oriented styles. This is an important finding as it gives interaction designers a validated best-practice approach to make interaction style more or less personal.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_02">
             10:30-10:45, Paper ThAT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('215'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Anomaly Detection Models for Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399689" title="Click to go to the Author Index">
             Mochizuki, Shota
            </a>
            (Nagoya University),
            <a href="ROMAN24_AuthorIndexWeb.html#399710" title="Click to go to the Author Index">
             Yamashita, Sanae
            </a>
            (Nagoya University),
            <a href="ROMAN24_AuthorIndexWeb.html#399794" title="Click to go to the Author Index">
             Yuasa, Reiko
            </a>
            (Nagoya University),
            <a href="ROMAN24_AuthorIndexWeb.html#399703" title="Click to go to the Author Index">
             Kubota, Tomonori
            </a>
            (Nagoya University),
            <a href="ROMAN24_AuthorIndexWeb.html#118298" title="Click to go to the Author Index">
             Ogawa, Kohei
            </a>
            (Nagoya University),
            <a href="ROMAN24_AuthorIndexWeb.html#360270" title="Click to go to the Author Index">
             Higashinaka, Ryuichiro
            </a>
            (Nagoya University/NTT)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab215" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#linguistic_communication_and_dialogue" title="Click to go to the Keyword Index">
               Linguistic Communication and Dialogue
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dialogue robots powered by large language models can generate advanced utterances. However, the interaction performance is not yet perfect, and the robots sometimes cause problems during interactions. In this study, to detect anomalies in human-robot interactions, we created a dataset and constructed anomaly detection models. For the dataset creation, we collected videos of human-robot interactions in a framework where humans intervene when a dialogue breakdown occurs and labeled the scenes where humans intervened as anomalies. Using this dataset, we built classification models and deep metric learning models utilizing encoders for video, audio, and multimodal information. The results showed that we could successfully train the models and achieve an accuracy and F1-score of over 80%. The performance of the deep metric learning models surpassed that of the classification models, thus demonstrating the importance of separating the differences between classes. We also clarified the importance of audio information.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_03">
             10:45-11:00, Paper ThAT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('376'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Toward a Multi-Dimensional Humor Dataset for Social Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#271540" title="Click to go to the Author Index">
             Zhang, Heng
            </a>
            (ENSTA Paris, Institut Polytechnique De Paris),
            <a href="ROMAN24_AuthorIndexWeb.html#290413" title="Click to go to the Author Index">
             Hei, Xiaoxuan
            </a>
            (ENSTA Paris, Institut Polytechnique De Paris),
            <a href="ROMAN24_AuthorIndexWeb.html#398029" title="Click to go to the Author Index">
             García Cárdenas, Juan José
            </a>
            (ENSTA - Institute Polytechinique De Paris),
            <a href="ROMAN24_AuthorIndexWeb.html#358975" title="Click to go to the Author Index">
             Miao, Xin
            </a>
            (Tsinghua University),
            <a href="ROMAN24_AuthorIndexWeb.html#106757" title="Click to go to the Author Index">
             Tapus, Adriana
            </a>
            (ENSTA Paris, Institut Polytechnique De Paris)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab376" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#narrative_and_story_telling_in_interaction" title="Click to go to the Keyword Index">
               Narrative and Story-telling in Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Expressing humor in social interactions presents a significant challenge for humans due to its intricate linguistic nature. This complexity is further magnified when teaching robots to express humor appropriately. Among the various expressions of humor, jokes are one of the most commonly used. Therefore, a well-annotated joke dataset holds significant promise in enhancing a robot's ability to express humor effectively. This paper introduces a dataset comprising over two thousand jokes, with the aim of providing rich material and multidimensional selection criteria for the humor expression of the robot. The creation of this joke dataset involved a collaborative effort among robot experts studying HRI, psychologists with rich humor research experience, and GPT-3. The annotation process primarily concentrated on four dimensions within the dataset: the humor style of jokes, semantic words (aligned with semantic gestures), keywords, and ratings of joke funniness. We additionally outline several prospective applications of this dataset. We introduce a BERT-based neural network model trained on the dataset with semantic word labels. This model aims to empower robots to choose suitable semantic words from jokes and articulate them alongside corresponding semantic gestures. Moreover, we offer suggestions for utilizing jokes from this dataset to facilitate the adaptive expression of humor by social robots. These endeavors will further enhance the multi-modal humor expression capability of social robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that5_04">
             11:00-11:15, Paper ThAT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('305'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Predictive Turn-Taking: Leveraging Language Models to Anticipate Turn Transitions in Human-Robot Dialogue
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400195" title="Click to go to the Author Index">
             Pinto-Bernal, Maria
            </a>
            (Ghent University—imec),
            <a href="ROMAN24_AuthorIndexWeb.html#126600" title="Click to go to the Author Index">
             Belpaeme, Tony
            </a>
            (Ghent University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab305" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Natural and engaging spoken dialogue systems require seamless turn-taking coordination to avoid awkward interruptions and unnatural pauses. Traditional systems often rely on simplistic silence thresholds, relinquishing the turn after a predetermined period of silence, which invariably leads to a suboptimal interaction experience. This work explores the potential of Large Language Models (LLMs) for improved turn-taking prediction. Building upon research that uses linguistic cues, we investigate how LLMs, with their rich contextual knowledge and semantic encoding of language, can be used for this task. We hypothesize that by analysing dialogue context, syntactic structure, and pragmatic cues within the user's utterance, LLMs can offer more accurate turn-completion predictions. This research evaluates the capabilities of recent LLMs such as Gemini, OpenAI's API, Anthropic's Claude2, and Google AI's Llama 2 to predict turn-ending points solely based on textual information, and demonstrates how the conversation between elderly users and companion robots can be enhanced by LLM-powered end-of-turn prediction.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="that6">
             <b>
              ThAT6
             </b>
            </a>
           </td>
           <td class="r">
            Room T6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#that6" title="Click to go to the Program at a Glance">
             <b>
              Novel Interfaces and Interaction Modalities I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_01">
             10:15-10:30, Paper ThAT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('208'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              How Much Progress Did I Make? an Unexplored Human Feedback Signal for Teaching Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#290020" title="Click to go to the Author Index">
             YU, HANG
            </a>
            (Tufts University),
            <a href="ROMAN24_AuthorIndexWeb.html#399920" title="Click to go to the Author Index">
             Fang, Qidi
            </a>
            (Tufts University),
            <a href="ROMAN24_AuthorIndexWeb.html#399922" title="Click to go to the Author Index">
             Fang, Shijie
            </a>
            (Tufts University),
            <a href="ROMAN24_AuthorIndexWeb.html#202986" title="Click to go to the Author Index">
             Aronson, Reuben
            </a>
            (Tufts University),
            <a href="ROMAN24_AuthorIndexWeb.html#149220" title="Click to go to the Author Index">
             Short, Elaine Schaertl
            </a>
            (Tufts University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab208" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_learning_and_skill_acquisition_via_teaching_and_imitation" title="Click to go to the Keyword Index">
               Social Learning and Skill Acquisition Via Teaching and Imitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Enhancing the expressiveness of human teaching is vital for both improving robots' learning from humans and human-teaching-robot experience. In this work, we characterize and test a little-used teaching signal: textit{progress}, designed to represent the completion percentage of a task. We conducted two online studies with 76 crowd-sourced participants and one public space study with 40 non-expert participants to validate the capability of this progress signal. For the public space study, we collected 40 demonstrations along with progress and scalar feedback through an ice cream topping-adding task. We find that progress could indicate whether the task is successfully performed, reflect the degree of task completion, identify unproductive but harmless behaviors, and may be more consistent across participants. Furthermore, our results show that giving progress does not require extra workload and time. An additional contribution of our work is a dataset of non-expert demonstrations from the public space study, which we observe to be multi-policy and sub-optimal, with sub-optimality not only from teleoperation errors but also from exploratory actions and attempts. The dataset is available at url{https://github.com/TeachingwithProgress/NonExpert_Dem onstrations}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_02">
             10:30-10:45, Paper ThAT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('62'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HGIC: A Hand Gesture Based Interactive Control System for Efficient and Scalable Multi-UAV Operations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#364081" title="Click to go to the Author Index">
             Hu, Mengsha
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#367878" title="Click to go to the Author Index">
             Li, Jinzhou
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#375502" title="Click to go to the Author Index">
             Jin, Runxiang
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#394419" title="Click to go to the Author Index">
             Shi, Chao
            </a>
            (Binghamton University),
            <a href="ROMAN24_AuthorIndexWeb.html#373682" title="Click to go to the Author Index">
             Xu, Lei
            </a>
            (Kent State University),
            <a href="ROMAN24_AuthorIndexWeb.html#238447" title="Click to go to the Author Index">
             Liu, Rui
            </a>
            (Kent State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab62" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As technological advancements continue to expand the capabilities of multi unmanned-aerial-vehicle systems (mUAV), human operators face control challenges in scalability and efficiency due to the large cognitive load and complex operations associated with individual motion adjustments and team coordination. Large cognitive demands limit the feasible size of mUAV teams and require extensive operator training, impeding broader adoption of the mUAV system. This paper developed a Hand Gesture based Interactive Control (HGIC), a novel interface system that integrates computer vision techniques, user interface, robot teaming control, and API developments to intuitively translate human hand gestures into modular control commands for UAV teaming; through learning control models, these commands enable efficient and scalable mUAV motion control and adjustments. HGIC eliminates the need for specialized hardware and minimizes training requirements through natural gestures, improving control efficiency; adaptive control commands will be selected to simplify UAV control and enable scalable control. By reducing operator cognitive burdens, HGIC opens the door for more effective large-scale mUAV applications in complex, dynamic, and uncertain scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_03">
             10:45-11:00, Paper ThAT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('318'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Precise Workcell Sketching from Point Clouds Using an AR Toolbox
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#257450" title="Click to go to the Author Index">
             Zieliński, Krzysztof
            </a>
            (University of Southern Denmark / Universal Robots A/S),
            <a href="ROMAN24_AuthorIndexWeb.html#378908" title="Click to go to the Author Index">
             Blumberg, Bruce
            </a>
            (Universal Robots A/S),
            <a href="ROMAN24_AuthorIndexWeb.html#296610" title="Click to go to the Author Index">
             Mikkel, Kjærgaard
            </a>
            (University of Southern Denmark)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab318" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Capturing real-world 3D spaces as point clouds is efficient and descriptive, but it comes with sensor errors and lacks object parametrization. These limitations render point clouds unsuitable for various real-world applications, such as robot programming, without extensive post-processing (e.g., outlier removal, semantic segmentation). On the other hand, CAD modeling provides high-quality, parametric representations of 3D space with embedded semantic data, but requires manual component creation that is time-consuming and costly. To address these challenges, we propose a novel solution that combines the strengths of both approaches. Our method for 3D workcell sketching from point clouds allows users to refine raw point clouds using an Augmented Reality (AR) interface that leverages their knowledge and the real-world 3D environment. By utilizing a toolbox and an AR-enabled pointing device, users can enhance point cloud accuracy based on the device’s position in 3D space. We validate our approach by comparing it with ground truth models, demonstrating that it achieves a mean error within 1cm - significant improvement over standard LiDAR scanner apps.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="that6_04">
             11:00-11:15, Paper ThAT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('197'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Force-Based Haptic Input Device and Online Motion Generator: Investigating Learning Curves in Robotic Telemanipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#319042" title="Click to go to the Author Index">
             Markert, Timo
            </a>
            (Resense GmbH),
            <a href="ROMAN24_AuthorIndexWeb.html#319086" title="Click to go to the Author Index">
             Matich, Sebastian
            </a>
            (WITTENSTEIN SE),
            <a href="ROMAN24_AuthorIndexWeb.html#370296" title="Click to go to the Author Index">
             Neykov, Daniel
            </a>
            (Resense GmbH),
            <a href="ROMAN24_AuthorIndexWeb.html#319088" title="Click to go to the Author Index">
             Pfannes, Jonas
            </a>
            (WITTENSTEIN SE),
            <a href="ROMAN24_AuthorIndexWeb.html#319044" title="Click to go to the Author Index">
             Theissler, Andreas
            </a>
            (Aalen University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#319045" title="Click to go to the Author Index">
             Atzmueller, Martin
            </a>
            (Osnabrück University, Institute of Computer Science, Semantic In)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab197" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Both robot-assisted surgery (RAS) and future manufacturing systems use telemanipulation setups to enable remote control by surgeons in the operating room and assembly technicians. Precision, intuitive handling, as well as short task execution times have the highest priority. In this paper, we present a novel force-sensing stick and velocity-based online motion generator for a robotic telemanipulator. This custom rigid stick with 6 degrees of freedom (DoF) force/torque (F/T) sensing capabilities is considered for telemanipulation. In a first study, 24 subjects perform three tasks which mimic relevant manipulation maneuvers for industrial assembly and RAS: (1) picking and placing objects, (2) inserting a peg into a hole, and (3) moving the end-effector precisely along a specific pathway. In a second study, three subjects repeat the tasks over a longer period of time in order to assess the learning characteristics and long-term effects on task performance and execution times. For comparison, the same tests are carried out using an off-the-shelf 3 DoF motion-based device. Our results show, that both devices lead to similar performance rates and task execution times over all trials. For the force-sensing stick, subjects report an intuitive and natural response to their force input with no perceptible latency. Conclusions of the long-term study are particularly interesting: within only a few days, task execution times for both input devices can be significantly reduced by 53-69%. The present study builds on previous works of the authors presented at the World Haptics Conference 2023 in Delft.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="tham2_br">
             <b>
              ThAM2_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#tham2_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break - TII
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt1">
             <b>
              ThBT1
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thbt1" title="Click to go to the Program at a Glance">
             <b>
              Human Modeling for Adaptive Interactions and Robot Autonomy II
             </b>
            </a>
           </td>
           <td class="r">
            Special Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_01">
             11:30-11:45, Paper ThBT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('308'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring Engagement in Human-Robot Interaction through the Quantification of Human Spontaneous Movement (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#348306" title="Click to go to the Author Index">
             Casso, Isabel
            </a>
            (Université De Lille),
            <a href="ROMAN24_AuthorIndexWeb.html#225299" title="Click to go to the Author Index">
             Ferreira Chame, Hendry
            </a>
            (University of Lorraine / CNRS),
            <a href="ROMAN24_AuthorIndexWeb.html#105872" title="Click to go to the Author Index">
             Henaff, Patrick
            </a>
            (Université De Lorraine, CNRS, INRIA, LORIA, F-54000 Nancy, Fra),
            <a href="ROMAN24_AuthorIndexWeb.html#348307" title="Click to go to the Author Index">
             Delevoye-Turrell, Yvonne N.
            </a>
            (Univ. Lille, CNRS, UMR 9193 - SCALab - Sciences Cognitives Et Sc)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab308" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#narrative_and_story_telling_in_interaction" title="Click to go to the Keyword Index">
               Narrative and Story-telling in Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#curiosity__intentionality_and_initiative_in_interaction" title="Click to go to the Keyword Index">
               Curiosity, Intentionality and Initiative in Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Developments in the field of social robotics open interesting opportunities for applications in healthcare, education, and services. For this, studying engagement in human-robot interaction (HRI) is crucial for improving the quality of interactive experiences. Questionnaires are powerful in describing voluntary behavior; however, engagement is often an implicit non-voluntary behavior that reaches awareness only once initiated. Inspired by research in cognitive psychology, we propose a behavioral feature to quantify engagement in HRI through the measurement of spontaneous movement and spectral wavelet analysis. For this, we conducted an experiment during which participants listened to sad stories narrated by a moving social robot. Throughout the experiment, we tracked the participants' spontaneous and non-voluntary sway movements with a motion capture system. The experiments were conducted with three robotic platforms (Buddy, Pepper, and Nao). Results showed that spontaneous body sway can be modulated by social robots within no-goal-oriented interaction. This opens up the application possibility of our methodology to various modes of HRI such as collaboration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_02">
             11:45-12:00, Paper ThBT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('340'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              I Am Part of the Robot’s Group: Evaluating Engagement and Group Membership from Egocentric Views (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400264" title="Click to go to the Author Index">
             Grimaldi, Carmine
            </a>
            (Università Degli Studi Di Napoli Federico II),
            <a href="ROMAN24_AuthorIndexWeb.html#182428" title="Click to go to the Author Index">
             Rossi, Alessandra
            </a>
            (University of Naples Federico II),
            <a href="ROMAN24_AuthorIndexWeb.html#128192" title="Click to go to the Author Index">
             Rossi, Silvia
            </a>
            (Universita' Di Napoli Federico II)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab340" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The evaluation of groups’ dynamics and engagement levels in Human-Robot Interaction (HRI) is crucial for enabling a robot to adapt its behavior and achieve sustained interaction. To address these tasks, various methods have been explored, but these aspects are often treated in isolation. In this work, we present a unified machine-learning approach that addresses the recognition of social group dynamics in a multiparty setting, using that information as a possible index of engagement from an egocentric perspective. We introduce an interaction grouping classifier that considers the robot as a potential group member. Our approach utilizes neural networks trained on a combination of an existing dataset and a newly created one, specifically labeled for this study. The proposed method leverages egocentric data to detect social groups and employs this information as an index of engagement, as it requires agents to be part of the same interacting group, including the robot. Experimental results demonstrate high accuracy in both engagement recognition and group detection. Additionally, tests on F-formations reveal complexities in scenarios involving the robot, underscoring the challenges in these configurations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_03">
             12:00-12:15, Paper ThBT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('400'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Investigating User Engagement Dynamics in Robot-To-Human Handovers with a Social Manipulator (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#243029" title="Click to go to the Author Index">
             Sorrentino, Alessandra
            </a>
            (University of Florence),
            <a href="ROMAN24_AuthorIndexWeb.html#243277" title="Click to go to the Author Index">
             La Viola, Carlo
            </a>
            (University of Florence),
            <a href="ROMAN24_AuthorIndexWeb.html#280778" title="Click to go to the Author Index">
             Mancioppi, Gianmaria
            </a>
            (Università Degli Studi Di Firenze),
            <a href="ROMAN24_AuthorIndexWeb.html#400383" title="Click to go to the Author Index">
             Papi, Luca
            </a>
            (University of Florence),
            <a href="ROMAN24_AuthorIndexWeb.html#101896" title="Click to go to the Author Index">
             Cavallo, Filippo
            </a>
            (University of Florence),
            <a href="ROMAN24_AuthorIndexWeb.html#172003" title="Click to go to the Author Index">
             Fiorini, Laura
            </a>
            (University of Florence)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab400" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Socially Assistive Robots (SARs) represent a valid support to professional caregivers in providing care to person with need. To improve the quality of the interaction, SARs should be able to automatically assess user engagement. In this work, we addressed this problem by investigating user engagement dynamics during a robot-to-human handover task, considering 3 main components of engagement: affective, cognitive, and behavioral. For this study, we automatically extracted 10 visual features from the camera recordings of 31 participants. Each individual engaged in eight consecutive sessions with a robot manipulator designed with social cues. Our statistical analysis indicates that prolonged interaction with the robot could influence user engagement. Namely, we observed a decrease in positive emotions (affective), a more regulated quantity of motion (behavioral), and a reduced attention on the robot tasks (cognitive). Overall, the results of this study suggests that engagement dynamics can be described by the selected behavioral features, and that the a more predictable robot’s behavior could negatively influence user engagement.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt1_04">
             12:15-12:30, Paper ThBT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Modeling and Experimental Verification of a Continuous Curvature-Based Soft Growing Manipulator
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#350736" title="Click to go to the Author Index">
             Allen, Justin
            </a>
            (Washington State University),
            <a href="ROMAN24_AuthorIndexWeb.html#350732" title="Click to go to the Author Index">
             Dorosh, Ryan
            </a>
            (Washington State University),
            <a href="ROMAN24_AuthorIndexWeb.html#350737" title="Click to go to the Author Index">
             Ninatanta, Chris
            </a>
            (Washington State University),
            <a href="ROMAN24_AuthorIndexWeb.html#382564" title="Click to go to the Author Index">
             Allen, Andrew
            </a>
            (Washington State University),
            <a href="ROMAN24_AuthorIndexWeb.html#382444" title="Click to go to the Author Index">
             Shui, Linlin
            </a>
            (Communication University of China),
            <a href="ROMAN24_AuthorIndexWeb.html#268920" title="Click to go to the Author Index">
             Yoshida, Kyle
            </a>
            (Washington State University),
            <a href="ROMAN24_AuthorIndexWeb.html#111906" title="Click to go to the Author Index">
             Luo, Jiecai
            </a>
            (Southern University),
            <a href="ROMAN24_AuthorIndexWeb.html#169919" title="Click to go to the Author Index">
             Luo, Ming
            </a>
            (Washington State University)
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt2">
             <b>
              ThBT2
             </b>
            </a>
           </td>
           <td class="r">
            Room T2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thbt2" title="Click to go to the Program at a Glance">
             <b>
              Ethical Issues in Human-Robot Interaction Research I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_01">
             11:30-11:45, Paper ThBT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('321'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Greeting Preferences in a Hospitality Context: A Cross-Cultural Study with a Social Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#398595" title="Click to go to the Author Index">
             Avijeet, Priyank
            </a>
            (University of Waterloo),
            <a href="ROMAN24_AuthorIndexWeb.html#302525" title="Click to go to the Author Index">
             Aliasghari, Pourya
            </a>
            (University of Waterloo),
            <a href="ROMAN24_AuthorIndexWeb.html#114390" title="Click to go to the Author Index">
             Dautenhahn, Kerstin
            </a>
            (University of Waterloo)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab321" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             HRI research has evolved to take a broader, more inclusive view of how culture influences our interaction with robots. As we delve deeper into cultural integration in HRI, it has become evident that while integrating cultural aspects offers new opportunities, it requires careful consideration due to the heightened sensitivity to the fluid nature of cultural dynamics. Our study focuses on a particular case and examines the role of context and personal preferences in a restaurant setting. We investigate how preferences for cross-cultural greetings performed by a humanoid robot can change based on the restaurant theme and describe what factors influence these preferences by looking at two different groups who participated based on different ethnic greetings. Our study reveals insight into how ethnicity, percentage of life lived in Western countries, personality variations, and implementation of cultural aspects influence the likability of robotic greeting gestures. Our investigations highlight the complexity of creating culturally adaptive robots that demonstrate the cultural norms and gestures that align with the expectations of the respective cultural groups.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_02">
             11:45-12:00, Paper ThBT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('115'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Comparative Analysis of Vision-Based Sensors for Human Monitoring in Care Robots: Exploring the Utility-Privacy Trade-Off
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#350658" title="Click to go to the Author Index">
             Baselizadeh, Adel
            </a>
            (University of Oslo (UiO)),
            <a href="ROMAN24_AuthorIndexWeb.html#247323" title="Click to go to the Author Index">
             Saplacan, Diana
            </a>
            (University of Oslo),
            <a href="ROMAN24_AuthorIndexWeb.html#134021" title="Click to go to the Author Index">
             Khaksar, Weria
            </a>
            (Norwegian University of Life Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#215814" title="Click to go to the Author Index">
             Uddin, Md. Zia
            </a>
            (University of Oslo),
            <a href="ROMAN24_AuthorIndexWeb.html#192025" title="Click to go to the Author Index">
             Torresen, Jim
            </a>
            (University of Oslo)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab115" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Striking a balance between utility and privacy holds significant importance in systems that rely on sensor utilization, such as robots. This balance is even more vital in care robots, given the sensitivity of personal data and the necessity for privacy-preserving monitoring to ensure user comfort. This paper presents a comprehensive investigation into the utility-privacy trade-off concerning different vision-based sensors. Specifically, RGB cameras, color and mono-color thermal cameras, and depth sensors are compared, considering technical aspects and users' perception of privacy. The technical analysis addresses human pose tracking, human presence detection, human vital sign monitoring, and human facial and emotion recognition. The quantitative examination of sensors in real-life scenarios highlights the mono-color thermal camera's effectiveness for user monitoring. Particularly, this sensor excels in challenging human presence detection scenarios compared to RGB cameras. Furthermore, interview and survey studies, encompassing two different age groups were carried out to compare how sensors are perceived in terms of user privacy. The quantitative and qualitative assessments of users' feedback in these studies reveal that apart from depth sensors, thermal mono-color, and thermal color sensors are perceived as better at preserving user privacy compared to RGB cameras. The analysis includes the influence of participant age on privacy perception, indicating non-significant effects. Considering both the technical assessment and user preferences, the mono-color thermal camera emerges as the optimal choice for human monitoring purposes.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_03">
             12:00-12:15, Paper ThBT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('389'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Balancing Innovation and Regulation: The Impact of Government Policies and Perceived Risk on Public Trust in AI Applications
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#189427" title="Click to go to the Author Index">
             Kim, Boyoung
            </a>
            (George Mason University Korea),
            <a href="ROMAN24_AuthorIndexWeb.html#399353" title="Click to go to the Author Index">
             Kwon, Soyoung
            </a>
            (George Mason University Korea)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab389" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#philosophical_issues_in_human_robot_coexistence" title="Click to go to the Keyword Index">
               Philosophical Issues in Human-Robot Coexistence
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We investigated the impact of a government policy on Artificial Intelligence (AI) technology on people's trust in AI applications in society. Specifically, we sought to examine how trust in AI applications is affected by the government's emphasis on either innovation or regulation, and the risks associated with AI applications. In an online experiment, participants were presented with a government policy that prioritized either the innovation of AI technology for economic growth or the regulation of AI technology for the protection of public interest. Participants then indicated their trust in various AI application cases with varying levels of perceived risk. We found that perceived risk influenced participants’ trust in the applications, and that participants strongly supported AI policies prioritizing regulation. There were also possible trends suggesting the interactive effect of policies and perceived risk on participants’ trust in AI applications and the effect of policies on the decrease of participants’ general trust in AI technology. This work represents a crucial preliminary step in contributing to the ongoing global debate on the AI regulation dilemma.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt2_04">
             12:15-12:30, Paper ThBT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('71'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              How Do People Intend to Disclose Personal Information to a Social Robot in Public Spaces?
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#393465" title="Click to go to the Author Index">
             Aryania, Azra
            </a>
            (Institut De Robotica I Informatica Industrial, CSIC-UPC),
            <a href="ROMAN24_AuthorIndexWeb.html#281807" title="Click to go to the Author Index">
             Huertas García, Rubén
            </a>
            (Universitat De Barcelona),
            <a href="ROMAN24_AuthorIndexWeb.html#281806" title="Click to go to the Author Index">
             Forgas, Santiago
            </a>
            (Universitat De Barcelona),
            <a href="ROMAN24_AuthorIndexWeb.html#101337" title="Click to go to the Author Index">
             Angulo, Cecilio
            </a>
            (Technical University of Catalonia),
            <a href="ROMAN24_AuthorIndexWeb.html#103995" title="Click to go to the Author Index">
             Alenyà, Guillem
            </a>
            (Institut De Robòtica I Informàtica Industrial CSIC-UPC)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab71" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Social robots interacting with people in public spaces may access and collect their personal information, which raises privacy concerns regarding the disclosure of personal information. This paper aims to investigate factors impacting individuals' intention to disclose personal information to a social robot in public spaces and evaluate the actual disclosure during the interaction with the robot. For this purpose, a model is proposed to predict people's intentions to disclose information to a social robot. We conducted our experiment at a public festival with more than 100 participants using the social robot ARI. The findings reveal the substantial impact of factors including risk beliefs, trusting beliefs, perceived enjoyment, and social influence on the intention to disclose personal information. Moreover, they reveal that although only a small percentage (6.20%) of people had the intention to disclose information to the social robot, most participants (98.00%) finally disclosed their personal information.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt3">
             <b>
              ThBT3
             </b>
            </a>
           </td>
           <td class="r">
            Room T3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thbt3" title="Click to go to the Program at a Glance">
             <b>
              Child-Robot Interaction II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_01">
             11:30-11:45, Paper ThBT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('214'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Song Gesture Recognition for a Robot-Enhanced Imitation Therapy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#322354" title="Click to go to the Author Index">
             Fassina, Gabriele
            </a>
            (Politecnico Di Milano),
            <a href="ROMAN24_AuthorIndexWeb.html#286690" title="Click to go to the Author Index">
             Santos, Laura
            </a>
            (Instituto Superior Tecnico, Universidade De Lisboa),
            <a href="ROMAN24_AuthorIndexWeb.html#399964" title="Click to go to the Author Index">
             Zorzella, Elisa
            </a>
            (Politecnico Di Milano),
            <a href="ROMAN24_AuthorIndexWeb.html#322362" title="Click to go to the Author Index">
             Caglio, Arianna
            </a>
            (IRCCS Fondazione Don Carlo Gnocchi),
            <a href="ROMAN24_AuthorIndexWeb.html#322365" title="Click to go to the Author Index">
             Annunziata, Silvia
            </a>
            (IRCCS Fondazione Don Carlo Gnocchi),
            <a href="ROMAN24_AuthorIndexWeb.html#399966" title="Click to go to the Author Index">
             Cavallini, Anna
            </a>
            (IRCCS Fondazione Don Carlo Gnocchi),
            <a href="ROMAN24_AuthorIndexWeb.html#153234" title="Click to go to the Author Index">
             Pedrocchi, Alessandra
            </a>
            (Politecnico Di Milano),
            <a href="ROMAN24_AuthorIndexWeb.html#156319" title="Click to go to the Author Index">
             Ambrosini, Emilia
            </a>
            (Politecnico Di Milano)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab214" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot-Enhanced Therapies (RET) offer a promising alternative for Autism Spectrum Disorder (ASD) children. Within this framework, an imitation therapy is proposed where children replicate gestures demonstrated by a robot, including those derived from popular children songs. To enable effective feedback provision by the robot, gesture recognition becomes paramount. This paper introduces two approaches for achieving this goal: a multiclass classifier, intended to recognize which gesture is executed, and a set of binary classifiers discerning whether the expected gesture is performed or not. Both models rely on kinematic data acquired through the Azure Kinect camera and a Residual Network as classification model. Moreover, considering the challenges in children's data collection for model training, the work explores the impact that the collection of their data can bring to the outcomes of a gesture classification algorithm. Beside testing on adults and healthy children datasets, the study leverages a dataset comprising 69 gestures performed by ASD children during therapy sessions. Results indicate that binary models trained also on children data outperform the multiclass approach trained solely on adults data, achieving a median accuracy of 86%. This underscores the effectiveness of binary classifiers and suggests that integrating children's data can enhance algorithm performance. To strengthen findings, future research should expand dataset size, especially considering ASD children, and explore alternative action recognition algorithms like Long Short-Term Memories (LSTM).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_02">
             11:45-12:00, Paper ThBT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('57'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring the Suitability of Conversational AI for Child-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#374386" title="Click to go to the Author Index">
             Mannava, Vivek
            </a>
            (Hochschule Bonn-Rhein-Sieg),
            <a href="ROMAN24_AuthorIndexWeb.html#195746" title="Click to go to the Author Index">
             Mitrevski, Alex
            </a>
            (Hochschule Bonn-Rhein-Sieg),
            <a href="ROMAN24_AuthorIndexWeb.html#104247" title="Click to go to the Author Index">
             Plöger, Paul G.
            </a>
            (Hochschule Bonn Rhein Sieg)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab57" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robotic_etiquette" title="Click to go to the Keyword Index">
               Robotic Etiquette
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Current approaches in education, while aiming to be universally effective, often struggle to fully adapt to the unique needs and communication styles of individual children; this disparity can limit the children’s engagement and hinder their learning progress. Similarly, parents or guardians, despite their good intentions, may also be unable to provide consistent and personalized support to each child. In this work, we investigate the use of conversational systems for socially assistive robots (SARs) as a potential solution to this problem, as such systems have the potential to allow children to interact and learn at their own pace, in a way that aligns with their communication preferences. To ensure that the robot’s language is suitable for children, we present a system that leverages a combination of natural language processing (NLP) techniques, including dialog management, child-friendly language generation, and context-aware response adaptation; to achieve this, our system combines Rasa for dialog management, GPT-3.5 for language generation, and textstat for language complexity evaluation. We evaluate the suitability of the generated language for a young audience through two user studies with adult participants, one in which the conversational system was embodied in a robot and involved direct interaction between a human and a robot, and another where participants evaluated conversational transcripts from the first study. Our results suggest that the system has the potential to maintain engaging and safe conversations, and adapt its language to individual needs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_03">
             12:00-12:15, Paper ThBT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('382'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A HeARTfelt Robot: Social Robot-Driven Deep Emotional Art Reflection with Children
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#390043" title="Click to go to the Author Index">
             Pu, Isabella
            </a>
            (Massachusetts Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#396531" title="Click to go to the Author Index">
             Nguyen, Golda
            </a>
            (Massachusetts Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#396279" title="Click to go to the Author Index">
             Alsultan, Lama
            </a>
            (Massachusetts Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#207951" title="Click to go to the Author Index">
             Picard, Rosalind W.
            </a>
            (MIT Media Lab),
            <a href="ROMAN24_AuthorIndexWeb.html#107193" title="Click to go to the Author Index">
             Breazeal, Cynthia
            </a>
            (MIT),
            <a href="ROMAN24_AuthorIndexWeb.html#281885" title="Click to go to the Author Index">
             Alghowinem, Sharifa
            </a>
            (MIT Media Lab, Prince Sultan University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab382" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Social-emotional learning (SEL) skills are essential for children to develop to provide a foundation for future relational and academic success. Using art as a medium for creation or as a topic to provoke conversation is a well-known method of SEL education. Similarly, social robots have been used to teach SEL competencies like empathy, but the combination of art and social robotics has been minimally explored. In this paper, we present a novel child-robot interaction designed to foster empathy and promote SEL competencies via a conversation about art scaffolded by a social robot. Participants (N=11, age range: 7-11) conversed with a social robot about emotional and neutral art. Analysis of video and speech data demonstrated that this interaction design successfully engaged children in the practice of SEL skills, like emotion recognition and self-awareness, and greater rates of empathetic reasoning were observed when children engaged with the robot about emotional art. This study demonstrated that art-based reflection with a social robot, particularly on emotional art, can foster empathy in children, and interactions with a social robot may help alleviate discomfort when sharing deep or vulnerable emotions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt3_04">
             12:15-12:30, Paper ThBT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('109'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Exploration into the Design of Multi-Session Robot-Mediated Joint Attention Intervention for Young Children with Autism
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#197065" title="Click to go to the Author Index">
             Nie, Guangtao
            </a>
            (Vanderbilt University),
            <a href="ROMAN24_AuthorIndexWeb.html#304783" title="Click to go to the Author Index">
             Zheng, Zhi
            </a>
            (University of Notre Dame),
            <a href="ROMAN24_AuthorIndexWeb.html#153586" title="Click to go to the Author Index">
             Swanson, Amy
            </a>
            (Vanderbilt University),
            <a href="ROMAN24_AuthorIndexWeb.html#197059" title="Click to go to the Author Index">
             Weitlauf, Amy
            </a>
            (Vanderbilt University),
            <a href="ROMAN24_AuthorIndexWeb.html#115938" title="Click to go to the Author Index">
             Warren, Zachary
            </a>
            (Vanderbilt University),
            <a href="ROMAN24_AuthorIndexWeb.html#106153" title="Click to go to the Author Index">
             Sarkar, Nilanjan
            </a>
            (Vanderbilt University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab109" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             One in 36 children in the United States has Autism Spectrum Disorder (ASD). Numerous robotic intervention systems have been proposed for children with ASD. It is widely acknowledged that engagement with the robotic system is important for intervention success. Visual attention toward the robot can be used as a proxy for engagement. However, less is known about how to maintain and enhance visual attention within a robotic system, and how the variation of visual attention may influence adaptation of dynamic intervention protocols to improve outcomes. Therefore, in this work, we propose a new metric, System Capture Ratio (SCR), that can be automatically quantified in real-time, to measure a participant’s visual attention within a robotic system designed for joint attention intervention for children with ASD. Then, we demonstrate that compared to a static intervention protocol, a dynamic intervention protocol can help sustain visual attention and thus achieve a significant performance improvement. The results support the implementation of adaptive and dynamic robotic intervention protocols for children with ASD that are based on SCR, offering suggestions on designing effective multi-session studies for autism intervention.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt4">
             <b>
              ThBT4
             </b>
            </a>
           </td>
           <td class="r">
            Room T4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thbt4" title="Click to go to the Program at a Glance">
             <b>
              Motion Planning and Navigation in Human-Centered Environments IV
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_01">
             11:30-11:45, Paper ThBT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('372'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Spatio-Temporal Multi-Scale Pedestrian Flow Model by Using Attention Module
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#299359" title="Click to go to the Author Index">
             Sakurai, Akihiro
            </a>
            (University of Tokyo),
            <a href="ROMAN24_AuthorIndexWeb.html#117307" title="Click to go to the Author Index">
             Yamamoto, Ko
            </a>
            (University of Tokyo)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab372" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_touch_in_human_robot_interaction" title="Click to go to the Keyword Index">
               Social Touch in Human–Robot Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Congestion in urban spaces sometimes leads to serious accidents. To prevent a congestion and serious accident, it is important to properly arrange-building layouts and guide pedestrians safely based on, mathematical modeling and simulation of pedestrian flows. In the real world, however, each pedestrian's behavior is so complicated and often includes spatio-temporally multiscale decision making while most of existing studies mainly focused on a crossing area in front of a station such as Shibuya scramble crossing. In a previous report, a trained model could simulate some pedestrian trajectory, but it also generated undesirable behavior that a pedestrian ignored a traffic light. In this study, we introduce an attention mechanism to solve this problem. The performance of the model is evaluated quantitatively by the prediction error and qualitatively by visualizing the time of a traffic light switching.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_02">
             11:45-12:00, Paper ThBT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('260'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safe and Efficient Operation of Emotional Support Robots: A Risk-Based Approach with User-Centric Tangent Bug for Blended Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#360942" title="Click to go to the Author Index">
             Saglam, Ahmet
            </a>
            (Old Dominion University),
            <a href="ROMAN24_AuthorIndexWeb.html#360943" title="Click to go to the Author Index">
             Papelis, Yiannis
            </a>
            (Old Dominion University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab260" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Hospitalization and isolation can be a traumatic experience for immunocompromised children, especially because they are separated from their families and friends. Social robots have been proposed as a way to improve the quality of care for children hospitalized in isolation by providing alternative means of social interaction and support. Remote control of such robots in a hospital setting, particularly where safety is a major concern, can be a daunting task for young patients. This paper introduces a risk-based blended control approach for mobile robots in hospital-like indoor spaces. Our approach implements a modified Tangent Bug algorithm called User-Centric Tangent Bug for Blended Control, U-CenTB2, with a user-driven directionality principle, wherein temporary goals are set in alignment with intended user commands rather than with a fixed global goal. Risk evaluation is conducted through a dual-speed strategy, addressing a critical gap in traditional trajectory estimation methods that may not detect incremental movements at slow speeds, potentially causing collisions in blind spots. This preemptive risk assessment prioritizes maintaining desired commands while dynamically avoiding collisions. The performance of the algorithm is evaluated by conducting batch simulations via Monte Carlo method. The results demonstrate the system's efficacy in preventing collisions and adhering to user inputs, thereby offering a significant contribution to teleoperation of assistive mobile robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_03">
             12:00-12:15, Paper ThBT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('345'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Joint Potential-Vector Fields for Obstacle-Aware Legible Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#392800" title="Click to go to the Author Index">
             Ngo, Huy Quyen
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#115102" title="Click to go to the Author Index">
             Steinfeld, Aaron
            </a>
            (Carnegie Mellon University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab345" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Traditionally, potential fields and vector fields have been extensively used for motion planning, especially in finding paths to a goal position while avoiding obstacles along the way. However, such methods have only been applied to the problem of finding the shortest path to only one goal position. In human-centered environments with multiple goals, the shortest path (e.g., most predictable) is often not the most intent-expressive path (e.g., most legible) to one of the goals. We devised a method for robot planning and navigation in human-centered environments that uses potential fields to plan intent-expressive motion to a specific goal among many, while utilizing adaptive vector fields to avoid obstacles without sacrificing the legibility property of the motion. We found that our method can produce motions that are of comparable legibility and shorter path length compared to a legible motion planner baseline, as well as more legible paths compared to a traditional potential field method. Our method was evaluated in several scenarios where legibility is useful, namely maps with and without obstacles and goal switching.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt4_04">
             12:15-12:30, Paper ThBT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('134'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Identifying and Detecting Inadvertent Socially Inappropriate Movement of Avatar Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#372881" title="Click to go to the Author Index">
             Abbas, Zulkafil
            </a>
            (Kyoto University),
            <a href="ROMAN24_AuthorIndexWeb.html#103798" title="Click to go to the Author Index">
             Brscic, Drazen
            </a>
            (Kyoto University),
            <a href="ROMAN24_AuthorIndexWeb.html#101689" title="Click to go to the Author Index">
             Kanda, Takayuki
            </a>
            (Kyoto University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab134" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robotic_etiquette" title="Click to go to the Keyword Index">
               Robotic Etiquette
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motion_planning_and_navigation_in_human_centered_environments" title="Click to go to the Keyword Index">
               Motion Planning and Navigation in Human-Centered Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Avatar robots are telepresence robots that allow people to project themselves physically in distant places. Teleoperators use such avatars to have remote social interactions. Mobility adds to their interactive capabilities. However, it also presents challenges in social spaces; for instance, teleoperators can mistakenly move their avatars in a socially inappropriate manner. Since these robots have yet to become ubiquitous, there is little real-world data to objectively analyze the presence and frequency of such undesirable movement. Consequently, the different ways such movements manifest are unknown, and their occurrence cannot be detected. To address this problem, we conducted a laboratory experiment with hired participants in a replica artwork exhibition so that we could gather realistic human-avatar interaction data using sensors. Our data analysis helped us identify three types of inadvertently inappropriate movement behavior. We present a definition of such behavior based on these three inappropriate movement types and a labeled dataset containing more than 400 interactions, 18% of which were inappropriate. We also present detection models for each inappropriate movement type trained using our dataset.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt5">
             <b>
              ThBT5
             </b>
            </a>
           </td>
           <td class="r">
            Room T5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thbt5" title="Click to go to the Program at a Glance">
             <b>
              Multimodal Interaction and Conversational Skills II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_01">
             11:30-11:45, Paper ThBT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('280'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              I2EDL: Interactive Instruction Error Detection and Localization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#380976" title="Click to go to the Author Index">
             Taioli, Francesco
            </a>
            (University of Verona),
            <a href="ROMAN24_AuthorIndexWeb.html#123015" title="Click to go to the Author Index">
             Rosa, Stefano
            </a>
            (Istituto Italiano Di Tecnologia),
            <a href="ROMAN24_AuthorIndexWeb.html#225224" title="Click to go to the Author Index">
             Castellini, Alberto
            </a>
            (Verona University),
            <a href="ROMAN24_AuthorIndexWeb.html#111056" title="Click to go to the Author Index">
             Natale, Lorenzo
            </a>
            (Istituto Italiano Di Tecnologia),
            <a href="ROMAN24_AuthorIndexWeb.html#133775" title="Click to go to the Author Index">
             Del Bue, Alessio
            </a>
            (Istituto Italiano Di Tecnologia),
            <a href="ROMAN24_AuthorIndexWeb.html#106097" title="Click to go to the Author Index">
             Farinelli, Alessandro
            </a>
            (University of Verona),
            <a href="ROMAN24_AuthorIndexWeb.html#166506" title="Click to go to the Author Index">
             Cristani, Marco
            </a>
            (University of Verona),
            <a href="ROMAN24_AuthorIndexWeb.html#192532" title="Click to go to the Author Index">
             Wang, Yiming
            </a>
            (Fondazione Bruno Kessler)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab280" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the Vision-and-Language Navigation in Continuous Environments (VLN-CE) task, the human user guides an autonomous agent to reach a target goal via a series of low-level actions following a textual instruction in natural language. However, most existing methods do not address the likely case where users may make mistakes when providing such instruction (e.g., "turn left" instead of "turn right"). In this work, we address a novel task of Interactive VLN in Continuous Environments (IVLN-CE), which allows the agent to interact with the user during the VLN-CE navigation to verify any doubts regarding the instruction errors. We propose an Interactive Instruction Error Detector and Localizer (I2EDL) that triggers the user-agent interaction upon the detection of instruction errors during the navigation. We leverage a pre-trained module to detect instruction errors and pinpoint them in the instruction by cross-referencing the textual input and past observations. In such a way, the agent is able to query the user for a timely correction, without demanding the user’s cognitive load, as we locate the probable errors to a precise part of the instruction. We evaluate the proposed I2EDL on a dataset of instructions containing errors, and further devise a novel metric, the Success weighted by Interaction Number (SIN), to reflect both the navigation performance and the interaction effectiveness. We show how the proposed method can ask focused requests for corrections to the user, which in turn increases the navigation success, while minimizing the interactions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_02">
             11:45-12:00, Paper ThBT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('281'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              People, Cracks, Stairs, and Doors: Vision-Based Semantic Mapping with a Quadruped Robot Supporting First Responders in Search &amp; Rescue
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#360905" title="Click to go to the Author Index">
             Betta, Zoe
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#155018" title="Click to go to the Author Index">
             Recchiuto, Carmine Tommaso
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#108312" title="Click to go to the Author Index">
             Sgorbissa, Antonio
            </a>
            (University of Genova)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab281" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study introduces a system implemented on a legged robot, designed to generate a multi-layered map that incorporates semantic information, specifically tailored for Search &amp; Rescue robotics. The article discusses the development of a Machine Learning model based on visual data for recognizing people and environmental features, and its integration into a mapping and navigation architecture. The system was extensively tested in two different locations using the Spot robot by Boston Dynamics, equipped with an external ZED2 depth camera, and these tests are described in detail and results analyzed.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_03">
             12:00-12:15, Paper ThBT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('247'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Users' Predictions of Robotic Pouring Behaviors Using Augmented Reality: A Case Study
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#281701" title="Click to go to the Author Index">
             Cleaver, Andre
            </a>
            (Tufts University),
            <a href="ROMAN24_AuthorIndexWeb.html#202986" title="Click to go to the Author Index">
             Aronson, Reuben
            </a>
            (Tufts University),
            <a href="ROMAN24_AuthorIndexWeb.html#123554" title="Click to go to the Author Index">
             Sinapov, Jivko
            </a>
            (Tufts University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab247" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             People effortlessly manipulate fluids due to their learned understanding of fluid dynamics, while robots struggle with complex fluid dynamic calculations, particularly in tasks like pouring. To enhance assistive robots in such tasks, we propose involving users in correcting and providing feedback by visualizing the planned pouring trajectories before they are executed. This paper investigates whether people can predict robotic pouring outcomes and make adjustments to minimize spills, using visualization devices like augmented reality. In a human-participant study, participants evaluated and adjusted robot pouring behaviors of unique configurations for various source containers. Results highlight the effectiveness of visualization tools such as augmented reality headsets, as well as traditional 2D display, especially with specific pouring parameters, and users noted their benefits in open-ended responses. This research illuminates the potential for human-robot collaboration in fluid manipulation tasks, with visualization tools reducing spills in robot-controlled pours.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt5_04">
             12:15-12:30, Paper ThBT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('203'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MaRMOT: A Modular and Reconfigurable Multiple Object Tracking Framework for Robots and Intelligent Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#306892" title="Click to go to the Author Index">
             Duncan, John Alexander
            </a>
            (The University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#190490" title="Click to go to the Author Index">
             Alambeigi, Farshid
            </a>
            (University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#114523" title="Click to go to the Author Index">
             Pryor, Mitchell
            </a>
            (University of Texas)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab203" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Multiple object tracking (MOT) is a valuable perception function for robots and intelligent systems. Despite rapid improvements in metrics such as Average Multiple Object Tracking Accuracy (AMOTA) on MOT benchmarks, many trackers are application-specific or run at speeds &lt;1 frames per second (FPS) making them impractical for dynamic tracking applications such as human-robot interaction. To this end, we introduce Modular and Reconfigurable Multiple Object Tracking (MaRMOT), a general-purpose tracking framework for robots implemented in ROS2. Using the nuScenes MOT development kit, we provide accuracy and speed metrics (AMOTA, average FPS, and worst-case FPS) for various tracking methods. We achieve AMOTA of 50.5% with an average of 62.0 FPS on the nuScenes test split, making MaRMOT suitable for online tracking in complex scenes. We demonstrate MaRMOT’s modularity on two human tracking applications with different hardware configurations: a 2x object detection camera “smart space” configuration, and a mobile robot configuration with 3D LiDAR detector and an object detection camera. We show that an alternate sensor configuration using human position measurements from a headset improves multiple object tracking accuracy (MOTA) over vision and LiDAR detection alone. Finally, we provide software and hardware design recommendations for tracking applications with tracker speed requirements &gt;10 FPS. MaRMOT is open source and can be extended with new detectors, process models, matching algorithms, and track management techniques.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thbt6">
             <b>
              ThBT6
             </b>
            </a>
           </td>
           <td class="r">
            Room T6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thbt6" title="Click to go to the Program at a Glance">
             <b>
              Novel Interfaces and Interaction Modalities II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_01">
             11:30-11:45, Paper ThBT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('351'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Gaze-Based Augmented Reality Interfaces to Support Scalable Human-Robot Teaming
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#321182" title="Click to go to the Author Index">
             Petlowany, Christina
            </a>
            (The University of Texas at Austin),
            <a href="ROMAN24_AuthorIndexWeb.html#114523" title="Click to go to the Author Index">
             Pryor, Mitchell
            </a>
            (University of Texas),
            <a href="ROMAN24_AuthorIndexWeb.html#400292" title="Click to go to the Author Index">
             Hahn, Nathan
            </a>
            (U.S. Army DEVCOM Army Research Laboratory)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab351" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multi_modal_situation_awareness_and_spatial_cognition" title="Click to go to the Keyword Index">
               Multi-modal Situation Awareness and Spatial Cognition
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As end users interact with increasing numbers of autonomous or semi-autonomous systems, collaboration and supervision become more complicated. People must simultaneously manage multiple systems, and most current interfaces do not scale. Augmented Reality (AR) offers a promising solution by placing information over the real world, allowing users to concurrently track the scene and robot(s) -- potentially improving scaling for these devices. In this work, we leverage user gaze - a powerful indicator of attention suitable for reactive systems - to lower cognitive burden and improve performance such that scaling to multiple agents is possible. Gaze is probed in two modalities. In active mode, the user looks at a menu and presses a button to request additional information. The passive mode provides more information when the user’s gaze dwells on the menu. We performed two studies: 1) participants complete a visual search task with increasing numbers of virtual robotic agents and 2) participants must track the dynamic status of a team with physical agents. Results from the first study show that the passive and active interfaces provide better scaling compared to a non-interactive interface as the number of robots increases. In both studies, users preferred the passive mode, citing a lower mental demand, effort, and frustration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_02">
             11:45-12:00, Paper ThBT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('49'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Voice-Controlled Human-Machine Interface for an Assistive Exoskeleton Glove Aiding Patients with Brachial Plexus Injuries
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#310974" title="Click to go to the Author Index">
             Guo, Yunfei
            </a>
            (Virginia Tech),
            <a href="ROMAN24_AuthorIndexWeb.html#310874" title="Click to go to the Author Index">
             Xu, Wenda
            </a>
            (Virginia Tech),
            <a href="ROMAN24_AuthorIndexWeb.html#310973" title="Click to go to the Author Index">
             Bravo, Cesar
            </a>
            (Carilion Clinic Institute of Orthopaedics and Neurosciences),
            <a href="ROMAN24_AuthorIndexWeb.html#109732" title="Click to go to the Author Index">
             Ben-Tzvi, Pinhas
            </a>
            (Virginia Tech)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab49" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a voice-controlled Human Machine Interface (HMI) tailored for an assistive robotic exoskeleton glove, aimed at assisting patients coping with Brachial Plexus Injuries (BPI) in regaining their lost grasping functionality. The development of this HMI draws upon clinical experimentation results, forming a foundation for its design. The paper delves into the challenges encountered while employing a prior voice-based HMI, which necessitated an internet connection for complex computations and exhibited limitations in effectively processing concise commands. To address these issues, an innovative voice-controlled HMI system is proposed, featuring fixed-word detection to replace the speech-to-text (STT) converter and the Neutral Language Processor (NLP) to reduce computational overhead. Furthermore, the new HMI replaces the previous text-independent speaker verification with a text-dependent, one-shot learning approach. This enhancement streamlines custom retraining, significantly improving speaker verification accuracy for concise commands. Experimental results substantiate the applicability of the proposed voice-controlled HMI for assisting individuals with BPI through specialized exoskeleton gloves.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_03">
             12:00-12:15, Paper ThBT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('131'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hardships in the Land of Oz: Robot Control Challenges Faced by HRI Researchers and Real-World Teleoperators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#302645" title="Click to go to the Author Index">
             Bejarano, Alexandra
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#223982" title="Click to go to the Author Index">
             Elbeleidy, Saad
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#287655" title="Click to go to the Author Index">
             Mott, Terran
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#399629" title="Click to go to the Author Index">
             Negrete-Alamillo, Sebastian
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#399634" title="Click to go to the Author Index">
             Armenta, Luis Angel
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#174132" title="Click to go to the Author Index">
             Williams, Tom
            </a>
            (Colorado School of Mines)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab131" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Wizard-of-Oz (WoZ) is one of the most widely used experimental methodologies across the field of Human-Robot Interaction (HRI), making WoZ teleoperation interfaces a critical tool for HRI research. Yet current WoZ teleoperation interfaces are overwhelmingly tailored towards a narrow set of HRI interaction paradigms. In this work, we conducted a set of interviews with HRI researchers to better understand the diversity of teleoperation needs across the HRI community. Our analysis highlighted (1) human challenges, with respect to wizards' expertise, the need for quick responses, and research participants' unpredictability; (2) robot challenges, with respect to robot malfunctions, delays, and robot-driven complexity, and (3) interaction challenges, with respect to researchers' varying control requirements and the need for precise experimental control. Moreover, our results revealed unexpected parallels between the experiences of HRI researchers and real-world teleoperators, which open up fundamentally new possibilities for future work in robot control interfaces and encourage radically different perspectives on what types of interfaces are even needed to best facilitate WoZ experimentation. Leveraging these insights, we recommend that WoZ interfaces (1) be designed with extensibility and customization in mind, (2) ease interaction management by accounting for unpredictability and multi-robot interactions, and (3) consider WoZ teleoperators beyond the context of experimentation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thbt6_04">
             12:15-12:30, Paper ThBT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('368'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robust Gesture-Based Appliance Control Via Operator Identification and Tracking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#359809" title="Click to go to the Author Index">
             Yokota, Masae
            </a>
            (Chuo University),
            <a href="ROMAN24_AuthorIndexWeb.html#191220" title="Click to go to the Author Index">
             Pathak, Sarthak
            </a>
            (Chuo University),
            <a href="ROMAN24_AuthorIndexWeb.html#102374" title="Click to go to the Author Index">
             Umeda, Kazunori
            </a>
            (Chuo University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab368" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we improve the robustness of a multi-camera gesture recognition system in multi-person situations by identifying and tracking the operator. This system is meant as an intelligent room to operate and interact with surrounding devices based on pointing gestures. In the method, we identify the operator by a hand-raising gesture, followed by tracking using the coordinates of the center of the operator's head and extracting only the operator's whole body. From the experimental results, we confirmed that highly accurate tracking could be performed in a multi-person situation of 2 to 5 persons, and that the success rate of extracting images of the operator's whole body was more than 70%. We also clarified issues in the operator identification process and the extraction process.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thlu_br">
             <b>
              ThLU_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thlu_br" title="Click to go to the Program at a Glance">
             <b>
              Boxed Lunch - T
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thkn">
             <b>
              ThKN
             </b>
            </a>
           </td>
           <td class="r">
            Room T9 - Ballroom
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thkn" title="Click to go to the Program at a Glance">
             <b>
              Keynote Talk - Leila Takayama
             </b>
            </a>
           </td>
           <td class="r">
            Keynote Session
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpm1_br">
             <b>
              ThPM1_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thpm1_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break - TIII
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct1">
             <b>
              ThCT1
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thct1" title="Click to go to the Program at a Glance">
             <b>
              Degrees of Autonomy and Teleoperation I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_01">
             14:40-16:10, Paper ThCT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('454'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task Management System for Construction Machinery Using the Open Platform OPERA
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#396383" title="Click to go to the Author Index">
             Kasahara, Yuichiro
            </a>
            (Kyushu University),
            <a href="ROMAN24_AuthorIndexWeb.html#335944" title="Click to go to the Author Index">
             Itsuka, Tomoya
            </a>
            (Kyushu University),
            <a href="ROMAN24_AuthorIndexWeb.html#368037" title="Click to go to the Author Index">
             Shibata, Koshi
            </a>
            (Kyushu University),
            <a href="ROMAN24_AuthorIndexWeb.html#396264" title="Click to go to the Author Index">
             Kouno, Tomoya
            </a>
            (Kyushu University),
            <a href="ROMAN24_AuthorIndexWeb.html#396257" title="Click to go to the Author Index">
             Maeda, Ryuichi
            </a>
            (Kyushu University),
            <a href="ROMAN24_AuthorIndexWeb.html#217773" title="Click to go to the Author Index">
             Matsumoto, Kohei
            </a>
            (Kyushu University),
            <a href="ROMAN24_AuthorIndexWeb.html#356674" title="Click to go to the Author Index">
             Kimura, Shunsuke
            </a>
            (Shimizu Corporation),
            <a href="ROMAN24_AuthorIndexWeb.html#399861" title="Click to go to the Author Index">
             FUKASE, YUTARO
            </a>
            (Shimizu Corporation),
            <a href="ROMAN24_AuthorIndexWeb.html#399921" title="Click to go to the Author Index">
             Yokoshima, Takashi
            </a>
            (Shimizu Corporation),
            <a href="ROMAN24_AuthorIndexWeb.html#155898" title="Click to go to the Author Index">
             Yamauchi, Genki
            </a>
            (Public Works Research Institute),
            <a href="ROMAN24_AuthorIndexWeb.html#400442" title="Click to go to the Author Index">
             Endo, Daisuke
            </a>
            (Public Works Research Institute),
            <a href="ROMAN24_AuthorIndexWeb.html#229648" title="Click to go to the Author Index">
             Hashimoto, Takeshi
            </a>
            (Public Works Research Institute),
            <a href="ROMAN24_AuthorIndexWeb.html#103231" title="Click to go to the Author Index">
             Kurazume, Ryo
            </a>
            (Kyushu University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab454" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years, labor accidents and a shortage of skilled workers due to an aging population have become significant issues at construction sites in Japan. To address these challenges, we are developing a Cyber-Physical System (CPS) platform called ROS2-TMS for Construction, which aims to improve both the efficiency and safety of earthwork operations. In this study, we propose a task management system for construction machinery using an open platform named OPERA as an additional function of ROS2-TMS for Construction. This task management system controls construction machinery using environmental information stored in a database, which collects and stores data from sensors deployed throughout the construction site, and an extended Behavior Tree. At the end of this study, the results of the initial validation tests of autonomous earthwork operations using an OPERA-compatible backhoe ZX200 are presented.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_02">
             14:40-16:10, Paper ThCT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('166'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sensorimotor Coordinated Multi-UAV Coverage Path Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#351683" title="Click to go to the Author Index">
             Liu, Siyuan
            </a>
            (Peking University),
            <a href="ROMAN24_AuthorIndexWeb.html#281161" title="Click to go to the Author Index">
             Fang, Hongyu
            </a>
            (Peking University, Beijing, China),
            <a href="ROMAN24_AuthorIndexWeb.html#351734" title="Click to go to the Author Index">
             Deng, Ziyang
            </a>
            (Peking University),
            <a href="ROMAN24_AuthorIndexWeb.html#152071" title="Click to go to the Author Index">
             Luo, Dingsheng
            </a>
            (Peking University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab166" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_and_sensorimotor_development" title="Click to go to the Keyword Index">
               Cognitive and Sensorimotor Development
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#curiosity__intentionality_and_initiative_in_interaction" title="Click to go to the Keyword Index">
               Curiosity, Intentionality and Initiative in Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a novel on-line multi-UAV coverage path planning method for 3D reconstruction. UAVs play ho- mogeneous roles in most existing multi-UAV path planning methods, where their flexibility is not fully exploited. The fact that UAVs can explore with different missions and strategies at different heights simultaneously is not well considered. Therefore, we give UAVs different identities by presenting a global-local pattern, where global UAV locates the interesting regions while local UAV explores these regions in succession. Leveraging the mechanism of sensorimotor coordination, our strategy adjusts the paths of local UAV on-line according to the input from global UAV in order to gather more information and save time. In addition, our method is compatible with real-time SLAM systems. Simulation shows that our strategy takes less time than other strategies based on area-division, and indicates that it gathers more information for better reconstruction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_03">
             14:40-16:10, Paper ThCT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('460'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Light-Weight and Rapid Table Tennis Ball Trajectory Prediction Approaches towards Online Bouncing Task
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400866" title="Click to go to the Author Index">
             Xu, Peisen
            </a>
            (Zhejiang University),
            <a href="ROMAN24_AuthorIndexWeb.html#190887" title="Click to go to the Author Index">
             Li, Gaofeng
            </a>
            (Zhejiang University),
            <a href="ROMAN24_AuthorIndexWeb.html#195776" title="Click to go to the Author Index">
             Ye, Qi
            </a>
            (Zhejiang University),
            <a href="ROMAN24_AuthorIndexWeb.html#163844" title="Click to go to the Author Index">
             Chen, Jiming
            </a>
            (Zhejiang University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab460" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_and_sensorimotor_development" title="Click to go to the Keyword Index">
               Cognitive and Sensorimotor Development
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             It is essentially required to predict the ball's flight trajectory accurately and timely for a robotic table tennis ball bouncing task. Existing solutions, which can be categorized into model-based and learning-based groups, both exhibits unpleasant disadvantages. For example, they often require to identify many dynamic parameters accurately or to collect extensive labeled data, which are generally very difficult or costly to achieve in real world. In this paper, we proposed a light-wight and rapid trajectory prediction approach for online table tennis bouncing tasks based on a simplified model. In the proposed approach, the ball's flight poses are captured and estimated by a low-cost RGB-D camera. Then the ball's landing position is predicted in advance by using a fitted 3D parabola. Compared with existing solutions, our proposed approach is lightweight and easy to deploy. In experiments, 66 flight trajectories of the ball are collected to serve as benchmark. The prediction errors for all landing positions are all less than 20mm, in which most of them are less than 10mm. In addition, the prediction can be achieved 141.7ms in advance, which is fast enough for the robotic arm to plan and move itself to the predicted landing point.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_04">
             14:40-16:10, Paper ThCT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('462'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Influence of Autonomy of a Teleoperated Robot on User's Objective and Subjective Performance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400223" title="Click to go to the Author Index">
             Wozniak, Mateusz
            </a>
            (Italian Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400227" title="Click to go to the Author Index">
             Ari, Ilkay
            </a>
            (Italian Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#230185" title="Click to go to the Author Index">
             De Tommaso, Davide
            </a>
            (Istituto Italiano Di Tecnologia),
            <a href="ROMAN24_AuthorIndexWeb.html#176869" title="Click to go to the Author Index">
             Wykowska, Agnieszka
            </a>
            (Istituto Italiano Di Tecnologia)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab462" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This article describes a study investigating the effects of decision-making autonomy of a robot, which is teleoperated in a simulated unstable or dangerous environment. It specifically focuses on robot’s autonomy to disregard the user’s command if the robot finds an alternative method of achieving the same goal as pursued by the user, but with significantly reduced risk of failure. Such autonomous control module might prove especially useful under circumstances where human operators cannot access or process all available information quickly enough to make the most optimal decision. We conducted an experiment in which subjects participated in a task of teleoperating either a robot that possesses such autonomous cognitive module, or not. We found that such module significantly reduced their sense of agency over the robot as well as the sense of doing the task together with the robot (sense of joint agency). Most interestingly, it reduced their subjective ratings of performance in the task, when in fact their actual performance was better with such an autonomous robot. A further analysis revealed that this counterintuitive finding was due to an effect of bias: in our study loss of control associated with operating an autonomous robot on average lowered the reported subjective performance by 11% points, with other factors staying equal. These results suggest that this type of assistive autonomy can be beneficial for performance, but might lead to unwanted effects that need to be overcome in order for such system to prove useful in practical applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_05">
             14:40-16:10, Paper ThCT1.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('250'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Evaluation of Affordance Templates for Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#301251" title="Click to go to the Author Index">
             Frering, Laurent
            </a>
            (Graz University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400095" title="Click to go to the Author Index">
             Mohr-Ziak, Peter
            </a>
            (Graz University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#399700" title="Click to go to the Author Index">
             Könczöl, Clemens
            </a>
            (University of Graz),
            <a href="ROMAN24_AuthorIndexWeb.html#399701" title="Click to go to the Author Index">
             Mosbacher, Jochen
            </a>
            (University of Graz),
            <a href="ROMAN24_AuthorIndexWeb.html#323647" title="Click to go to the Author Index">
             Eder, Matthias
            </a>
            (Graz University of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400097" title="Click to go to the Author Index">
             Albert, Dietrich
            </a>
            (University of Graz),
            <a href="ROMAN24_AuthorIndexWeb.html#323649" title="Click to go to the Author Index">
             Kubicek, Bettina
            </a>
            (University of Graz),
            <a href="ROMAN24_AuthorIndexWeb.html#109833" title="Click to go to the Author Index">
             Steinbauer-Wagner, Gerald
            </a>
            (Graz University of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab250" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             There is an interest in and need for the use of semiautonomous robots in various fields, such as disaster response. Over the years, different techniques for semiautonomous control were developed, ranging from teleoperation guidance to different user interface designs for setting task constraints and goals interactively. Among those, affordance templates emerged as a recent method for users to efficiently provide robots with contextual information about object shapes, properties, and affordances. In many fields where direct teleoperation is common, affordance templates seem to be a promising candidate for improving performance and usability. However, despite the reports on the potential benefits of this technique in comparison to direct teleoperation, they are often qualitative or focus on tasks where teleoperation is particularly challenging. This can be a problem because task difficulty can influence different performance metrics and human factors, so results from studies that show large differences in task difficulty between interaction modes cannot be directly generalized to tasks with smaller or non-existent differences in difficulty. In this study, we aim to evaluate the effectiveness of affordance templates in the context of debris removal for disaster response, a task where direct teleoperation is a viable technique. We compared the two methods in a simulated setting through a user study involving 41 participants by measuring a) usability through questionnaires and b) performance on secondary tasks, an established measure of spare information processing capacity. The study results show that despite similar difficulty, users performed better on secondary tasks when using affordance template-based semiautonomy in this setting.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct1_06">
             14:40-16:10, Paper ThCT1.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('268'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Low-Cost Teleoperation with Haptic Feedback through Vision-Based Tactile Sensors for Rigid and Soft Object Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#219770" title="Click to go to the Author Index">
             Lippi, Martina
            </a>
            (University of Roma Tre),
            <a href="ROMAN24_AuthorIndexWeb.html#239713" title="Click to go to the Author Index">
             Welle, Michael C.
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#333704" title="Click to go to the Author Index">
             Wozniak, Maciej Kazimierz
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#104096" title="Click to go to the Author Index">
             Gasparri, Andrea
            </a>
            (Università Degli Studi Roma Tre),
            <a href="ROMAN24_AuthorIndexWeb.html#101716" title="Click to go to the Author Index">
             Kragic, Danica
            </a>
            (KTH)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab268" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Haptic feedback is essential for humans to successfully perform complex and delicate manipulation tasks. A recent rise in tactile sensors has enabled robots to leverage the sense of touch and expand their capability drastically. However, many tasks still need human intervention/guidance. For this reason, we present a teleoperation framework designed to provide haptic feedback to human operators based on the data from camera-based tactile sensors mounted on the robot gripper. Partial autonomy is introduced to prevent slippage of grasped objects during task execution. Notably, we rely exclusively on low-cost off-the-shelf hardware to realize an affordable solution. We demonstrate the versatility of the framework on nine different objects ranging from rigid to soft and fragile ones, using three different operators on real hardware.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct2">
             <b>
              ThCT2
             </b>
            </a>
           </td>
           <td class="r">
            Room T2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thct2" title="Click to go to the Program at a Glance">
             <b>
              HRI and Collaboration in Manufacturing Environments II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_01">
             14:40-16:10, Paper ThCT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('283'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Identifying Worker Motion through a Manufacturing Plant: A Finite Automaton Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400093" title="Click to go to the Author Index">
             Yang, Shaoze
            </a>
            (University of Michigan),
            <a href="ROMAN24_AuthorIndexWeb.html#319943" title="Click to go to the Author Index">
             Bhat, Shreyas
            </a>
            (University of Michigan),
            <a href="ROMAN24_AuthorIndexWeb.html#400088" title="Click to go to the Author Index">
             Yutong, Ren
            </a>
            (University of Michigan),
            <a href="ROMAN24_AuthorIndexWeb.html#400171" title="Click to go to the Author Index">
             Salour, Al
            </a>
            (The Boeing Company),
            <a href="ROMAN24_AuthorIndexWeb.html#400159" title="Click to go to the Author Index">
             Stroup, Terra
            </a>
            (The Boeing Company),
            <a href="ROMAN24_AuthorIndexWeb.html#134012" title="Click to go to the Author Index">
             Pridham, Paul
            </a>
            (The University of Michigan),
            <a href="ROMAN24_AuthorIndexWeb.html#254071" title="Click to go to the Author Index">
             Yang, X. Jessie
            </a>
            (University of Michigan)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab283" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous Guided Vehicles (AGVs) are becoming increasingly common in industrial environments to transport heavy equipment around warehouses. Within the idea of Industry 5.0, these AGVs are expected to work alongside humans in the same shared workspace. To enable smooth and trustworthy interaction between workers and AGVs, the AGVs must be able to model the workers' behavior and plan their trajectories around it. In this paper, we introduce a Finite Automaton Model (FAM) to model worker walking behavior in such a context. We conduct a human subject experiment using a Virtual Reality (VR) environment to collect data about worker trajectories to tune our model. We show that not only is our model more interpretable, but also outperforms machine learning models at classifying worker behavior with limited training data. Future research can use our model to modify AGV behavior to promote trustworthy human-AGV interaction.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_02">
             14:40-16:10, Paper ThCT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('101'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Analyzing Perceptions on Barriers to Safety in the Workforce and Expectations on Human and Robotic Assistance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#394682" title="Click to go to the Author Index">
             Liu, Yu
            </a>
            (Clarkson University),
            <a href="ROMAN24_AuthorIndexWeb.html#268974" title="Click to go to the Author Index">
             Banerjee, Sean
            </a>
            (Wright State Univeristy),
            <a href="ROMAN24_AuthorIndexWeb.html#191250" title="Click to go to the Author Index">
             Banerjee, Natasha Kholgade
            </a>
            (Wright State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab101" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Prior research on the fear of robots in blue-collar workplaces shows that perception is influenced by minority status, education level, job domains, age, and location. However, the perception of robots is not universally negative, as workers exhibit positive views when robots can improve safety, enhance existing skills, or take over less desirable tasks. Currently, a gap exists on the types of barriers to safety that a worker faces or how the job context, e.g. hours spent in heavy lifts, or demographics, e.g. weight or height, contribute to perceptions of safety, need for and type of assistance from a co-worker, and perception of an assistive robot. Through a survey of 530 blue-collar workers, we find that 85% of the barriers to safety are physical, psychological, and a combination of physical and psychological factors. Our survey shows that the time spent in performing heavy lifts is a significant predictor for barriers to safety, while the interaction between weight and time is a marginal predictor. We find that the barriers to safety significantly influence perception of job longevity. We find the worker's height is a significant predictor to comfort levels in receiving assistance from co-workers, while weight and the interaction between height and weight are marginally significant. We find that the worker's weight and hours spent in lifting are significant predictors for the type of assistance expected. Our findings show that the number of hours spent in lifting is a significant predictor on the type of robotic assistance expected. Based on our results, collaborative robots should be designed to show awareness of physical and physiological needs and cognizance to the worker's physical traits, as well as the amount of time the worker has spent on lifting tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_03">
             14:40-16:10, Paper ThCT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('74'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Data-Driven Approach Utilizing Body Motion Data for Trust Evaluation in Industrial Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#310229" title="Click to go to the Author Index">
             Campagna, Giulio
            </a>
            (Aalborg University),
            <a href="ROMAN24_AuthorIndexWeb.html#372431" title="Click to go to the Author Index">
             Dadgostar, Mahed
            </a>
            (Aalborg University),
            <a href="ROMAN24_AuthorIndexWeb.html#108136" title="Click to go to the Author Index">
             Chrysostomou, Dimitrios
            </a>
            (Aalborg University),
            <a href="ROMAN24_AuthorIndexWeb.html#158245" title="Click to go to the Author Index">
             Rehm, Matthias
            </a>
            (Aalborg University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab74" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Industry 5.0 signifies a transformative era where humans and robots collaborate closely, leading to advancements in manufacturing efficiency and personalization. In light of this, it becomes essential to assess the robot’s trustworthiness to ensure a secure environment and equitable workload distribution. The majority of trust assessments hinge on post-hoc questionnaires for the extent of trust experienced during the interaction. A data-driven approach is required to promptly assess trust levels in real-time, allowing for the adjustment of robot behavior to align with human needs. The paper proposes a chemical industry scenario where a robot assisted a human in the process of mixing chemicals. Several machine learning models, including deep learning, were developed using body motion data to categorize the level of trust exhibited by the human operator. The models achieve an accuracy exceeding 90%. The results clearly show the feasibility of data-driven trust assessment.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_04">
             14:40-16:10, Paper ThCT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('394'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Extended Reality for Enhanced Human-Robot Collaboration: A Human-In-The-Loop Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400384" title="Click to go to the Author Index">
             Karpichev, Yehor
            </a>
            (University of Victoria),
            <a href="ROMAN24_AuthorIndexWeb.html#322614" title="Click to go to the Author Index">
             Charter, Todd
            </a>
            (University of Victoria),
            <a href="ROMAN24_AuthorIndexWeb.html#339484" title="Click to go to the Author Index">
             Hong, Jayden
            </a>
            (Uvic ACIS Lab),
            <a href="ROMAN24_AuthorIndexWeb.html#311700" title="Click to go to the Author Index">
             Soufi Enayati, Amir Mehdi
            </a>
            (University of Victoria),
            <a href="ROMAN24_AuthorIndexWeb.html#353686" title="Click to go to the Author Index">
             Honari, Homayoun
            </a>
            (University of Victoria),
            <a href="ROMAN24_AuthorIndexWeb.html#339132" title="Click to go to the Author Index">
             Ghafarian Tamizi, Mehran
            </a>
            (University of Victoria),
            <a href="ROMAN24_AuthorIndexWeb.html#103596" title="Click to go to the Author Index">
             Najjaran, Homayoun
            </a>
            (University of Victoria)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab394" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The rise of automation has provided an opportunity to achieve higher efficiency in manufacturing processes, yet it often compromises the flexibility required to promptly respond to evolving market needs and meet the demand for customization. Human-robot collaboration attempts to tackle these challenges by combining the strength and precision of machines with human ingenuity and perceptual understanding. In this paper, we conceptualize and propose an implementation framework for an autonomous, machine learning-based manipulator that incorporates human-in-the-loop principles and leverages Extended Reality (XR) to facilitate intuitive communication and programming between humans and robots. Furthermore, the conceptual framework foresees human involvement directly in the robot learning process, resulting in higher adaptability and task generalization. The paper highlights key technologies enabling the proposed framework, emphasizing the importance of developing the digital ecosystem as a whole. Additionally, we review the existent implementation approaches of XR in human-robot collaboration, showcasing diverse perspectives and methodologies. The challenges and future outlooks are discussed, delving into the major obstacles and potential research avenues of XR for more natural human-robot interaction and integration in the industrial landscape.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_05">
             14:40-16:10, Paper ThCT2.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             SRL-VIC: A Variable Stiffness-Based Safe Reinforcement Learning for Contact-Rich Robotic Tasks
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#367275" title="Click to go to the Author Index">
             Zhang, Heng
            </a>
            (Italian Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#202395" title="Click to go to the Author Index">
             Solak, Gokhan
            </a>
            (Italian Institute of Technology, Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#205776" title="Click to go to the Author Index">
             Giardini Lahr, Gustavo Jose
            </a>
            (Hospital Israelita Albert Einstein),
            <a href="ROMAN24_AuthorIndexWeb.html#150752" title="Click to go to the Author Index">
             Ajoudani, Arash
            </a>
            (Istituto Italiano di Tecnologia)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct2_06">
             14:40-16:10, Paper ThCT2.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('493'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Selecting Source Tasks for Transfer Learning of Human Preferences
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#224839" title="Click to go to the Author Index">
             Nemlekar, Heramb
            </a>
            (Virginia Tech),
            <a href="ROMAN24_AuthorIndexWeb.html#384258" title="Click to go to the Author Index">
             Sivagnanadasan, Naren
            </a>
            (University of Southern California),
            <a href="ROMAN24_AuthorIndexWeb.html#384256" title="Click to go to the Author Index">
             Banga, Subham
            </a>
            (University of Southern California),
            <a href="ROMAN24_AuthorIndexWeb.html#254126" title="Click to go to the Author Index">
             Dhanaraj, Neel
            </a>
            (University of Southern California),
            <a href="ROMAN24_AuthorIndexWeb.html#107914" title="Click to go to the Author Index">
             Gupta, Satyandra K.
            </a>
            (University of Southern California),
            <a href="ROMAN24_AuthorIndexWeb.html#117265" title="Click to go to the Author Index">
             Nikolaidis, Stefanos
            </a>
            (University of Southern California)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab493" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#transfer_learning" title="Click to go to the Keyword Index">
               Transfer Learning
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assembly" title="Click to go to the Keyword Index">
               Assembly
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We address the challenge of transferring human preferences for action selection from simpler source tasks to complex target tasks. Our goal is to enable robots to support humans proactively by predicting their actions --- without requiring demonstrations of their preferred action sequences in the target task. Previous research has relied on human experts to design or select a simple source task that can be used to effectively learn and transfer human preferences to a known target. However, identifying such source tasks for new target tasks can demand substantial human effort. Thus, we focus on automating the selection of source tasks, introducing two new metrics. Our first metric selects source tasks in which human preferences can be accurately learned from demonstrations, while our second metric selects source tasks in which the learned preferences, although not as accurate, can match the preferred human actions in the target task. We evaluate our metrics in simulated tasks and two human-led assembly studies. Our results indicate that selecting high-scoring source tasks on either metric improves the accuracy of predicting human actions in the target task. Notably, tasks chosen by our second metric can be simpler than the first, sacrificing learning accuracy but preserving prediction accuracy.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct3">
             <b>
              ThCT3
             </b>
            </a>
           </td>
           <td class="r">
            Room T3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thct3" title="Click to go to the Program at a Glance">
             <b>
              Robot Companions and Social Robots I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_01">
             14:40-16:10, Paper ThCT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('178'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring the Impact of Confirmation and Interaction During Human-Robot Collaboration with a Proactive Robot Assistant
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#359569" title="Click to go to the Author Index">
             Smith, Ronnie
            </a>
            (Heriot-Watt University),
            <a href="ROMAN24_AuthorIndexWeb.html#159688" title="Click to go to the Author Index">
             Dragone, Mauro
            </a>
            (Heriot-Watt University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab178" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robotic_etiquette" title="Click to go to the Keyword Index">
               Robotic Etiquette
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robots should not make us feel uncomfortable in our own homes, and so we must be able to trust them. Proactive robot assistants, designed to collaborate with humans on Activities of Daily Living (ADLs), operate in a tricky social situation as they emulate complex human-human interactions. This paper considers how humans expect robots to interact in such situations, particularly with regard to action confirmations, and how this impacts trust. A video-based user study was conducted with over 100 participants, comparing three distinct interaction personas. Our findings highlight that: (i) while communication and explanation are a significant factor in improving trust, it may be necessary in many cases to actually confirm actions before executing them; and (ii) there is a divide between individuals who value efficiency and speed versus those who put control above all else.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_02">
             14:40-16:10, Paper ThCT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('218'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data Augmentation for 3DMM-Based Arousal-Valence Prediction for HRI
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399654" title="Click to go to the Author Index">
             Arzate Cruz, Christian
            </a>
            (Honda Research Institute Japan),
            <a href="ROMAN24_AuthorIndexWeb.html#399659" title="Click to go to the Author Index">
             Sechayk, Yotam
            </a>
            (The University of Tokyo),
            <a href="ROMAN24_AuthorIndexWeb.html#127125" title="Click to go to the Author Index">
             Igarashi, Takeo
            </a>
            (The University of Tokyo),
            <a href="ROMAN24_AuthorIndexWeb.html#116318" title="Click to go to the Author Index">
             Gomez, Randy
            </a>
            (Honda Research Institute Japan Co., Ltd)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab218" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humans use multiple communication channels to interact with each other. For instance, body gestures or facial expressions are commonly used to convey an intent. The use of such non-verbal cues has motivated the development of prediction models. One such approach is predicting arousal and valence (AV) from facial expressions. However, making these models accurate for human-robot interaction (HRI) settings is challenging as it requires handling multiple subjects, challenging conditions, and a wide range of facial expressions. In this paper, we propose a data augmentation (DA) technique to improve the performance of AV predictors using 3D morphable models (3DMM). We then utilize this approach in an HRI setting with a mediator robot and a group of three humans. Our augmentation method creates synthetic sequences for underrepresented values in the AV space of the SEWA dataset, which is the most comprehensive dataset with continuous AV labels. Results show that using our DA method improves the accuracy and robustness of AV prediction in real-time applications. The accuracy of our models on the SEWA dataset is 0.793 for arousal and valence.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_03">
             14:40-16:10, Paper ThCT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('41'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Who Should Speak? Voice Cue Design for a Mobile Robot Riding in a Smart Elevator
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#101690" title="Click to go to the Author Index">
             Shiomi, Masahiro
            </a>
            (ATR),
            <a href="ROMAN24_AuthorIndexWeb.html#391244" title="Click to go to the Author Index">
             Kakio, Masayuki
            </a>
            (Mitsubishi Electric Corporation),
            <a href="ROMAN24_AuthorIndexWeb.html#106750" title="Click to go to the Author Index">
             Miyashita, Takahiro
            </a>
            (ATR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab41" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_presence_for_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Social Presence for Robots and Virtual Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent mobile robots have the capability to take elevators from different floors in buildings. Although various technologies enable robots to use elevators for their daily environments, past research works mainly focused on the side of mobile robots, e.g., social behaviors through the robot’s device. In other words, the effects of social behaviors from smart elevators and the collaborative situations between mobile robots and elevators have received inadequate attention. Therefore, in this study, we investigate the effects of interaction design for a mobile robot and a smart elevator when the former takes the latter. We conducted an experiment where a mobile robot and/or an elevator spoke to passengers as voice cues and evaluated their perceived impressions toward the robot and the elevator. The experiment results showed positive effects of the voice cues of the smart elevator and the mobile robot, although we found no combination effects of both voice cues. On the other hand, either voice cue creates a positive impression toward the other, i.e., the elevator voice cue improves the perceived impressions of the mobile robot and vice versa. This phenomenon provides useful knowledge for both elevator/robot developers.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_04">
             14:40-16:10, Paper ThCT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('164'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              How Robots Influence Human Perception: Investigating the Role of Body Language and Music in Emotion Perception for Social HRI
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#286804" title="Click to go to the Author Index">
             Liang, Nan
            </a>
            (University of Toronto),
            <a href="ROMAN24_AuthorIndexWeb.html#104747" title="Click to go to the Author Index">
             Nejat, Goldie
            </a>
            (University of Toronto)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab164" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#motivations_and_emotions_in_robotics" title="Click to go to the Keyword Index">
               Motivations and Emotions in Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Emotional dance is an engaging and stimulating multimodal social activity involving the display of both body language and music. An interesting area of research is in the investigation of how people perceive the emotions of robots. In particular, how the interaction between several modalities influences emotion perception in human-robot interactions (HRI). In this paper, we present the first study that investigates how robot body movements and music influence human emotion perception with respect to robot emotional dance. Through an online survey, 115 participants rated the emotion expressed by the dancing robot with varying body movements and music in two conditions: 1) robot dancing with music (visual + auditory) and 2) robot dancing without music (only visual). Our results showed that perceived valence is primarily influenced by robot body language and movements, especially with respect to positive valence, regardless of the presence of the music mode. Women also had higher perceived negative valence when observing the negative valence body movements displayed by the robot than men did. Furthermore, music had limited influence on the perception of: 1) valence when negative valence body language is displayed while the music had positive valence, and 2) arousal when the music had negative arousal. Our study provides insights on how to effectively design social HRI when considering human emotional perception of robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_05">
             14:40-16:10, Paper ThCT3.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('405'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing Human Perception of Direct Gaze from a Social Robot through Eye-Head Coordination
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#303515" title="Click to go to the Author Index">
             Fang, Yu
            </a>
            (Honda Research Institute Japan Co., Ltd),
            <a href="ROMAN24_AuthorIndexWeb.html#377241" title="Click to go to the Author Index">
             Perez-Moleron, Jose Manuel
            </a>
            (Universidad Pablo De Olavide),
            <a href="ROMAN24_AuthorIndexWeb.html#103530" title="Click to go to the Author Index">
             Merino, Luis
            </a>
            (Universidad Pablo De Olavide),
            <a href="ROMAN24_AuthorIndexWeb.html#116318" title="Click to go to the Author Index">
             Gomez, Randy
            </a>
            (Honda Research Institute Japan Co., Ltd)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab405" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The development and integration of robots capable of expressing gaze directionality through eye-head movements are crucial for effective human-robot interaction, especially for those with eye designs on 2D screens. Our proposed mutual eye-head gaze model aligns eye movements with head/body rotation, incorporating an attention engine for estimating the most saliency location, and a retina-fovea engine for precise gaze alignment. Additionally, the eye-head engine controls head movements, enhancing the robot's ability to perform responsive coordinated eye-head gaze behaviors. This improvement leads to enhanced human subjective perception of direct gaze from the robot, ultimately holding potential for advancing human-robot interaction in social dynamics and human-centered robot development research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct3_06">
             14:40-16:10, Paper ThCT3.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('448'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Can You Fill Me? a Coffee Machine with Robotic Behavior That Promotes Prosocial Behavior
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400411" title="Click to go to the Author Index">
             Gery, Adi
            </a>
            (Media Innovation Lab, Reichman University),
            <a href="ROMAN24_AuthorIndexWeb.html#400416" title="Click to go to the Author Index">
             Weizman, Chen
            </a>
            (Media Innovation Lab, Reichman University),
            <a href="ROMAN24_AuthorIndexWeb.html#400418" title="Click to go to the Author Index">
             Klipshtein, Guy
            </a>
            (Media Innovation Lab, Reichman University),
            <a href="ROMAN24_AuthorIndexWeb.html#230433" title="Click to go to the Author Index">
             Megidish, Benny
            </a>
            (Media Innovation Lab, the Interdisciplinary Center (IDC) Herzliy),
            <a href="ROMAN24_AuthorIndexWeb.html#230312" title="Click to go to the Author Index">
             Erel, Hadas
            </a>
            (Media Innovation Lab, Interdisciplinary Center Herzliya)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab448" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human societies rely on cooperation and mutual assistance, yet individuals often prioritize personal interests over acting prosocially in daily life. To explore the potential of Human-Robot Interaction (HRI) to enhance prosocial behavior, we designed a robotic coffee machine that encourages a behavior that would benefit others. We examined the impact of embedding robotic gestures into a familiar context of making coffee in a public machine and tested participants' willingness to fill in the water tank after making coffee. Participants' behavior was compared to a baseline group performing the same task without the robotic gesture. Results revealed that the majority of participants in the experimental group demonstrated a willingness to fill in the water tank, indicating a prosocial inclination. Overall, participants reported a positive experience, with clear indications of their understanding of the robotic request to fill the water tank and recognition of the robot's contribution to fostering prosocial behavior.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct4">
             <b>
              ThCT4
             </b>
            </a>
           </td>
           <td class="r">
            Room T4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thct4" title="Click to go to the Program at a Glance">
             <b>
              Social Intelligence for Robots I
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_01">
             14:40-16:10, Paper ThCT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('64'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Should I Help?: A Skill-Based Framework for Deciding Socially Appropriate Assistance in Human-Robot Interactions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#283594" title="Click to go to the Author Index">
             Ramnauth, Rebecca
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#103798" title="Click to go to the Author Index">
             Brscic, Drazen
            </a>
            (Kyoto University),
            <a href="ROMAN24_AuthorIndexWeb.html#106996" title="Click to go to the Author Index">
             Scassellati, Brian
            </a>
            (Yale)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab64" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#assistive_robotics" title="Click to go to the Keyword Index">
               Assistive Robotics
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As robots are increasingly integrated into various aspects of everyday life, it becomes essential to develop intelligent systems capable of providing assistance while maintaining social appropriateness. In this paper, we challenge the prevailing assumption that robots should always offer help, prompting an essential discussion of when robots should offer help. We present a systematic way of considering socially appropriate assistance in human-robot interaction and introduce a theoretical framework that enables robots to discern whether or not to offer help to a human user. We examine the factors that influence the social appropriateness of help, including the relative skill levels between the robot and user and measures for assessing the social value and cost of help. Through a series of illustrative examples, we demonstrate the feasibility of our framework in providing socially appropriate assistance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_02">
             14:40-16:10, Paper ThCT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('66'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Social Space Segmentation for Approaching Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#338168" title="Click to go to the Author Index">
             Silva, Aline Fernanda Furtado
            </a>
            (Universidade Federal De Minas Gerais - Ufmg),
            <a href="ROMAN24_AuthorIndexWeb.html#127628" title="Click to go to the Author Index">
             G. Macharet, Douglas
            </a>
            (Universidade Federal De Minas Gerais)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab66" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#interaction_kinesics" title="Click to go to the Keyword Index">
               Interaction Kinesics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a novel methodology for the partitioning of the social space associated with arbitrary groups of individuals into approachable regions. The proposed method exploits boundary points within pre-identified social spaces and classifies them into three distinct levels based on the field of view between pairs of individuals within the group. The effectiveness of this approach was rigorously assessed across both static and dynamic scenarios, illustrating successful segmentation and classification of approachable regions. Furthermore, the methodology underwent comprehensive testing using publicly available datasets, yielding consistently satisfactory outcomes. This innovative approach represents a substantial contribution to the field of human-robot interaction, showcasing significant potential for future enhancements and advancements.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_03">
             14:40-16:10, Paper ThCT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('386'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Effects of a Gossiping Robot on Team Cohesion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400376" title="Click to go to the Author Index">
             Limprayoon, Jirachaya "Fern"
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#289740" title="Click to go to the Author Index">
             Georgiou, Nicholas C.
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#400377" title="Click to go to the Author Index">
             Ua-arak, Natnaree "Proud"
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#106996" title="Click to go to the Author Index">
             Scassellati, Brian
            </a>
            (Yale)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab386" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Gossip is a human behavior that has been shown to strengthen bonds, trust, and the feeling of inclusion between the gossiper and the person with whom they share the gossip. As humans engage more with social robots, fostering bonds between them is critical for meaningful interactions. In this paper, we investigated how gossiping can affect the perception of group inclusion and trust between a human and a robot. In this between-subjects user study (N = 38), we compared the effects of a robot that gossips to the participant in either a positive or negative way about the experimenter during an interaction. We found that participants in the positive condition reported a significant increase in group inclusion with the robot, while participants in the negative condition did not. We also found that participants' moral trust in the negative condition significantly decreased. Our results suggested that positive gossip can be beneficial to human-robot team cohesion.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_04">
             14:40-16:10, Paper ThCT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('286'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              “Bad Idea, Right?” Exploring Anticipatory Human Reactions for Outcome Prediction in HRI
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#354373" title="Click to go to the Author Index">
             Parreira, Maria Teresa
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#400170" title="Click to go to the Author Index">
             Gowdru Lingaraju, Sukruth
            </a>
            (Cornell University, Cornell Tech),
            <a href="ROMAN24_AuthorIndexWeb.html#354384" title="Click to go to the Author Index">
             Ramirez-Artistizabal, Adolfo
            </a>
            (Accenture),
            <a href="ROMAN24_AuthorIndexWeb.html#354370" title="Click to go to the Author Index">
             Bremers, Alexandra
            </a>
            (Cornell Tech),
            <a href="ROMAN24_AuthorIndexWeb.html#400172" title="Click to go to the Author Index">
             Saha, Manaswi
            </a>
            (Accenture Labs),
            <a href="ROMAN24_AuthorIndexWeb.html#354390" title="Click to go to the Author Index">
             Kuniavsky, Michael
            </a>
            (Accenture),
            <a href="ROMAN24_AuthorIndexWeb.html#124149" title="Click to go to the Author Index">
             Ju, Wendy
            </a>
            (Cornell Tech)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab286" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Humans have the ability to anticipate what will happen in their environment based on perceived information; this ability is often manifested as an externally observable behavioral reaction. As robots become more prevalent in human spaces, robots could attend to these visible anticipatory responses to their actions to assess ``Is this a bad idea?'' In this study, we delved into the potential of human anticipatory reaction recognition to predict outcomes. We conducted a user study wherein 30 participants watched videos of action scenarios and were asked about their anticipated outcome of the situation shown in each video (``good'' or ``bad''). We collected video and audio data of the participants reactions as they were watching these videos. We then carefully analyzed the participants' behavioral anticipatory responses; this data was used to train machine learning models to predict anticipated outcomes based on human observable behavior. Reactions are multimodal, compound and diverse, and we find significant differences in facial reactions. Model performances are around 0.5-0.6 test accuracy, and increase notably when nonreactive participants are excluded from the dataset. We discuss the implications of these findings and future work. This research offers insights into improving the safety and efficiency of human-robot interactions, contributing to the evolving field of robotics and human-robot collaboration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_05">
             14:40-16:10, Paper ThCT4.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('135'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Where Can I Park My Robot? Modeling Out-Of-The-Way Parking Spots in the Home Using Room Geometry
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#106721" title="Click to go to the Author Index">
             Glas, Dylan F.
            </a>
            (Amazon),
            <a href="ROMAN24_AuthorIndexWeb.html#104078" title="Click to go to the Author Index">
             Smart, William
            </a>
            (Oregon State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab135" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             For social robots operating in home environments, identifying appropriate parking locations which are "out of the way" is a challenging and multi-faceted problem. This paper proposes a solution to one core aspect of that problem, specifically a model for estimating locations where the robot may block walking paths through narrow spaces. For generality, this model assumes no
             <i>
              a priori
             </i>
             knowledge about user behaviors or semantic features in the space, and is derived purely from spatial geometry based on a standard 2D occupancy map. An experimental validation based on self-reported parking spot preferences from long-term robot users demonstrates that the proposed model captures 74% of user preferences, outperforming a naive baseline condition in selecting user-preferred parking spots. The proposed method provides a basis for estimating socially-appropriate parking locations for robots operating in the home or other unstructured social spaces and serves as a foundation for developing more sophisticated parking spot preference models in the future.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct4_06">
             14:40-16:10, Paper ThCT4.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('335'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Let's Move On: Topic Change in Robot-Facilitated Group Discussions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400260" title="Click to go to the Author Index">
             Hadjiantonis, Georgios
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#210363" title="Click to go to the Author Index">
             Gillet, Sarah
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#149191" title="Click to go to the Author Index">
             Vázquez, Marynel
            </a>
            (Yale University),
            <a href="ROMAN24_AuthorIndexWeb.html#115856" title="Click to go to the Author Index">
             Leite, Iolanda
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#217970" title="Click to go to the Author Index">
             Dogan, Fethiye Irmak
            </a>
            (University of Cambridge)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab335" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robot-moderated group discussions have the potential to facilitate engaging and productive interactions among human participants. Previous work on topic management in conversational agents has predominantly focused on human engagement and topic personalization, with the agent having an active role in the discussion. Also, studies have shown the usefulness of including robots in groups, yet further exploration is still needed for robots to learn when to change the topic while facilitating discussions. Accordingly, our work investigates the suitability of machine-learning models and audiovisual non-verbal features in predicting appropriate topic changes. We utilized interactions between a robot moderator and human participants, which we annotated and used for extracting acoustic and body language-related features. We provide a detailed analysis of the performance of machine learning approaches using sequential and non-sequential data with different sets of features. The results indicate promising performance in classifying inappropriate topic changes, outperforming rule-based approaches. Additionally, acoustic features exhibited comparable performance and robustness compared to the complete set of multimodal features. Our annotated data is publicly available at https://github.com/ghadj/topic-change-robot-discussions-data-2024.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct5">
             <b>
              ThCT5
             </b>
            </a>
           </td>
           <td class="r">
            Room T5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thct5" title="Click to go to the Program at a Glance">
             <b>
              Storytelling in HRI
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_01">
             14:40-16:10, Paper ThCT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('122'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Degrees of Freedom: A Storytelling Game That Supports Technology Literacy about Social Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#287655" title="Click to go to the Author Index">
             Mott, Terran
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#399595" title="Click to go to the Author Index">
             Higger, Mark
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#302645" title="Click to go to the Author Index">
             Bejarano, Alexandra
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#174132" title="Click to go to the Author Index">
             Williams, Tom
            </a>
            (Colorado School of Mines)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab122" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#philosophical_issues_in_human_robot_coexistence" title="Click to go to the Keyword Index">
               Philosophical Issues in Human-Robot Coexistence
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#ethical_issues_in_human_robot_interaction_research" title="Click to go to the Keyword Index">
               Ethical Issues in Human-robot Interaction Research
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             To critically analyze and adapt to the risks and benefits of social robotics, future user communities will require technology and AI literacy: the ability to use new robotic technologies, understand their strengths and limitations, and critically evaluate the implications of their use. Research shows that collaborative, creative, and informal learning experiences can support AI literacy among non-technologists. Therefore, we designed Degrees of Freedom, a multiplayer interactive storytelling game that supports technology literacy about social robots. Degrees of Freedom supports technology literacy competencies by encouraging players to explore how values are encoded in robot designs, compelling players to consider the risks and limitations of robots, and encouraging them to make connections to their own lives and values. We present both the design of Degrees of Freedom and the results of game playtesting. Our results show that the narrative, collaborative nature of the game supported players in critical thinking about the role robots can or should have in their communities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_02">
             14:40-16:10, Paper ThCT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('342'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Robot to Guide Group-Therapy against Cognitive Decline: An Italian Pilot Study
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400265" title="Click to go to the Author Index">
             Amabili, Giulio
            </a>
            (IRCCS INRCA),
            <a href="ROMAN24_AuthorIndexWeb.html#357182" title="Click to go to the Author Index">
             Maranesi, Elvira
            </a>
            (IRCCS INRCA),
            <a href="ROMAN24_AuthorIndexWeb.html#400268" title="Click to go to the Author Index">
             Felici, Elisa
            </a>
            (Scientific Direction, IRCCS INRCA, Ancona),
            <a href="ROMAN24_AuthorIndexWeb.html#400269" title="Click to go to the Author Index">
             Margaritini, Arianna
            </a>
            (Scientific Direction, IRCCS INRCA, Ancona),
            <a href="ROMAN24_AuthorIndexWeb.html#400270" title="Click to go to the Author Index">
             Barbarossa, Federico
            </a>
            (Scientific Direction, IRCCS INRCA, Ancona),
            <a href="ROMAN24_AuthorIndexWeb.html#400272" title="Click to go to the Author Index">
             Marin, Andrei Iulian
            </a>
            (SC IRIS ROBOTICS SRL, Iasi,),
            <a href="ROMAN24_AuthorIndexWeb.html#400273" title="Click to go to the Author Index">
             Anghel, Ionut Manuel
            </a>
            (Tecnhical Univeristy of Cluj-Napoca, Cluj-Napoca),
            <a href="ROMAN24_AuthorIndexWeb.html#400267" title="Click to go to the Author Index">
             Bevilacqua, Roberta
            </a>
            (Scientific Direction, IRCCS INRCA, Ancona)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab342" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#narrative_and_story_telling_in_interaction" title="Click to go to the Keyword Index">
               Narrative and Story-telling in Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cognitive_skills_and_mental_models" title="Click to go to the Keyword Index">
               Cognitive Skills and Mental Models
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The ageing of population led to an increase of mild cognitive impairment cases, causing a struggling for society. Even though cognitive therapy is efficient in counteracting and slowing down the decline, due to the high demand for treatments, it is becoming increasingly difficult to deliver the therapy. In this scenario, the engAGE project aims to build a platform by integrating existing technologies like mobile app and social robot, to innovate cognitive therapy for older people in Europe. In this paper, the usability and the acceptability of the Pepper robot by Italian older people with mild cognitive impairment after 3 months of use in weekly group sessions are analyzed and discussed. Results show that acceptability, expressed in terms of intention to use, perceived usefulness, and positive attitude, overcomes the ease to use. Moreover, the group sessions with Pepper encourage the socialization and the participation of older people, who feel included and like protagonists of the co-design process.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_03">
             14:40-16:10, Paper ThCT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('179'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AI-Enhanced Social Robots for Older Adults Care: Evaluating the Efficacy of ChatGPT-Powered Storytelling in the EBO Platform
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399815" title="Click to go to the Author Index">
             Blanco, Antonio
            </a>
            (Universidad De Extremadura),
            <a href="ROMAN24_AuthorIndexWeb.html#337899" title="Click to go to the Author Index">
             Pérez-González, Gerardo
            </a>
            (Universidad De Extremadura),
            <a href="ROMAN24_AuthorIndexWeb.html#399816" title="Click to go to the Author Index">
             Condón, Alicia
            </a>
            (Universidad De Extremadura),
            <a href="ROMAN24_AuthorIndexWeb.html#360046" title="Click to go to the Author Index">
             Rodríguez-Domínguez, Trinidad
            </a>
            (Universidad De Extremadura),
            <a href="ROMAN24_AuthorIndexWeb.html#136981" title="Click to go to the Author Index">
             Núñez, Pedro
            </a>
            (University of Extremadura)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab179" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#narrative_and_story_telling_in_interaction" title="Click to go to the Keyword Index">
               Narrative and Story-telling in Interaction
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The population of older adults is growing worldwide, and with it, there is a need for appropriate and effective cognitive therapies. Social robots have shown potential as therapeutic tools for older adults, providing companionship, entertainment, and therapeutic support. In particular, storytelling activities promote cognitive stimulation, socio-emotional skills, and simple entertainment. However, conventional storytelling methodologies concerning engagement, personalization, and interactivity exhibit a potential for greater diversity. These traditional approaches often require extensive preparation time, which can limit their feasibility and adaptability in diverse settings. Social robots can help overcome these limitations by providing a more interactive and engaging storytelling experience. In this paper, we present an approach to enhance the storytelling capabilities of the EBO robot, a social robot designed for interaction with older adult people. As a novelty, in addition to integrating the proposed system in the CORTEX cognitive architecture, we offer to include the therapist in the loop and integrate Artificial Intelligence techniques, including a Large Language Model, ChatGPT, to enable the robot to generate natural-sounding and engaging narratives, adjust its interactions based on the cognitive impairments, and align with the interests and preferences of the user. We demonstrate the effectiveness of our approach through a case study involving older adult participants and therapists. Our preliminary results show that the EBO robot, integrating AI techniques, can provide an interactive, customizable, and socially aware storytelling experience that promotes cognitive stimulation, socio-emotional skills, and simple entertainment that is engaging and enjoyable for
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_04">
             14:40-16:10, Paper ThCT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('72'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Binded to the Lights - Storytelling with a Physically Embodied and a Virtual Robot Using Emotionally Adapted Lights
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#279916" title="Click to go to the Author Index">
             Steinhaeusser, Sophia C.
            </a>
            (University of Würzburg),
            <a href="ROMAN24_AuthorIndexWeb.html#349379" title="Click to go to the Author Index">
             Ganal, Elisabeth
            </a>
            (University of Würzburg),
            <a href="ROMAN24_AuthorIndexWeb.html#395094" title="Click to go to the Author Index">
             Yalcin, Murat
            </a>
            (University of Würzburg),
            <a href="ROMAN24_AuthorIndexWeb.html#199562" title="Click to go to the Author Index">
             Latoschik, Marc
            </a>
            (University of Würzburg),
            <a href="ROMAN24_AuthorIndexWeb.html#160324" title="Click to go to the Author Index">
             Lugrin, Birgit
            </a>
            (University of Wuerzburg)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab72" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Virtual environments (VEs) can be designed to evoke specific emotions for example by using colored light, not only applicable for games but also for virtual storytelling with a single storyteller. Social robots are perfectly suited as storytellers due to their multimodality. However, there is no research yet on the transferability of robotic storytelling to virtual reality (VR). In addition, the transfer of concepts from VE design such as adaptive room illumination to robotic storytelling has yet not been tested. Thus, we conducted a study comparing the same robotic storytelling with a physically embodied robotic storyteller and in VR to investigate the transferability of robotic storytelling to VR. As a second factor, we manipulated the room light following design guidelines for VEs or kept it constant. Results show that a virtual robotic storyteller is not perceived worse than a physically embodied storyteller, suggesting the applicability of virtual static robotic storytellers. Regarding emotion-driven lighting, no significant effect of colored lights on self-reported emotions was found, but adding colored light increased the social presence of the robot and its' perceived competence in both VR and reality. As our study was limited by a static robotic storyteller not using bodily expressiveness future work is needed to investigate the interaction between well-researched robot modalities and the rather new modality of colored light based on our results.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_05">
             14:40-16:10, Paper ThCT5.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('157'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Teachers, Take Care of the Essential. the Rest Is Story: Using LLM and Social Robots for Content Approaching by Storytelling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#198035" title="Click to go to the Author Index">
             Tozadore, Daniel
            </a>
            (École Polytechnique Fédérale De Lausanne (EPFL)),
            <a href="ROMAN24_AuthorIndexWeb.html#399770" title="Click to go to the Author Index">
             Rusu, Anne-Marie
            </a>
            (EPFL)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab157" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#child_robot_interaction" title="Click to go to the Keyword Index">
               Child-Robot Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Social robots are widely used for educational activities, especially to attract children's attention. As a side effect, pupils' excitement can suppress their focus on the tutors and the content being approached. A potential solution to tackle this issue is to equip the robot with storytelling strategies, which have been growing remarkably in recent years thanks to advances in Large Language Models. However, few studies are still addressing the resulting application in real-world conditions. In this work, we are exploring the GPT3 model for story generation based on content to be approached in maker-space classes. To achieve our goals, we implemented a web application for content insertion that connects to the robot through ROS. The proposal was validated in two phases: a first phase of interviews with 5 tutors of maker-space to present our solution and get their feedback, and two 180-minutes sessions with pupils for real-world validation. Results suggested the proposal has high potential for supporting multiple languages and generating suitable stories for diverse contexts. Furthermore, adding social behaviors, as encouragement and sentiment analysis, can help in the students' expectation handling.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct5_06">
             14:40-16:10, Paper ThCT5.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('48'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              What a Laugh! - Effects of Voice and Laughter on a Social Robot's Humorous Appeal and Recipients' Transportation and Emotions in Humorous Robotic Storytelling
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#279916" title="Click to go to the Author Index">
             Steinhaeusser, Sophia C.
            </a>
            (University of Würzburg),
            <a href="ROMAN24_AuthorIndexWeb.html#393206" title="Click to go to the Author Index">
             Knauer, Lara
            </a>
            (University of Wuerzburg),
            <a href="ROMAN24_AuthorIndexWeb.html#160324" title="Click to go to the Author Index">
             Lugrin, Birgit
            </a>
            (University of Wuerzburg)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab48" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#storytelling_in_hri" title="Click to go to the Keyword Index">
               Storytelling in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Storytelling is an important method not only in entertainment but also for education or therapy. Humor can improve storytelling due to its social functions. We investigate whether the humorous function of laughter can also be beneficial for robotic storytelling. We conducted a study focusing on the effects of voice type (human vs. synthetic) and laughter (presence vs. absence) on the robot’s perceived humorous appeal as well as recipients' transportation, i.e. their absorption into the story, and emotions. No significant differences were found regarding the manipulations. The type of voice and use of laughter are therefore less relevant than assumed. However, it is shown that negative emotions are significantly reduced by the humorous story presented by the robot. Furthermore, the story was perceived as more funny and elicited increased transportation when being received by the robot compared to reading the story. In conclusion, this study indicates that robots are generally suitable for humorous storytelling and interesting results for future research on humorous robotic storytelling.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thct6">
             <b>
              ThCT6
             </b>
            </a>
           </td>
           <td class="r">
            Room T6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thct6" title="Click to go to the Program at a Glance">
             <b>
              Creating Human-Robot Relationships II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_01">
             14:40-16:10, Paper ThCT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('257'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Identity and Community Matter(s): Exploring Sociocultural Dimensions of Functional Service Robots Acceptance in Public Spaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#327666" title="Click to go to the Author Index">
             Dobrosovestnova, Anna
            </a>
            (TU Wien),
            <a href="ROMAN24_AuthorIndexWeb.html#115798" title="Click to go to the Author Index">
             Weiss, Astrid
            </a>
            (TU Wien),
            <a href="ROMAN24_AuthorIndexWeb.html#400123" title="Click to go to the Author Index">
             Vetter, Ralf
            </a>
            (University of Salzburg, Human-Computer Interaction Division)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab257" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#long_term_experience_and_longitudinal_hri_studies" title="Click to go to the Keyword Index">
               Long-term Experience and Longitudinal HRI Studies
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As autonomous functional robots proliferate in public spaces, understanding their acceptance among diverse populations becomes crucial. Traditional models of technology acceptance focus on behavioral and cognitive factors, overlooking the sociocultural and temporal scaffolding of human-robot interaction. To address this gap, we present two themes (Identity Matters, Community Matters) resulting from a qualitative analysis of ethnographic interviews (n = 20) with passersby and vendors who have experienced Starship delivery robots deployed in their communities. Situating the outcomes of our analysis in relation to existing technology acceptance models, we offer four lessons contributing to the efforts in understanding existence and usage-related acceptance of autonomous functional robots in public spaces.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_02">
             14:40-16:10, Paper ThCT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('385'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Lifecycle of Social Robots: Obsolescence and Values in Repair
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#300485" title="Click to go to the Author Index">
             Kamino, Waki
            </a>
            (Cornell University),
            <a href="ROMAN24_AuthorIndexWeb.html#116082" title="Click to go to the Author Index">
             Sabanovic, Selma
            </a>
            (Indiana University Bloomington),
            <a href="ROMAN24_AuthorIndexWeb.html#185174" title="Click to go to the Author Index">
             Jung, Malte
            </a>
            (Cornell University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab385" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#long_term_experience_and_longitudinal_hri_studies" title="Click to go to the Keyword Index">
               Long-term Experience and Longitudinal HRI Studies
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a “lifecycle perspective” on social robot design and human-robot interaction, and explores the practices of maintenance, repair, and letting go of social robots. Drawing on interviews with robot owners and representatives of robot development and repair companies, we argue that these previously disregarded aspects of everyday use provide a context for negotiating the social value and meaning of interactions with robots. We discuss owner concerns about robot obsolescence, as well as company support for long term human-robot interaction through repair, reuse, and giving owners closure in letting go of robots they can no longer use. Our work expands the purview of HRI study and design beyond the common focus on initial design and adoption and to perceptions and practices that can foster more enduring relationships with social robots, support sustainability in robot design, and address owners’ emotional attachment to robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_03">
             14:40-16:10, Paper ThCT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('420'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Laughter: Does an Appropriate Laugh Facilitate the Robot's Humor Performance?
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#271540" title="Click to go to the Author Index">
             Zhang, Heng
            </a>
            (ENSTA Paris, Institut Polytechnique De Paris),
            <a href="ROMAN24_AuthorIndexWeb.html#290413" title="Click to go to the Author Index">
             Hei, Xiaoxuan
            </a>
            (ENSTA Paris, Institut Polytechnique De Paris),
            <a href="ROMAN24_AuthorIndexWeb.html#175925" title="Click to go to the Author Index">
             Zhong, Junpei
            </a>
            (The Hong Kong Polytechnic University),
            <a href="ROMAN24_AuthorIndexWeb.html#106757" title="Click to go to the Author Index">
             Tapus, Adriana
            </a>
            (ENSTA Paris, Institut Polytechnique De Paris)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab420" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Laughter serves as a subtle social signal in human interaction, playing an essential role in expressing emotions and facilitating social connections. However, laughter comes in various forms and is usually accompanied by different non-verbal expressions, such as facial expressions and gestures. These accompanying factors can significantly influence the effect of laughter in diverse contexts, thus complicating the research on laughter, especially in understanding its role in social dynamics. Consequently, endowing robots with the ability to appropriately use laughter in interactions with humans is still a big challenge. Our current study focuses on the effect of robot laughter on robot humor expression. Our objective is to investigate whether and how two factors, the type of laughter and the robot laughter gesture, impact the overall humor performance. In this study, we selected four types of laughter (sarcastic, joyful, embarrassed, and relieved laughter) from a laughter corpus based on four specific types of jokes (Affiliative, Aggressive, Self-enhancing, and Self-defeating). For each type of laughter, we designed distinct robot gestures. During the humor performance, the robot NAO delivered jokes accompanied by matching or mismatching laughter, with or without corresponding gestures. To enhance the quantity and diversity of experimental data, we conducted an online survey utilizing recordings of the robot's humor performance. The experimental findings indicate that when the robot's laughter matches the type of humor in the joke, participants rate the humor performance significantly higher compared to situations where there is a mismatch. Additionally, the results confirm the positive impact of robot laughter gestures on humor performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_04">
             14:40-16:10, Paper ThCT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('363'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Seeing Eye to Eye with Robots: An Experimental Study Predicting Trust in Social Robots for Domestic Use
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#278654" title="Click to go to the Author Index">
             Fischer, Katrin
            </a>
            (University of Southern California),
            <a href="ROMAN24_AuthorIndexWeb.html#233289" title="Click to go to the Author Index">
             Velentza, Anna Maria
            </a>
            (University of Macedonia),
            <a href="ROMAN24_AuthorIndexWeb.html#400326" title="Click to go to the Author Index">
             Lucas, Gale
            </a>
            (University of Southern California),
            <a href="ROMAN24_AuthorIndexWeb.html#400323" title="Click to go to the Author Index">
             Williams, Dmitri
            </a>
            (University of Southern California)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab363" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#applications_of_social_robots" title="Click to go to the Keyword Index">
               Applications of Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The use of social robots in service tasks is spreading, showcasing advantages for both consumers and service providers. However, their widespread adoption is hindered by a notable lack of trust. Our study aims to uncover insights into the factors influencing the adoption of social robots in home settings, exploring the factors that lead users to trust and eventually adopt robots. We designed two different experimental conditions, presenting the Amazon Astro robot from different perspectives (high-angle and eye-level) and demonstrating its different abilities to 198 people recruited from MTurk. We employed both quantitative (trust, first impressions of warmth and competence as well as usability, familiarity, and attitudes) questionnaires and qualitative (word analysis) assessments, and results showed that participants had higher trust scores when seeing the robot from an eye-level perspective. In addition, usability, familiarity and competence were shown to explain a significant amount of variance in trust. While existing negative attitudes towards robots and the participants’ age were shown to be the strongest predictors for participants’ willingness to purchase a robot, trust was able to significantly affect use intention. We contribute to the broader understanding of the challenges and opportunities in integrating social robots into daily life, shedding light on the dynamics between technological innovation and consumer adoption.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_05">
             14:40-16:10, Paper ThCT6.5
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('261'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Connecting the Dots: Advancing the Understanding of Group-Robot Interactions in Public Spaces through Ego Network Analysis
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#327700" title="Click to go to the Author Index">
             Müller, Ana
            </a>
            (University of Applied Sciences Cologne),
            <a href="ROMAN24_AuthorIndexWeb.html#326261" title="Click to go to the Author Index">
             Richert, Anja
            </a>
            (University of Applied Sciences Cologne)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab261" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we introduce the adaptation of ego network analysis (ENA) as a methodology to investigate group- robot interactions (GRI) in public spaces. Based on a field study with 16 GRI, our research explores the complexities of group dynamics and offers a reflection on the use of ENA to assess these interactions. Our results contribute to a nuanced understanding of how group structures influence the perception of social robots in GRI. The insights on the use of ENA in human-robot interaction (HRI) lay the foundation for strengthening methodologies and advancing our understanding of social dynamics in HRI.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thct6_06">
             14:40-16:10, Paper ThCT6.6
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('388'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Building Intelligent and Intelligible AI: A Framework for Human-Like Autonomy and Explainability in Critical Infrastructure
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#308258" title="Click to go to the Author Index">
             Vemula, Srikanth
            </a>
            (CSBSJU),
            <a href="ROMAN24_AuthorIndexWeb.html#400381" title="Click to go to the Author Index">
             DuFresne-To, Kainen
            </a>
            (CSBSJU)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab388" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#creating_human_robot_relationships" title="Click to go to the Keyword Index">
               Creating Human-Robot Relationships
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a novel AI system architecture designed for critical infrastructure, emphasizing the importance of human-robot interaction (HRI) in improving the explainability of AI decision-making. The proposed approach combines Case-Based Reasoning (CBR) and ontology to create a dynamic AI framework that ensures transparency and trustworthiness in autonomous operations. In which the Case-Based Reasoning (CBR) component employs prototype and exemplar methods from cognitive psychology to mimic human decision-making, while the ontology component organizes the knowledge base, enhancing clarity and comprehension. The proposed system architecture consists of three interconnected components that work together to improve the AI's explainability and adaptability in dynamic environments. These components include: 1) a Case-Based Reasoning (CBR)-powered decision-making module that enables the AI to learn from past experiences and justify its actions in a human-understandable format; 2) an ontology-guided knowledge framework that provides a structured and semantic representation of information to guide AI operations; and 3) an HRI mechanism that facilitates effective collaboration between humans and the AI system, ensuring that autonomous decisions are transparent and aligned with human oversight.We evaluate the proposed architecture in a simulated environment, demonstrating improved AI explainability and reliability. Our findings show that the integration of CBR and ontology links decision-making with transparency, highlighting the role of HRI in accountable AI for critical infrastructure. The paper discusses the design and outcomes of the study, paving the way for the development of more transparent and trustworthy AI systems for critical infrastructure.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thpm2_br">
             <b>
              ThPM2_BR
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thpm2_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break - TIV
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt1">
             <b>
              ThDT1
             </b>
            </a>
           </td>
           <td class="r">
            Room T1
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thdt1" title="Click to go to the Program at a Glance">
             <b>
              Degrees of Autonomy and Teleoperation II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_01">
             16:20-16:35, Paper ThDT1.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('100'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot, Take the Joystick: Understanding Space Robotics Experts' Views on Autonomy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#399410" title="Click to go to the Author Index">
             Smith, Cailyn
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#287655" title="Click to go to the Author Index">
             Mott, Terran
            </a>
            (Colorado School of Mines),
            <a href="ROMAN24_AuthorIndexWeb.html#174132" title="Click to go to the Author Index">
             Williams, Tom
            </a>
            (Colorado School of Mines)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab100" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#human_factors_and_ergonomics" title="Click to go to the Keyword Index">
               Human Factors and Ergonomics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As robots become increasingly used in space exploration, it is important to ensure that space robots are developed with the appropriate level of autonomy. Semiautonomous robots operating in space contexts face unique challenges, as these robots often operate in situations that may be safety-critical, environments that are not fully known, and with communication delay to operators on Earth. Due to these challenges, there exist both advantages and risks to developing systems with high levels of autonomy to operate in space contexts. Therefore, we aim to investigate perspectives on the trade-offs of increased autonomy for space robotic systems and the human factors considerations that should be evaluated when designing these systems. We conducted qualitative interviews with five professionals in the space robotics industry to explore these perspectives. Our findings demonstrate that decisions regarding the level of autonomy of space robots is shaped not only by technical considerations, but also by operators' willingness to accept new technology, financial considerations, and even human operators' sense of control. Based on these results, we present design recommendations for roboticists and human factors engineers in the space robotics domain.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_02">
             16:35-16:50, Paper ThDT1.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('431'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Locating the Fruit to Be Harvested and Estimating Cut Positions from RGBD Images Acquired by a Camera Moved Along Fixed Paths Using a Mask-R-CNN Based Method
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#349722" title="Click to go to the Author Index">
             Zhao, Wentao
            </a>
            (Waseda University),
            <a href="ROMAN24_AuthorIndexWeb.html#140460" title="Click to go to the Author Index">
             Otani, Takuya
            </a>
            (Shibaura Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#371942" title="Click to go to the Author Index">
             Sugiyama, Soma
            </a>
            (Waseda University),
            <a href="ROMAN24_AuthorIndexWeb.html#371940" title="Click to go to the Author Index">
             Mitani, Kento
            </a>
            (Waseda University),
            <a href="ROMAN24_AuthorIndexWeb.html#343265" title="Click to go to the Author Index">
             Masaya, Koki
            </a>
            (Waseda University),
            <a href="ROMAN24_AuthorIndexWeb.html#102287" title="Click to go to the Author Index">
             Takanishi, Atsuo
            </a>
            (Waseda University),
            <a href="ROMAN24_AuthorIndexWeb.html#343266" title="Click to go to the Author Index">
             Aotake, Shuntaro
            </a>
            (Waseda University),
            <a href="ROMAN24_AuthorIndexWeb.html#343267" title="Click to go to the Author Index">
             Funabashi, Masatoshi
            </a>
            (SonyCSL),
            <a href="ROMAN24_AuthorIndexWeb.html#213926" title="Click to go to the Author Index">
             Ohya, Jun
            </a>
            (Waseda University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab431" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#machine_learning_and_adaptation" title="Click to go to the Keyword Index">
               Machine Learning and Adaptation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Compared to traditional agricultural environments, the high density and diversity of vegetation layouts in Synecoculture farms present significant challenges in locating and harvesting occluded fruits and pedicels (cutting points). To address this challenge, this study proposes a Mask R-CNN-based method for locating fruits (tomatoes, yellow bell peppers, etc.) and estimating the pedicels from RGBD images acquired by a camera moved along fixed paths. After obtaining masks of all fruits and pedicels, this method judges the matching relationship between the located fruit and pedicel according to the 3D distance between the fruit and pedicel. Subsequently, this research determines the least occluded best viewpoint for harvesting based on the visible real areas of located fruits in images acquired under the fixed paths, and harvesting is then completed from this best viewpoint following a straight path. Experimental results show this method effectively identifies occluded targets and their cutting positions in both Gazebo simulation environments and real-world farms. This method can select the least occluded viewpoint for a high harvesting success rate.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_03">
             16:50-17:05, Paper ThDT1.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('161'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Comparing a 2D Keyboard and Mouse Interface to Virtual Reality for Human-In-The-Loop Robot Planning for Mobile Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#268504" title="Click to go to the Author Index">
             LeMasurier, Gregory
            </a>
            (University of Massachusetts Lowell),
            <a href="ROMAN24_AuthorIndexWeb.html#360892" title="Click to go to the Author Index">
             Tukpah, James
            </a>
            (Northeastern University),
            <a href="ROMAN24_AuthorIndexWeb.html#202314" title="Click to go to the Author Index">
             Wonsick, Murphy
            </a>
            (Boston Dynamics AI Institute),
            <a href="ROMAN24_AuthorIndexWeb.html#197998" title="Click to go to the Author Index">
             Allspaw, Jordan
            </a>
            (University of Massachusetts Lowell),
            <a href="ROMAN24_AuthorIndexWeb.html#283495" title="Click to go to the Author Index">
             Hertel, Brendan
            </a>
            (University of Masssachusetts Lowell),
            <a href="ROMAN24_AuthorIndexWeb.html#374612" title="Click to go to the Author Index">
             Epstein, Jacob
            </a>
            (University of Massachusetts Amherst),
            <a href="ROMAN24_AuthorIndexWeb.html#160355" title="Click to go to the Author Index">
             Azadeh, Reza
            </a>
            (University of Massachusetts Lowell),
            <a href="ROMAN24_AuthorIndexWeb.html#129456" title="Click to go to the Author Index">
             Padir, Taskin
            </a>
            (Northeastern University),
            <a href="ROMAN24_AuthorIndexWeb.html#111458" title="Click to go to the Author Index">
             Yanco, Holly
            </a>
            (UMass Lowell),
            <a href="ROMAN24_AuthorIndexWeb.html#212062" title="Click to go to the Author Index">
             Phillips, Elizabeth
            </a>
            (George Mason University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab161" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human-in-the-loop robot teleoperation interfaces enable operators to control robots to complete complex tasks, as seen by the success of teams in the DARPA Robotics Challenge (DRC). In this work, we compare two human-in-the-loop planning interfaces, a 2D keyboard and mouse (KBM) interface modeled after those used in the DRC and a 3D virtual reality (VR) interface, for teleoperating a robot to perform navigation and manipulation tasks. In our study, we investigated operator performance, and cognitive workload while using the interface, as well as the perceived usability of each. We found that participants had better performance in both task types when using the KBM interface, however they experienced fewer collisions between the robot and the world in the VR interface. Given these findings, we recommend utilizing a KBM interface in low-risk situations where task performance is the primary factor. In high-risk scenarios, where collisions can be detrimental, we recommend using VR. With this work we aim to contribute to building effective and intuitive interfaces for human-in-the-loop planning to allow robots to complete complex tasks in challenging environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt1_04">
             17:05-17:20, Paper ThDT1.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('63'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Point, Segment, and Inspect: Leveraging Promptable Segmentation Models for Semi-Autonomous Aerial Inspection
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#291462" title="Click to go to the Author Index">
             Franceschini, Riccardo
            </a>
            (Eurecat, Centre Tecnològic De Catalunya, 08290 Cerdanyola Del Va),
            <a href="ROMAN24_AuthorIndexWeb.html#394533" title="Click to go to the Author Index">
             Rodriguez, Javier
            </a>
            (Eurecat),
            <a href="ROMAN24_AuthorIndexWeb.html#103432" title="Click to go to the Author Index">
             Fumagalli, Matteo
            </a>
            (Danish Technical University),
            <a href="ROMAN24_AuthorIndexWeb.html#311282" title="Click to go to the Author Index">
             Cayero, Julian
            </a>
            (Eurecat)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab63" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#degrees_of_autonomy_and_teleoperation" title="Click to go to the Keyword Index">
               Degrees of Autonomy and Teleoperation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Operating unmanned aerial vehicles (UAVs) for assets inspections poses distinct challenges encompassing the need to maintain a safe distance from the inspection area, ensure correct orientation towards the inspected surface, and achieve comprehensive coverage of the entire surface. Achieving these tasks is inherently complex and stressful. Therefore, a novel approach that seeks to enhance the piloting experience by harnessing the latest advancements in segmentation models, such as Segment Anything Model (SAM), is proposed. These models, thanks to their prompting capabilities, allow seamless communication between the operator and the UAV, opening up the possibility of defining intricate inspection regions through simple interactions. Within this approach, decision-making authority remains with the operator, while the UAV takes on the demanding task of segmenting the designated area and devising an appropriate traversal plan. Throughout this process, the operator's situational awareness is heightened through visual cues overlaid on the camera stream and a 3D panel presenting information of the drone position and spatially sensed data. This teleoperation framework allows the operator to maintain continuous control of the ongoing operation through a simplified interface. The paper delineates both the system and the methodology employed, showcasing the effectiveness of integrating segmentation models into the decision-making workflow. The validity of the proposed framework is established through testing within a photorealistic UAV simulator along with real experiments in a controlled laboratory environment.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt2">
             <b>
              ThDT2
             </b>
            </a>
           </td>
           <td class="r">
            Room T2
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thdt2" title="Click to go to the Program at a Glance">
             <b>
              Sound Design for Robots
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_01">
             16:20-16:35, Paper ThDT2.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('73'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An On-Device Robust Sound Recognition System for Real-Time Context Awareness of Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#326132" title="Click to go to the Author Index">
             Song, Ju-man
            </a>
            (LGE),
            <a href="ROMAN24_AuthorIndexWeb.html#395180" title="Click to go to the Author Index">
             kim, changmin
            </a>
            (Lg Electronics),
            <a href="ROMAN24_AuthorIndexWeb.html#326151" title="Click to go to the Author Index">
             Son, Jungkwan
            </a>
            (LGE)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab73" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#sound_design_for_robots" title="Click to go to the Keyword Index">
               Sound design for robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper suggests an on-device robust sound recognition system for robots in real-time. The proposed system is designed to enable the robot to detect a variety of sound events in a variety of locations, including noisy and reverberant sound environments. To use suggested system on target robots, two VGGish models are trained on sever-side and the pre-trained models infer using an audio topic from a on-device real-time buffer handling system. The buffer handling system and the training system of deep learning model are designed to get almost silmilar input audio stream with each normalization system. To get robust performance in various environments, we use log-mel feature for general environments and per-chennal energy normalization for noisy and reverberant environments. Each feature is switched and used in real time on the robot depending on the sound environment mode. Several experimental results demonstrate the robust performance of the proposed real-time robust sound recognition system on a target robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_02">
             16:35-16:50, Paper ThDT2.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('275'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Evaluating Gender Ambiguity, Novelty and Anthropomorphism in Humming and Talking Voices for Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#359742" title="Click to go to the Author Index">
             Kuch, Johanna Magdalena
            </a>
            (Augsburg University),
            <a href="ROMAN24_AuthorIndexWeb.html#190376" title="Click to go to the Author Index">
             Nasir, Jauwairia
            </a>
            (University of Augsburg),
            <a href="ROMAN24_AuthorIndexWeb.html#380826" title="Click to go to the Author Index">
             Mertes, Silvan
            </a>
            (University of Augsburg),
            <a href="ROMAN24_AuthorIndexWeb.html#400072" title="Click to go to the Author Index">
             Schlagowski, Ruben
            </a>
            (Universität Augsburg),
            <a href="ROMAN24_AuthorIndexWeb.html#123920" title="Click to go to the Author Index">
             Becker-Asano, Christian
            </a>
            (Stuttgart Media University),
            <a href="ROMAN24_AuthorIndexWeb.html#213960" title="Click to go to the Author Index">
             Andre, Elisabeth
            </a>
            (Augsburg University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab275" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#sound_design_for_robots" title="Click to go to the Keyword Index">
               Sound design for robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#anthropomorphic_robots_and_virtual_humans" title="Click to go to the Keyword Index">
               Anthropomorphic Robots and Virtual Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper investigates the effects of gender neutralization on the perception of anthropomorphism, gender specificity, and novelty for human voices, comparing spoken and hummed voice modalities. We evaluated gender-neutralized and original voice samples in both spoken and hummed formats using an online survey. Our results confirm that gender-neutralizing filters effectively reduce perceived gender specificity in both modalities, supporting their use in creating gender-neutral voices for humanoid robots. Hummed voices were perceived as more anthropomorphic and less novel than spoken voices, suggesting that non-verbal sound modalities can enhance the human likeness of gender-neutral androids while maintaining gender ambiguity. The study contributes to HRI by highlighting the potential of humming to fulfill users' expectations of interaction with android robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_03">
             16:50-17:05, Paper ThDT2.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('56'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vocalics in Human-Drone Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#192628" title="Click to go to the Author Index">
             Lieser, Marc
            </a>
            (RheinMain University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#192631" title="Click to go to the Author Index">
             Schwanecke, Ulrich
            </a>
            (RheinMain University of Applied Sciences)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab56" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#sound_design_for_robots" title="Click to go to the Keyword Index">
               Sound design for robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As the presence of flying robots continues to grow in both commercial and private sectors, it necessitates an understanding of appropriate methods for nonverbal interaction with humans. While visual cues, such as gestures incorporated into trajectories, are more apparent and thoroughly researched, acoustic cues have remained unexplored, despite their potential to enhance human-drone interaction. Given that additional audiovisual and sensory equipment is not always desired or practicable, and flight noise often masks potential acoustic communication in rotary-wing drones, such as through a loudspeaker, the rotors themselves offer potential for nonverbal communication. In this paper, the consequential sound during the flight of a quadrotor is utilized and modified to carry acoustic information while maintaining the visually perceived flight characteristics. A user study (N=192) demonstrates that acoustically augmenting the trajectories of two aerial gestures with the proposed approach makes them more easily distinguishable. This enhancement contributes to human-drone interaction through onboard means, particularly in situations where the human cannot see or look at the drone.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt2_04">
             17:05-17:20, Paper ThDT2.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('174'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sound Matters: Auditory Detectability of Mobile Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#277784" title="Click to go to the Author Index">
             Agrawal, Subham
            </a>
            (University of Bonn),
            <a href="ROMAN24_AuthorIndexWeb.html#399786" title="Click to go to the Author Index">
             Wessels, Marlene
            </a>
            (Johannes Gutenberg-University Mainz),
            <a href="ROMAN24_AuthorIndexWeb.html#307784" title="Click to go to the Author Index">
             de Heuvel, Jorge
            </a>
            (University of Bonn),
            <a href="ROMAN24_AuthorIndexWeb.html#327562" title="Click to go to the Author Index">
             Kraus, Johannes
            </a>
            (Ulm University, Dept. Human Factors),
            <a href="ROMAN24_AuthorIndexWeb.html#113220" title="Click to go to the Author Index">
             Bennewitz, Maren
            </a>
            (University of Bonn)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab174" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#sound_design_for_robots" title="Click to go to the Keyword Index">
               Sound design for robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile robots are increasingly being used in noisy environments for social purposes, e.g. to provide support in healthcare or public spaces. Since these robots also operate beyond human sight, the question arises as to how different robot types, ambient noise or cognitive engagement impacts the detection of the robots by their sound. To address this research gap, we conducted a user study measuring auditory detection distances for a wheeled (Turtlebot 2i) and quadruped robot (Unitree Go 1), which emit different consequential sounds when moving. Additionally, we also manipulated background noise levels and participants’ engagement in a secondary task during the study. Our results showed that the quadruped robot sound was detected significantly better (i.e., at a larger distance) than the wheeled one, which demonstrates that the movement mechanism has a meaningful impact on the auditory detectability. The detectability for both robots diminished significantly as background noise increased. But even in high background noise, participants detected the quadruped robot at a significantly larger distance. The engagement in a secondary task had hardly any impact. In essence, these findings highlight the critical role of distinguishing auditory characteristics of different robots to improve the smooth human-centered navigation of mobile robots in noisy environments.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt3">
             <b>
              ThDT3
             </b>
            </a>
           </td>
           <td class="r">
            Room T3
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thdt3" title="Click to go to the Program at a Glance">
             <b>
              Robot Companions and Social Robots II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_01">
             16:20-16:35, Paper ThDT3.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('186'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Excuse Me, May I Disturb You? the Influence of Politeness of a Social Robot on the Perception of Interruptions
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#349379" title="Click to go to the Author Index">
             Ganal, Elisabeth
            </a>
            (University of Würzburg),
            <a href="ROMAN24_AuthorIndexWeb.html#390751" title="Click to go to the Author Index">
             Habenicht, Michelle
            </a>
            (University of Wuerzburg),
            <a href="ROMAN24_AuthorIndexWeb.html#160324" title="Click to go to the Author Index">
             Lugrin, Birgit
            </a>
            (University of Wuerzburg)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab186" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robotic_etiquette" title="Click to go to the Keyword Index">
               Robotic Etiquette
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Social robots are finding their way into our society and must adapt to the social norms of everyday life. It is often unavoidable for robotic assistants to interrupt humans in their activities. These interruptions must therefore be designed in such a way that they have as little negative impact as possible on the ongoing activity and the perception of the robot. In this study, we investigate whether interruptions regarding health-promoting behavior delivered by a social robot in an office situation should be phrased politely rather than directly. A system was implemented to trigger either politely or directly phrased interruptions in a laboratory user study with participants executing an office task. We examined the perception of the robot, the task, and the interruption, as well as whether the health-promoting behavior was performed by the users. The results show that there were no significant differences in the perception of the robot and the interruptions, while the temporal demand of the task was perceived to be higher in the polite condition. In addition, the intention to follow the polite prompts was significantly higher, but there was no difference in the actual compliance with the prompts in the study itself.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_02">
             16:35-16:50, Paper ThDT3.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('147'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Patbot: Designing a Social Robot to Reduce Anxiety in Waiting Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#391127" title="Click to go to the Author Index">
             Kong, Zhilei
            </a>
            (Southern University of Science and Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#274413" title="Click to go to the Author Index">
             Lupetti, Maria Luce
            </a>
            (Politecnico Di Torino),
            <a href="ROMAN24_AuthorIndexWeb.html#396203" title="Click to go to the Author Index">
             Chen, Baihui
            </a>
            (Southern University of Science and Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#399456" title="Click to go to the Author Index">
             Li, Xueliang
            </a>
            (Southern University of Science and Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab147" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_art_and_entertainment" title="Click to go to the Keyword Index">
               Robots in art and entertainment
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents the design of a novel social robot, namely Patbot, to engage people in playful interactions to help reduce their anxiety in waiting environments. We introduce the rationale and design decisions made during the development of the robot. We evaluated the robot's usability through an experiment within a simulated waiting environment, followed by interviews with the participants. The study indicated a positive effect of interacting with Patbot regarding STAI-6 anxiety measures. Other quantitative and qualitative measures, including Godspeed questionnaires, observation, and post-interviews, revealed positive impressions made by Patbot regarding its likability and animacy and that it could engage individuals in intuitive and playful activities. Our findings suggest the promise of designing social robots for entertainment and relaxation to enhance people's emotional experiences in waiting environments.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_03">
             16:50-17:05, Paper ThDT3.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('10'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Exploring the Potential of Wheel-Based Mobile Motion As an Emotional Expression Modality
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#297831" title="Click to go to the Author Index">
             Lee, Jiyeon
            </a>
            (Ulsan National Institute of Science and Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#276443" title="Click to go to the Author Index">
             Park, Haeun
            </a>
            (Ulsan National Institue of Science and Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#116054" title="Click to go to the Author Index">
             Lee, Hui Sung
            </a>
            (UNIST (Ulsan National Institute of Science and Technology))
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab10" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We explore the potential of mobile motion as an expression modality for home social robots with a low expressivity. To gauge the potential of mobile motion, we examine its relative efficacy in terms of emotion perception accuracy, emotion intensity, and impression (anthropomorphism and animacy) compared to screen-based facial expressions. Additionally, we explore how users perceive the emotional intensity of expressions based on different degrees of expression by manipulating emotional features such as motion size and speed. Motion expressions are less accurate than facial expressions, but perform on par with facial expressions in other metrics. Its dynamic and expressive features elicit powerful emotional conveyance, in contrast to the low emotional impact associated with the monotonous nature of screen-based facial expressions. Further research is needed on ME for specific emotions, but in general, the higher the degree of expression, the more intense the emotion conveyed. We show that the degree of expression, i.e., the combination of emotional features and their modulation, can be utilized to express the emotional intensity, situational dependence, and personality of robots. In conclusion, we argue that mobile motion is a promising method to compensate for the weaknesses of screen-based facial expression, which is the dominant expression modality for low expressivity robots.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt3_04">
             17:05-17:20, Paper ThDT3.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('293'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Walking Your Robot Dog: Experiences and Lessons Learned
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400181" title="Click to go to the Author Index">
             Ahmed, Eshtiak
            </a>
            (Tampere University),
            <a href="ROMAN24_AuthorIndexWeb.html#400182" title="Click to go to the Author Index">
             Genç, Çağlar
            </a>
            (Tampere University),
            <a href="ROMAN24_AuthorIndexWeb.html#400184" title="Click to go to the Author Index">
             Spors, Velvet
            </a>
            (Tampere University),
            <a href="ROMAN24_AuthorIndexWeb.html#400185" title="Click to go to the Author Index">
             Hamari, Juho
            </a>
            (Tampere University),
            <a href="ROMAN24_AuthorIndexWeb.html#400187" title="Click to go to the Author Index">
             'Oz' Buruk, Oğuz
            </a>
            (Tampere University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab293" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Walking is an integral part of daily human lives which also has a great influence on happiness and wellbeing. Walking with a dog companion is one of the most popular forms of recreational walking that has similar benefits. The recent development of mobile zoomorphic robots, especially robot dogs has opened up new opportunities in the landscape of walking with companions. This led us to investigate how such robots can accompany humans in walking, and how the mobility and behavior of robots in a daily-life walking scenario affect humans’ walking experience. We interviewed nine participants who took a 15–20-minute walk with a companion robot around a university campus, to understand how diverse walking behaviors of a companion robot influence their perceived experiences. We have generated four key themes through thematic analysis. They imply that affective relationalities between humans and robots can build intimacy and empathy, whereas personal space and physical proximity need to be thought carefully to ensure interaction comfort and spontaneity. Additionally, the robot influenced people’s self-reflection and social values refraining them from enjoying an unknown experience, while ambiguity in communication led to less confidence and trust.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt4">
             <b>
              ThDT4
             </b>
            </a>
           </td>
           <td class="r">
            Room T4
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thdt4" title="Click to go to the Program at a Glance">
             <b>
              Social Intelligence for Robots II
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_01">
             16:20-16:35, Paper ThDT4.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('432'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Modeling Social Interaction Dynamics Using Temporal Graph Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#287096" title="Click to go to the Author Index">
             Kim, Joanne Taery
            </a>
            (Georgia Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400037" title="Click to go to the Author Index">
             Naik, Archit
            </a>
            (Honda Research Institute Japan),
            <a href="ROMAN24_AuthorIndexWeb.html#400329" title="Click to go to the Author Index">
             Jayarathne, Isuru
            </a>
            (Honda Research Institute Japan),
            <a href="ROMAN24_AuthorIndexWeb.html#180081" title="Click to go to the Author Index">
             Ha, Sehoon
            </a>
            (Georgia Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#164508" title="Click to go to the Author Index">
             Chew, Jouh Yeong
            </a>
            (Honda Research Institute Japan)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab432" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Integrating intelligent systems, such as robots, into dynamic group settings poses challenges due to the mutual influence of human behaviors and internal states. A robust representation of social interaction dynamics is essential for effective human-robot collaboration. Existing approaches often narrow their focus to facial expressions or speech, overlooking the broader context. We propose employing an adapted Temporal Graph Networks to comprehensively represent social interaction dynamics while enabling its practical implementation. Our method incorporates temporal multi-modal behavioral data including gaze interaction, voice activity and environmental context. This representation of social interaction dynamics is trained as a link prediction problem using annotated gaze interaction data. The F1-score outperformed the baseline model by 37.0%. This improvement is consistent for a secondary task of next speaker prediction which achieves an improvement of 29.0%. Our contributions are two-fold, including a model to representing social interaction dynamics which can be used for many downstream human-robot interaction tasks like human state inference and next speaker prediction. More importantly, this is achieved using a more concise yet efficient message-passing method, significantly reducing the message size from 768 to 14 while outperforming the baseline model.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_02">
             16:35-16:50, Paper ThDT4.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('256'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Shielding for Socially Appropriate Robot Listening Behaviors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#210363" title="Click to go to the Author Index">
             Gillet, Sarah
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#298371" title="Click to go to the Author Index">
             Marta, Daniel
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400129" title="Click to go to the Author Index">
             Akif, Mohammed
            </a>
            (KTH Royal Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#115856" title="Click to go to the Author Index">
             Leite, Iolanda
            </a>
            (KTH Royal Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab256" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#multimodal_interaction_and_conversational_skills" title="Click to go to the Keyword Index">
               Multimodal Interaction and Conversational Skills
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A crucial part of traditional reinforcement learning (RL) is the initial exploration phase, in which trying available actions randomly is a critical element. As random behavior might be detrimental to a social interaction, this work proposes a novel paradigm for learning social robot behavior - the use of shielding to ensure socially appropriate behavior during exploration and learning. We explore how a data-driven approach for shielding could be used to generate listening behavior. In a video-based user study (N=110), we compare shielded exploration to two other exploration methods. We show that the shielded exploration is perceived as more comforting and appropriate than a straightforward random approach. Based on our findings, we discuss the potential for future work using shielded and socially guided approaches for learning idiosyncratic social robot behaviors through RL.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_03">
             16:50-17:05, Paper ThDT4.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('196'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhancing LLM-Based Human-Robot Interaction with Nuances for Diversity Awareness
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#274214" title="Click to go to the Author Index">
             Grassi, Lucrezia
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#155018" title="Click to go to the Author Index">
             Recchiuto, Carmine Tommaso
            </a>
            (University of Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#108312" title="Click to go to the Author Index">
             Sgorbissa, Antonio
            </a>
            (University of Genova)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab196" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a system for diversity-aware autonomous conversation leveraging the capabilities of large language models (LLMs). The system adapts to diverse populations and individuals, considering factors like background, personality, age, gender, and culture. The conversation flow is guided by the structure of the system's pre-established knowledge base, while LLMs are tasked with various functions, including generating diversity-aware sentences. Achieving diversity-awareness involves providing carefully crafted prompts to the models, incorporating comprehensive information about users, conversation history, contextual details, and specific guidelines. To assess the system's performance, we conducted both controlled and real-world experiments, measuring a wide range of performance indicators.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt4_04">
             17:05-17:20, Paper ThDT4.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('272'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              What Am I? Evaluating the Effect of Language Fluency and Task Competency on the Perception of a Social Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#353491" title="Click to go to the Author Index">
             Ali, Shahira
            </a>
            (University of Virginia),
            <a href="ROMAN24_AuthorIndexWeb.html#331802" title="Click to go to the Author Index">
             Green, Haley N.
            </a>
            (University of Virginia),
            <a href="ROMAN24_AuthorIndexWeb.html#172618" title="Click to go to the Author Index">
             Iqbal, Tariq
            </a>
            (University of Virginia)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab272" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#social_intelligence_for_robots" title="Click to go to the Keyword Index">
               Social Intelligence for Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent advancements in robot capabilities have enabled them to interact with people in various human-social environments (HSEs). In many of these environments, the perception of the robot often depends on its capabilities, e.g., task competency, language fluency, etc. To enable fluent human-robot interaction (HRI) in HSEs, it is crucial to understand the impact of these capabilities on the perception of the robot. Although many works have investigated the effects of various robot capabilities on the robot's perception separately, in this paper, we present a large-scale HRI study (n = 60) to investigate the combined impact of both language fluency and task competency on the perception of a robot. The results suggest that while language fluency may play a more significant role than task competency in the perception of the verbal competency of a robot, both language fluency and task competency contribute to the perception of the intelligence and reliability of the robot. The results also indicate that task competency may play a more significant role than language fluency in the perception of meeting expectations and being a good teammate. The findings of this study highlight the relationship between language fluency and task competency in the context of social HRI and will enable the development of more intelligent robots in the future.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt5">
             <b>
              ThDT5
             </b>
            </a>
           </td>
           <td class="r">
            Room T5
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thdt5" title="Click to go to the Program at a Glance">
             <b>
              Multimodal Sensing, Learning, and Control for Robots in Human-Robot
              <br/>
              Collaborative Contexts
             </b>
            </a>
           </td>
           <td class="r">
            Special Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_01">
             16:20-16:35, Paper ThDT5.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('220'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Modular Framework for Flexible Planning in Human-Robot Collaboration (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#329286" title="Click to go to the Author Index">
             Belcamino, Valerio
            </a>
            (Università Degli Studi Di Genova),
            <a href="ROMAN24_AuthorIndexWeb.html#377207" title="Click to go to the Author Index">
             Kilina, Mariya
            </a>
            (University of Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#299506" title="Click to go to the Author Index">
             Lastrico, Linda
            </a>
            (Italian Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#213447" title="Click to go to the Author Index">
             Carfì, Alessandro
            </a>
            (University of Genoa),
            <a href="ROMAN24_AuthorIndexWeb.html#108313" title="Click to go to the Author Index">
             Mastrogiovanni, Fulvio
            </a>
            (University of Genoa)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab220" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#cooperation_and_collaboration_in_human_robot_teams" title="Click to go to the Keyword Index">
               Cooperation and Collaboration in Human-Robot Teams
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#novel_interfaces_and_interaction_modalities" title="Click to go to the Keyword Index">
               Novel Interfaces and Interaction Modalities
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a comprehensive framework to enhance Human-Robot Collaboration (HRC) in real-world scenarios. It introduces a formalism to model articulated tasks, requiring cooperation between two agents, through a smaller set of primitives. Our implementation leverages Hierarchical Task Networks (HTN) planning and a modular multisensory perception pipeline, which includes vision, human activity recognition, and tactile sensing. To showcase the system's scalability, we present an experimental scenario where two humans alternate in collaborating with a Baxter robot to assemble four pieces of furniture with variable components. This integration highlights promising advancements in HRC, suggesting a scalable approach for complex, cooperative tasks across diverse applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_02">
             16:35-16:50, Paper ThDT5.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('383'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              How Is the Pilot Doing: VTOL Pilot Workload Estimation by Multimodal Machine Learning on Psycho-Physiological Signals (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400168" title="Click to go to the Author Index">
             Park, Jong Hoon
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#400189" title="Click to go to the Author Index">
             Chen, Lawrence
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#354878" title="Click to go to the Author Index">
             Higgins, Ian
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#214561" title="Click to go to the Author Index">
             Zheng, Zhaobo
            </a>
            (Honda Research Institute USA, Inc),
            <a href="ROMAN24_AuthorIndexWeb.html#362720" title="Click to go to the Author Index">
             Mehrotra, Shashank
            </a>
            (Honda Research Institute USA, Inc),
            <a href="ROMAN24_AuthorIndexWeb.html#400169" title="Click to go to the Author Index">
             Salubre, Kevin
            </a>
            (Honda Research Institute),
            <a href="ROMAN24_AuthorIndexWeb.html#234248" title="Click to go to the Author Index">
             Mousaei, Mohammadreza
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#373300" title="Click to go to the Author Index">
             Willits, Steven
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#239516" title="Click to go to the Author Index">
             Levedahl, Blaine
            </a>
            (Olis Robotics),
            <a href="ROMAN24_AuthorIndexWeb.html#400167" title="Click to go to the Author Index">
             Buker, Timothy
            </a>
            (Honda Research Institute USA, Inc),
            <a href="ROMAN24_AuthorIndexWeb.html#278576" title="Click to go to the Author Index">
             Xing, Eliot
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#163593" title="Click to go to the Author Index">
             Misu, Teruhisa
            </a>
            (Honda Research Institute USA, Inc),
            <a href="ROMAN24_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
            (Carnegie Mellon University),
            <a href="ROMAN24_AuthorIndexWeb.html#179403" title="Click to go to the Author Index">
             Oh, Jean
            </a>
            (Carnegie Mellon University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab383" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#monitoring_of_behaviour_and_internal_states_of_humans" title="Click to go to the Keyword Index">
               Monitoring of Behaviour and Internal States of Humans
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#detecting_and_understanding_human_activity" title="Click to go to the Keyword Index">
               Detecting and Understanding Human Activity
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Vertical take-off and landing (VTOL) aircraft do not require a prolonged runway, thus allowing them to land almost anywhere. In recent years, their flexibility has made them popular in development, research, and operation. When compared to traditional fixed-wing aircraft and rotorcraft, VTOLs bring unique challenges as they combine many maneuvers from both types of aircraft. Pilot workload is a critical factor for safe and efficient operation of VTOLs. In this work, we conduct a user study to collect multimodal data from 28 pilots while they perform a variety of VTOL flight tasks. We analyze and interpolate behavioral patterns related to their performance and perceived workload. Finally, we build machine learning models to estimate their workload from the collected data. Our results are promising, suggesting that quantitative and accurate VTOL pilot workload monitoring is viable. Such assistive tools would help the research field understand VTOL operations and serve as a stepping stone for the industry to ensure VTOL safe operations and further remote operations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_03">
             16:50-17:05, Paper ThDT5.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('418'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Generating and Evaluating Synthetic Data in Virtual Reality Simulation Environments for Pose Estimation (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#359882" title="Click to go to the Author Index">
             Sabbella, Sandeep Reddy
            </a>
            (Sapienza University of Rome),
            <a href="ROMAN24_AuthorIndexWeb.html#400419" title="Click to go to the Author Index">
             Serrarens, Pascal
            </a>
            (PaleBlue),
            <a href="ROMAN24_AuthorIndexWeb.html#360508" title="Click to go to the Author Index">
             Leotta, Francesco
            </a>
            (Sapienza Università Di Roma),
            <a href="ROMAN24_AuthorIndexWeb.html#137880" title="Click to go to the Author Index">
             Nardi, Daniele
            </a>
            (Sapienza University of Rome)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab418" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#virtual_and_augmented_tele_presence_environments" title="Click to go to the Keyword Index">
               Virtual and Augmented Tele-presence Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#evaluation_methods" title="Click to go to the Keyword Index">
               Evaluation Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Virtual Reality (VR) environments are used as a way to let humans experiment with algorithms, techniques, and situations in various application areas, including emergency management, serious games, smart manufacturing, and precision agriculture. They are especially relevant when experiments in the real world may be harmful to human operators. As VR environments are the closest possible faithful replicas of real environments, many recent works focus on the employment of such tools as a means to generate synthetic datasets that can be used for training machine and deep learning models, especially in situations where obtaining real datasets can be difficult. In this paper, we introduce a strategy to generate a dataset for pose estimation in the challenging scenario of precision agriculture. Finally, the quality of the generated dataset was evaluated.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt5_04">
             17:05-17:20, Paper ThDT5.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('466'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Speech and Facial Information Based Emotion Recognition System of Collaborative Robot for Empathic Human-Robot Collaboration (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#401034" title="Click to go to the Author Index">
             Loor, Jianna
            </a>
            (Montclair State University),
            <a href="ROMAN24_AuthorIndexWeb.html#400108" title="Click to go to the Author Index">
             Murphy, Jordan
            </a>
            (Montclair State University),
            <a href="ROMAN24_AuthorIndexWeb.html#219838" title="Click to go to the Author Index">
             Li, Rui
            </a>
            (Montclair State University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab466" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#affective_computing" title="Click to go to the Keyword Index">
               Affective Computing
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A robot’s ability to effectively recognize human emotions is critical in human-robot collaboration. However, most of the current collaborative robots were designed to improve productivity. Few of these robots consider human emotions. This situation would cause humans to be unwilling to work with robots for a long time. Motivated by this gap, this research developed a human emotion recognition system for enhancing the interaction abilities of collaborative robots. In this project, both speech and facial information were analyzed for robust human emotion recognition in complex working environments like manufacturing assembly environments. In the experiment, the developed system has been tested through three different human-robot co-assembly scenarios: (1) the robot effectively assists humans in finishing the task, (2) the robot is slow in response, leading to the task failing, (3) the robot frequently picks up wrong tools leading to task failure. Experimental results have demonstrated the effectiveness of the developed system in recognizing human co-worker’s emotions when the human and the robot were working in the above scenarios. It further shows the developed system has the potential to contribute to the development of an empathic collaborative robot companion in the manufacturing area.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thdt6">
             <b>
              ThDT6
             </b>
            </a>
           </td>
           <td class="r">
            Room T6
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thdt6" title="Click to go to the Program at a Glance">
             <b>
              Innovative Robot Designs
             </b>
            </a>
           </td>
           <td class="r">
            Regular Session
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_01">
             16:20-16:35, Paper ThDT6.1
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('353'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              R-R Manipulator Using Variable-Stiff Joints for Safe Human-Robot-Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#325320" title="Click to go to the Author Index">
             Sharma, Manoj Kumar
            </a>
            (Santa Clara University),
            <a href="ROMAN24_AuthorIndexWeb.html#100046" title="Click to go to the Author Index">
             Kitts, Christopher
            </a>
            (Santa Clara University)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab353" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#innovative_robot_designs" title="Click to go to the Keyword Index">
               Innovative Robot Designs
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#hri_and_collaboration_in_manufacturing_environments" title="Click to go to the Keyword Index">
               HRI and Collaboration in Manufacturing Environments
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#user_centered_design_of_robots" title="Click to go to the Keyword Index">
               User-centered Design of Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human joints are unique in the sense that their stiffness is not fixed and rather adjustable which aids in interacting with the physical world. A robot with servo-adjustable stiffness is intrinsically safe and capable of imitating human-like interactions. Apart from safety, soft joints are efficient in executing cyclic tasks which opens the door to have an agile behaviour. In this paper we extend our prior work on variable stiff joints and use two of them to design a two-link planar robot. Here we modify the dynamic model for this manipulator to include the variable stiffness term in addition to the joint torques as inputs, followed by joint torque calibration method, and then a simplified manipulator control template. This paper serves as an introduction to our ongoing work on vSEA and a foray into “true” manipulator space application.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_02">
             16:35-16:50, Paper ThDT6.2
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('254'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Fusing Components for an Attentive and Emotionally Expressive Companion Robot: Meet ZENIT
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400076" title="Click to go to the Author Index">
             Purps, Christian Felix
            </a>
            (Karlsruhe University of Applied Sciences),
            <a href="ROMAN24_AuthorIndexWeb.html#400082" title="Click to go to the Author Index">
             Wölfel, Matthias
            </a>
            (Karlsruhe University of Applied Sciences)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab254" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#innovative_robot_designs" title="Click to go to the Keyword Index">
               Innovative Robot Designs
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#non_verbal_cues_and_expressiveness" title="Click to go to the Keyword Index">
               Non-verbal Cues and Expressiveness
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#computational_architectures" title="Click to go to the Keyword Index">
               Computational Architectures
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper introduces a new companion robot as an open research platform for studying human-robot interaction: The ZENIT Enabling Natural Interaction Technology. Open-source solutions for companion robots are often complicated to build or not thought of holistically. ZENIT is based on a few low-cost components, namely a smartphone and a stationary robot arm in conjunction with an average computer and a separate camera. This concept makes it possible not to be tied to specific hardware. Our platform aims to enable easy use of the multimodality of human communication channels such as body language, spatial behavior, facial expressions and speech so that ZENIT can understand its interlocutor and express itself appropriately. The fusion of various open-source software components in an adaptable, lightweight and resource-efficient distributed system simplifies its replication. The system demonstrated an adequate response time (speech: 2.69s, facial expressions: 0.55s, bodily movements: 0.32s) for perception and expression combined, as well as reliability in public testing. Evaluation of the robot's expressive capabilities in an online survey (N=23) revealed users were able to understand six of eight emotions conveyed by ZENIT using facial expressions and/or body language (except for contempt and fear) and perceived its non-verbal feedback as significantly more expressive when facial expressions were accompanied with bodily movements.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_03">
             16:50-17:05, Paper ThDT6.3
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('461'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Rapid and Robust Tendon-Driven Robotic Hand for Human-Robot Interactions Playing Rock-Paper-Scissors
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#207518" title="Click to go to the Author Index">
             Deng, Xiang
            </a>
            (ETH Zurich),
            <a href="ROMAN24_AuthorIndexWeb.html#367124" title="Click to go to the Author Index">
             Weirich, Stefan
            </a>
            (ETH Zurich),
            <a href="ROMAN24_AuthorIndexWeb.html#164300" title="Click to go to the Author Index">
             Katzschmann, Robert Kevin
            </a>
            (ETH Zurich),
            <a href="ROMAN24_AuthorIndexWeb.html#164327" title="Click to go to the Author Index">
             Delbruck, Tobi
            </a>
            (Univ. of Zurich &amp; ETH Zurich)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab461" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#innovative_robot_designs" title="Click to go to the Keyword Index">
               Innovative Robot Designs
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robots_in_education__therapy_and_rehabilitation" title="Click to go to the Keyword Index">
               Robots in Education, Therapy and Rehabilitation
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Rapid human-robot interactions require fast hardware platforms with minimal latency and high reliability. In response, we present a cost-effective, electrically actuated, tendon-driven robotic hand. This hand features a unique spool-free actuation mechanism that achieves a 66 ms limit-to-limit flexion movement, matching human speed. To our knowledge, it is among the fastest electric motor tendon-driven robotic hands available today. The high speed of the robotic hand was successfully demonstrated in public by playing Rock Paper Scissors at a science fair. This research work outlines the design methodology and introduces a simulation-optimization framework that allows users to preview the hand motion, quantify the actuation performance, and customize the design parameters prior to fabrication. The proposed actuation mechanism, along with the simulation and optimization tools, illustrates design principles and computational methods applicable to other dynamic human-robot applications that require fast reaction times. The Dextra hand design is available at https://sensorsini.github.io/dextra-robot-hand.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thdt6_04">
             17:05-17:20, Paper ThDT6.4
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('354'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Sacred or Uncanny? Exploring Visitors’ Reaction to a Robotic Saint in Exhibition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <a href="ROMAN24_AuthorIndexWeb.html#400296" title="Click to go to the Author Index">
             León Coral, Rafael Felipe
            </a>
            (Pontificia Universidad Catolica Del Peru),
            <a href="ROMAN24_AuthorIndexWeb.html#400302" title="Click to go to the Author Index">
             Kumar, Ujwal
            </a>
            (Shibaura Institute of Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400297" title="Click to go to the Author Index">
             Battaglia, Antonio
            </a>
            (University of Messina),
            <a href="ROMAN24_AuthorIndexWeb.html#400299" title="Click to go to the Author Index">
             Lam, Vu Hoai
            </a>
            (Ha Noi University of Science and Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400300" title="Click to go to the Author Index">
             Tong Van, Sinh
            </a>
            (Ha Noi University of Science and Technology),
            <a href="ROMAN24_AuthorIndexWeb.html#400301" title="Click to go to the Author Index">
             Wu, Kam Long
            </a>
            (University of Sao Paulo),
            <a href="ROMAN24_AuthorIndexWeb.html#250408" title="Click to go to the Author Index">
             Pariasca, Franco
            </a>
            (Pontificia Universidad Catolica Del Peru),
            <a href="ROMAN24_AuthorIndexWeb.html#150234" title="Click to go to the Author Index">
             Trovato, Gabriele
            </a>
            (Shibaura Institute of Technology)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab354" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ROMAN24_KeywordIndexWeb.html#interaction_with_believable_characters" title="Click to go to the Keyword Index">
               Interaction with Believable Characters
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#robot_companions_and_social_robots" title="Click to go to the Keyword Index">
               Robot Companions and Social Robots
              </a>
              ,
              <a href="ROMAN24_KeywordIndexWeb.html#innovative_robot_designs" title="Click to go to the Keyword Index">
               Innovative Robot Designs
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The intersection of religion and robotics presents a novel area of inquiry, challenging conventional wisdom in the relationship between technology and spirituality. Previous research has explored theoretical concepts, while reception remains polarised, and user studies showed mixed results. This paper introduces SanTO-MI, the latest iteration of the Catholic robot, placed in a robots exhibition. We investigates whether the improvements in interface and dialogue, and the leveraging of Large Language Models can enhance visitor perceptions in terms of sacredness, uncanniness, and more.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thawd">
             <b>
              ThAwD
             </b>
            </a>
           </td>
           <td class="r">
            Room T9 - Ballroom
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thawd" title="Click to go to the Program at a Glance">
             <b>
              Award and Inclusion Party
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td>
            <a name="thrc">
             <b>
              ThRC
             </b>
            </a>
           </td>
           <td class="r">
            Room T8 - hallway
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ROMAN24_ProgramAtAGlanceWeb.html#thrc" title="Click to go to the Program at a Glance">
             <b>
              Robot Competition - Prep II
             </b>
            </a>
           </td>
           <td class="r">
           </td>
          </tr>
         </table>
        </div>
        <p>
         <br/>
        </p>
        <p>
         <br/>
        </p>
       </td>
       <td height="100%" style="background-color:#000;" width="5">
       </td>
      </tr>
     </table>
     <table border="0" cellpadding="0" cellspacing="0" height="80%" width="100%">
      <tr>
       <td alt="" border="0" height="8" style="background-color:#000;" valign="center" width="100%">
        <p align="center">
         <span style="font-size:8pt;line-height:10pt;color:white;">
          Technical Content ©
IEEE Robotics &amp; Automation Society
         </span>
        </p>
       </td>
      </tr>
      <tr>
       <td width="100%">
        <p align="right">
         <span style="text-decoration:none;">
          <img align="right" border="0" src="/images/pc_logo_small.png" style="margin-left: 10px; margin-right: 10px"/>
          This site is protected
by copyright and trademark laws under US and International law.
          <br/>
          All rights
reserved. © 2002-2024 PaperCept, Inc.
          <br/>
          Page generated 2024-07-31  01:22:01 PST
          <a href="" onclick="window.open('/conferences/scripts/about.pl','tc','width=1000,scrollbars=yes'); return false">
           Terms
of use
          </a>
         </span>
        </p>
       </td>
      </tr>
     </table>
    </body>
   </div>
  </form>
 </body>
</html>
