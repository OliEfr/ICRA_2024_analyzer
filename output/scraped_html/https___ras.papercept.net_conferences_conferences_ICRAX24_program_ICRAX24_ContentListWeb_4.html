<!DOCTYPE HTML>
<html>
 <head>
  <meta content="en-us" http-equiv="Content-Language"/>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type"/>
  <meta content="width=device-width" name="viewport"/>
  <script src="https://ras.papercept.net/conferences/scripts/dom-drag.js" type="text/javascript">
  </script>
  <script src="jquery-1.11.1.min.js">
  </script>
  <title>
   ICRA@40 Program | Thursday September 26, 2024
  </title>
  <style type="text/css">
   body, table, td, th{
	Font-Family : sans-serif;
	Font-Size : 10pt;
}
.r {text-align: right}
.blue {color: #0000FF;}
td {vertical-align: top; text-align: left}
.c {text-align: center}
table.s {
	border-collapse:collapse;
	border-width: 1px;
}
table.s td{
	border-width: 1px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
table.t {
	border-collapse: collapse;
	border-width: 0px;
}
table.t td{
	border-width: 0px;
	padding: 4px;
	border-style: solid;
	border-color: gray;
}
.dots {
    background:url('./images/dot.gif') repeat-x center;
}
.field {
    background-color: #FFFFFF;
}
#pTitle { /* Page title */
   font-size: 14pt;
   line-height: 1.5em;
}
#pSubTitle { /* Page subtitle */
   color: #909090;
   font-size: 10pt; 
   line-height: 1.5em;
}
#container {
	position: absolute;
	width: 100%;
	margin-top: 2px;
/*	overflow: hidden; */
}

.sHdr {   /* Session header Content list */
   background-color: #F0E68C
}
      
.sSHdr {   /* Subsession header Content list */
   background-color: #f8f3c6 
}
      
table.trk { /* Track table Content list */
   border-collapse: collapse;
   border-width: 0px;
   margin: auto;
/**   width: 640px; **/
   width: 720px;
}
table.trk td{
   border-width: 0px;
   padding: 4px;
   border-style: solid;
   border-color: gray;
 }
      
.pHdr {  /* Paper header Content list */
   background-color: #E6E6FA;
   color: black;
}
hr.thin { /* Horizontal rule content list */
   border: 0px; 
   height: .8px; 
   background-color: #8888FF;
}
      
.pTtl {  /* Paper title Content list */
   font-size: 11pt;
   font-style: italic;
}
      
.ssHdr {  /* Subsession header container session Content list */
   background-color: #DDDDDD;
   color: black;
}
      
.ssTtl {  /* Subsession title container session Content list */
   font-size: 10pt;
   font-style: normal;
   font-weight: bold;
}
  </style>
  <script language="JavaScript">
   function initXMLHttp(){
   var oRequest = false;
   try {
      oRequest = new XMLHttpRequest();
   }  catch (trymicrosoft) {
      try {
         oRequest = new ActiveXObject("Msxml2.XMLHTTP");
      }  catch (othermicrosoft) {
         try {
            oRequest = new ActiveXObject("Microsoft.XMLHTTP");
         }  catch (failed) {
            oRequest = false;
         }
      }
   }
   if (!oRequest){
      alert("Error initializing XMLHttpRequest! Your browser does not support AJAX");
   }
   return oRequest;
}
function modify(number,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'Add';
   }
   else{
      action = 'Delete';
   }
   
//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=467&' + action + number;
//   window.open(url,'myprogrampage');

   modifyItem("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","467",action,number)

}


function modifyItem(url,ConfID,action,number){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&Number=' + number;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

var iIntervalId;  // Global variable
function modsession(id,kk){
   var check = document.getElementById('mod' + kk).checked;
   if (check){
      action = 'AddSession';
   }
   else{
      action = 'DelSession';
   }

//   url = 'https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=467&' + action + id;
//   window.open(url,'myprogrampage');

   modifySession("https://ras.papercept.net/conferences/scripts/myprogram_aja.pl","467",action,id)

}

function modifySession(url,ConfID,action,id){
   var oRequest = initXMLHttp();
   if (!oRequest){return;}
   
   // Send the request

   oRequest.open("post",url, true);
   var sParams = 'ConfID=' + ConfID + '&Action=' + action + '&ID=' + id;
   document.body.style.cursor = 'wait';
   oRequest.send(sParams);

   // Process the response
   
   oRequest.onreadystatechange = function(){ 
      if (oRequest && oRequest.readyState && oRequest.readyState == 4){
         document.body.style.cursor = 'auto';
         var responseText = oRequest.responseText;
         if (responseText.substring(0,5) == 'Error'){
            alert(responseText);
         }
         else{
         
//            alert(responseText);

         }
      }
   } 
}

function getCookie(sName){
   var sRE = "(?:; )?" + sName + "=([^;]*);?";   
   var oRE = new RegExp(sRE);
   if (oRE.test(document.cookie)){
      return decodeURIComponent(RegExp["$1"]);}
   else{
      return null;
   }
}
function loadprogram(){
   var list = getCookie("ICRAX24");
   if (list){
      var List = list.split(",");
      for (var i=0; i<List.length; i++){
         var names = document.getElementsByName('modify' + List[i]);
         if (names.length){
            for (var j=0; j<names.length; j++){
               names[j].checked = true;
            }
         }
      }
   }
}
function reset(){

   // Uncheck all modify and addsession checkboxes

   var ins = document.getElementsByTagName('input');
   for (var i=0; i<ins.length; i++){
      if (ins[i].type == 'checkbox' && ins[i].id && ins[i].id.substring(0,3) == 'mod'){
         ins[i].checked = false;
      }
   }
   
   // Reload the program
   
   loadprogram();
}
function startreset(){
   iIntervalId = setInterval(reset,2000);
}
function viewAbstract(number){
   var box = document.getElementById('Ab' + number);
   if (box.style.display == 'block'){
      box.style.display = 'none';
   }
   else if (box && box.style.display == 'none'){
      box.style.display = 'block';
   }
}
function openAllAbstracts(){
   var d = document.getElementsByTagName('div');
   var count = d.length;
   if (count == 0){return;}
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab' && d[i].style.display == 'none'){
         d[i].style.display = 'block';
      }
   }
}
function closeAllAbstracts(){
   var d = document.getElementsByTagName('div');
   for (var i=0; i<d.length; i++){
      if (d[i].id && d[i].id.substring(0,2) == 'Ab'){
         d[i].style.display = 'none';
      }
   }
}
var uhash;
var pColor;
$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){
   
      // Mark the session
   
      pColor = $('#' + uhash).parent().css('backgroundColor');
      $('#' + uhash).parent().css('backgroundColor','#FF8888');
   }
});


$(function() { 

   // Check for URL hash

   uhash = location.hash.substring(1);
   if (uhash.slice(-1) == '_'){
      uhash = null;
   }
   if (uhash){

      // Set the widths
      
      setwidth();
      
      // If claasical view is required then return

      if (!ghit){return;} 
      
      // Reset left margin for FF

      document.getElementById('container').scrollLeft = 0;;

      // Discover the table and the block and determine the block Id
   
      var rt = $('#' + uhash);
      var done = false;
      while (!done){
         rt = rt.parent();    
         var etype = rt.get(0).tagName;  
         if (rt.is("table")){      
            done = true;
         }
      }
      rt = rt.parent().parent().parent();
      var iid = rt.attr('id')

      // Show the block

      initialize();
      $('#' + iid).show();
      $( '#A' + iid ).focus();
      var ypos = $('#' + iid).offset().top;      
      window.scrollTo(0,ypos);

      // Cancel the scroll to uhash

      var url = location.href;
      url += '_';
      location.href = url;
      
      // Scroll into view

      var leftPosition = $('#' + uhash).parent().position().left;
      var topOffset = $('#' + uhash).parent().offset().top;
      var divOffset = $('#' + iid).find('div').offset().top;
      var topPosition = topOffset-divOffset;
      $('#' + iid).find('div').scrollLeft(leftPosition);
      $('#' + iid).find('div').scrollTop(topPosition);
   }
   else{
      setwidth();
      initialize();
   }
});

var ghit = false;
function setwidth(){
   var viewportwidth = $( window ).width();
   var viewportheight = $( window ).height();
   var sdiv = $( ".sdiv" );
   for (var i=0; i<sdiv.length; i++){
      $(sdiv[i]).css({width: .98*viewportwidth + 'px'});
      $(sdiv[i]).css("height", .9*viewportheight-50 + 'px');      
   }

   // Detect horizontal overflow on any of the divs
   
   var divs = document.getElementsByTagName('div');
   for (var i=0; i<divs.length; i++){
      if (divs[i].id && divs[i].id.substring(0,3) == 'div'){
         if (divs[i].scrollWidth > divs[i].clientWidth){
            ghit = true;
            break;
         }
      }
   }
   if (!ghit){
      for (var i=0; i<divs.length; i++){
         divs[i].style.height = 'auto';
      }
   }
}

function selfollowing(hsh){
   $('#' + uhash).parent().css('backgroundColor',pColor);
   setwidth();
   initialize();
   if (hsh == 'TheTop'){
      var ypos = $('#container').offset().top;
      window.scrollTo(0,ypos)
   }
   else{
      $('#' + hsh).show();
      $( '#A' + hsh ).focus();
      var ypos = $('#' + hsh).offset().top;
      window.scrollTo(0,ypos)
   }
}

function initialize(){

   // Show all day blocks
   
   var blcks = $('.blck');
   for (var i=0; i<blcks.length; i++){
      blcks[i].style.display = 'block';
   }

   // Detect horizontal overflow on any of the divs
   
   var hit = false;
   var divs = document.getElementsByTagName('div');
   for (var i=0; i<divs.length; i++){
      if (divs[i].id && divs[i].id.substring(0,3) == 'div'){
         if (divs[i].scrollWidth > divs[i].clientWidth || divs[i].scrollHeight > divs[i].clientHeight){
            hit = true;
            break;
         }
      }
   }
   if (hit){
   
      // Set overflow hidden on body. This will prevent it from scrolling
      
      $("body").css("overflow", "hidden");
      document.getElementById('start').style.display = 'inline';
      
      // Hide all day blocks
   
      var blcks = $('.blck');
      for (var i=0; i<blcks.length; i++){
         blcks[i].style.display = 'none';
      }
      var scrlis = $('.scrlis');
      for (var i=0; i<scrlis.length; i++){
         scrlis.show();
      }
   }
   else{
      $("body").css("overflow", "auto");
      document.getElementById('start').style.display = 'none';
      var blcks = $('.sdiv');
      for (var i=0; i<blcks.length; i++){
        blcks[i].style.height = 'auto';
      }
      var scrlis = $('.scrlis');
      for (var i=0; i<scrlis.length; i++){
         scrlis.hide();
      }
   }
   return;
}
  </script>
 </head>
 <body onload="loadprogram(); startreset()" onresize="setwidth(); initialize()">
  <form action="https://ras.papercept.net/conferences/scripts/myprogram.pl" name="myprogram">
   <div id="container">
    <body leftmargin="0" marginheight="0" marginwidth="0" topmargin="0">
     <table border="0" cellpadding="0" cellspacing="0" width="100%">
      <tr>
       <td height="140" style="background-color:#5B2999;" width="100%">
        <img alt="" border="0" height="120" src="/images/icra/icraa40_logo.webp" style="position:absolute;left:20px;top:10px;z-index:1;"/>
        <span style="font-size: 56px; font-family:Arial,sans serif;text-align:left;position:absolute;left:250px;top:10px;
background-image:linear-gradient(to bottom,#DCAB4F 0,#cb9b51 22%,#f6e27a 45%,#f6f2c0 50%,#f6e27a 55%,#cb9b51 78%,#DCAB4F 100%);
color:transparent;-webkit-background-clip:text;">
         <strong>
          ICRA@40
         </strong>
        </span>
        <span style="font-size: 20px; font-family:Arial,sans serif;text-align:left;position:absolute;left:550px;top:19px;
color:#FFF59A;">
         September 23-26, 2024
         <br/>
         Rotterdam, The Netherlands
        </span>
        <span style="font-size: 24px; font-family:Arial,sans serif;text-align:left;position:absolute;left:250px;top:80px;
color:#FFF59A;">
         <strong>
          40th Anniversary, IEEE International Conference on
          <br/>
          Robotics and Automation
         </strong>
        </span>
        <img alt="" border="0" height="140" src="/images/icra/icraa40r.webp" style="position:absolute;right:0px;top:0px;"/>
       </td>
      </tr>
     </table>
     <table border="0" cellpadding="0" cellspacing="0" height="80%" width="100%">
      <tr>
       <td height="100%" style="background-color:#5B2999;" width="5">
       </td>
       <td width="5">
       </td>
       <td height="100%" valign="top" width="100%">
        <br/>
        <div class="c" id="TheTop">
         <span id="pTitle">
          <a href="http://icra40.ieee.org" target="_blank">
           <b>
            40th Anniversary of the IEEE Conference on Robotics and Automation (ICRA@40)
           </b>
          </a>
          <br/>
         </span>
         <span id="pSubTitle">
          <b>
           September 23-26, 2024, Rotterdam, Netherlands
          </b>
         </span>
         <br/>
         <br/>
        </div>
        <div class="c" style="position: relative">
         <a href="ICRAX24_ProgramAtAGlanceWeb.html">
          Program at a Glance
         </a>
         <a href="ICRAX24_ContentListWeb_1.html">
          Monday
         </a>
         <a href="ICRAX24_ContentListWeb_2.html">
          Tuesday
         </a>
         <a href="ICRAX24_ContentListWeb_3.html">
          Wednesday
         </a>
         <a href="ICRAX24_ContentListWeb_4.html">
          Thursday
         </a>
         <a href="ICRAX24_AuthorIndexWeb.html">
          Author Index
         </a>
         <a href="ICRAX24_KeywordIndexWeb.html">
          Keyword Index
         </a>
         <a href="#TheTop" onclick="window.open('https://ras.papercept.net/conferences/scripts/myprogram.pl?ConfID=467','myprogrampage')">
          My Program
         </a>
        </div>
        <div class="c">
         <p style="color: gray">
          Last updated on September 30, 2024. This conference program is tentative and subject to change
         </p>
        </div>
        <div class="c">
         <h3>
          Technical Program for Thursday September 26, 2024
         </h3>
        </div>
        <p class="c">
        </p>
        <div class="c">
         <span style="color:gray ">
          To show or hide the keywords and abstract (text summary) of a paper (if available), click on the paper title
         </span>
         <br/>
         <a href="javascript:openAllAbstracts()" title="Click to open all abstracts">
          Open all abstracts
         </a>
         <a href="javascript:closeAllAbstracts()" title="Click to close all abstracts">
          Close all abstracts
         </a>
        </div>
        <div class="c">
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thk1n">
             <b>
              ThK1N
             </b>
             RTM Stage
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod410" name="modifyThK1N" onclick="modsession(67,410)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thk1n" title="Click to go to the Program at a Glance">
             <b>
              Keynote Session 10
              <br/>
              <br/>
              Dong-Soo Kwon – Kevin Lynch – Michael Yu Wang –
              <br/>
              Jing Xiao – Maria Gini – Thomas C Henderson – Abderrahmane Kheddar –
              <br/>
              Eiichi Yoshida
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="tham_br">
             <b>
              ThAM_BR
             </b>
             Rotterdam + Port
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod411" name="modifyThAM_BR" onclick="modsession(71,411)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#tham_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break 7
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thk2n">
             <b>
              ThK2N
             </b>
             RTM Stage
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod412" name="modifyThK2N" onclick="modsession(73,412)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thk2n" title="Click to go to the Program at a Glance">
             <b>
              Keynote Session 11
              <br/>
              <br/>
              Steven LaValle – Zexiang Li – Dario Floreano –
              <br/>
              Anibal Ollero – Soon-Jo Chung – Antonio Franchi – Marco Pavone –
              <br/>
              Georgia Chalvatzaki
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thint1s">
             <b>
              ThINT1S
             </b>
             Rotterdam + Port
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod413" name="modifyThINT1S" onclick="modsession(75,413)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thint1s" title="Click to go to the Program at a Glance">
             <b>
              Interactive Session 7
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_01">
             12:30-13:15, Paper ThINT1S.1
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod414" name="modify5" onclick="modify(5,414)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('5'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enabling Versatility and Dexterity of the Dual-Arm Manipulators: A General Framework Toward Universal Cooperative Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#201808" title="Click to go to the Author Index">
             Ren, Yi
            </a>
           </td>
           <td class="r">
            Huawei Technologies
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#250367" title="Click to go to the Author Index">
             Zhou, Zhehua
            </a>
           </td>
           <td class="r">
            University of Alberta
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#305322" title="Click to go to the Author Index">
             Xu, Ziwei
            </a>
           </td>
           <td class="r">
            Technische Universität München
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#181709" title="Click to go to the Author Index">
             Yang, Yang
            </a>
           </td>
           <td class="r">
            Nanjing University of Information Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#245079" title="Click to go to the Author Index">
             Zhai, Guangyao
            </a>
           </td>
           <td class="r">
            Technical University of Munich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#115801" title="Click to go to the Author Index">
             Leibold, Marion
            </a>
           </td>
           <td class="r">
            Technische Universität München
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#206595" title="Click to go to the Author Index">
             Ni, Fenglei
            </a>
           </td>
           <td class="r">
            State Key Laboratory of Robotics and System, Harbin Institute Of
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#276978" title="Click to go to the Author Index">
             Zhang, Zhengyou
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#10005" title="Click to go to the Author Index">
             Buss, Martin
            </a>
           </td>
           <td class="r">
            Technische Universität München
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#115743" title="Click to go to the Author Index">
             Zheng, Yu
            </a>
           </td>
           <td class="r">
            Tencent
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab5" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#dual_arm_manipulation" title="Click to go to the Keyword Index">
               Dual Arm Manipulation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_in_robotics_and_automation" title="Click to go to the Keyword Index">
               Deep Learning in Robotics and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this work, we propose a general framework for dual-arm manipulators that contains two correlative modules. The learning-based dexterity-reachability-aware perception module deals with vision-based bimanual grasping. It employs an end-to-end evaluation network and probabilistic modeling of the robot's reachability to deliver feasible and dexterity-optimum grasp pairs for unseen objects. The optimization-based versatility-oriented control module addresses the online cooperative manipulation control by using a hierarchical quadratic programming formulation. Self-collision avoidance and dual-arm manipulability ellipsoid tracking with high reliability and fidelity are simultaneously achieved based on a learned lightweight distance proxy function and a speed-level tracking technique on Riemannian Manifold. Intrinsic system safety is guaranteed, and a novel interface for skill transfer is enabled. A long-horizon rearrangement experiment, a bimanual turnover manipulation, and a comparative performance evaluation verify the effectiveness of the proposed framework.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_02">
             12:30-13:15, Paper ThINT1S.2
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod415" name="modify7" onclick="modify(7,415)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('7'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Persistent Homology Meets Object Unity: Object Recognition in Clutter
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#241719" title="Click to go to the Author Index">
             Samani, Ekta
            </a>
           </td>
           <td class="r">
            Amazon.com
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#118318" title="Click to go to the Author Index">
             Banerjee, Ashis
            </a>
           </td>
           <td class="r">
            University of Washington
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab7" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#topological_learning" title="Click to go to the Keyword Index">
               Topological Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recognition of occluded objects in unseen and unstructured indoor environments is a challenging problem for mobile robots. To address this challenge, we propose a new descriptor, TOPS, for point clouds generated from depth images and an accompanying recognition framework, THOR, inspired by human reasoning. The descriptor employs a novel slicing-based approach to compute topological features from filtrations of simplicial complexes using persistent homology, and facilitates reasoning-based recognition using object unity. Apart from a benchmark dataset, we report performance on a new dataset, the UW Indoor Scenes (UW-IS) Occluded dataset, curated using commodity hardware to reflect real-world scenarios with different environmental conditions and degrees of object occlusion. THOR outperforms state-of-the-art methods on both the datasets and achieves substantially higher recognition accuracy for all the scenarios of the UW-IS Occluded dataset. Therefore, THOR, is a promising step toward robust recognition in low-cost robots, meant for everyday use in indoor settings.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_03">
             12:30-13:15, Paper ThINT1S.3
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod416" name="modify8" onclick="modify(8,416)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('8'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Bio-Inspired Deformable Mouthpart Device with Adaptive Control for Negative Pressure Therapy on Unstructured Limb Surfaces
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#368173" title="Click to go to the Author Index">
             Li, Zihao
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#334759" title="Click to go to the Author Index">
             Nie, Zhenguo
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#192649" title="Click to go to the Author Index">
             Zhao, Huichan
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#287659" title="Click to go to the Author Index">
             Shao, Qi
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#141100" title="Click to go to the Author Index">
             Xie, Fugui
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#121288" title="Click to go to the Author Index">
             Liu, Xin-Jun
            </a>
           </td>
           <td class="r">
            Tsinghua University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab8" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Negative pressure (NP) therapy is effective in managing chronic lymphedema of the extremities. However, the seal formation of the general head (GH) can fail due to the interspace between the lip and the irregular skin surface on limbs before suction, resulting in the inefficiency of therapy, prolonging the time required for physiotherapy, and increasing the workload of physiotherapists. In this letter, we present a bio-inspired head (BIH) that uses adaptive control for NP therapy on unstructured surfaces. Its lip is designed with soft material inspired by the bloodworm to ensure safe human contact. Additionally, the lip size can be changed with the variation of the cavity pressure, and sensors on the lip could provide contact feedback. An adaptive control method is proposed for autonomous suction on unstructured surfaces. Several experiments were carried out to illustrate the characteristics of this novel head with a deformable lip and contact detection, while the adaptive control method for the NP suction on the unstructured surfaces was validated by phantom and body experiments. Compared with the GH, the results showed that the BIH with adaptive control could automatically fit the irregular arm structures to form a seal and complete the NP suction with a higher success rate(SR).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_04">
             12:30-13:15, Paper ThINT1S.4
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod417" name="modify27" onclick="modify(27,417)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('27'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Self-Supervised Traversability with Navigation Experiences of Mobile Robots: A Risk-Aware Self-Training Approach
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#302725" title="Click to go to the Author Index">
             Cho, Ikhyeon
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#104838" title="Click to go to the Author Index">
             Chung, Woojin
            </a>
           </td>
           <td class="r">
            Korea University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab27" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile robots operating in outdoor environments face the challenge of navigating various terrains with different degrees of difficulty. Therefore, traversability estimation is crucial for safe and efficient robot navigation. Current approaches utilize a robot's driving experience to learn traversability in a self-supervised fashion. However, providing sufficient and diverse experience to the robot is difficult in many practical applications.
             <p>
              In this paper, we propose a self-supervised traversability learning method that adapts to challenging terrains with limited prior experience. One key aspect is to enable prioritized learning of scarce yet high-risk terrains by using a risk-sensitive approach. To this end, we train a neural network through a risk-aware instance weighting scheme. Another key aspect is to leverage traversability pseudo-labels on the basis of a self-training scheme. The proposed confidence-regularized self-training generates high-quality pseudo-labels, thereby achieving reliable data augmentation for unexperienced terrains. The effectiveness of the proposed method is verified in extensive real-world experiments, ranging from structured urban environments to complex rugged terrains.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_05">
             12:30-13:15, Paper ThINT1S.5
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod418" name="modify39" onclick="modify(39,418)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('39'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Investigating Haptic Feedback in Vision - Deficient Millirobot Telemanipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#304508" title="Click to go to the Author Index">
             Riaziat, Naveed Dennis
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#204280" title="Click to go to the Author Index">
             Erin, Onder
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#119227" title="Click to go to the Author Index">
             Krieger, Axel
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#144181" title="Click to go to the Author Index">
             Brown, Jeremy DeLaine
            </a>
           </td>
           <td class="r">
            Johns Hopkins University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab39" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The evolution of magnetically actuated millirobots gives rise to unique teleoperation challenges due to their non-traditional kinematic and dynamic architectures, as well as their frequent use of suboptimal imaging modalities. Recent investigations into haptic interfaces for millirobots have shown promise but lack the clinically motivated task scenarios necessary to justify future development. In this work, we investigate the utility of haptic feedback on bilateral teleoperation of a magnetically actuated millirobot in visually deficient conditions. We conducted an N=23 user study in an aneurysm coiling inspired procedure, which required participants to navigate the robot through a maze in near total darkness to manipulate beads to a target under simulated fluoroscopy. We hypothesized that users will be better able to complete the telemanipulation task with haptic feedback while reducing excess forces on their surroundings compared to the no feedback conditions. Our results showed an over 40% improvement in participants' bead scoring, a nearly 10% reduction in mean force, and 13% reduction in maximum force with haptic feedback, as well as significant improvements in other metrics. Results highlight that benefits of haptic feedback are retained when haptic feedback is removed. These findings suggest that haptic feedback has the potential to significantly improve millirobot telemanipulation and control in traditionally vision deficient tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_06">
             12:30-13:15, Paper ThINT1S.6
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod419" name="modify91" onclick="modify(91,419)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('91'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              High-Resolution Mapping of Underwater Assets Using a Laser Scanner
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#335405" title="Click to go to the Author Index">
             Jung, Kyungmin
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#346925" title="Click to go to the Author Index">
             Del Castillo Bernal, Arturo
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#275560" title="Click to go to the Author Index">
             Hitchcox, Thomas
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#407221" title="Click to go to the Author Index">
             Wicks, Ryan
            </a>
           </td>
           <td class="r">
            Voyis Imaging
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#140525" title="Click to go to the Author Index">
             Forbes, James Richard
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab91" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#mapping" title="Click to go to the Keyword Index">
               Mapping
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             There is an increasing need for the application of unmanned underwater vehicles (UVs) in challenging subsea environments. The purpose of this submission is to highlight recent advances in underwater simultaneous localization and mapping (SLAM), including laser-based and vision-based approaches, which together demonstrate the potential of these sensors to enable long-duration underwater navigation and mapping. This talk will include results from challenging field datasets, including the wreck of Earnest Shackleton's Endurance discovered in Antarctica in March 2022.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_07">
             12:30-13:15, Paper ThINT1S.7
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod420" name="modify97" onclick="modify(97,420)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('97'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Programmatic Imitation Learning from Unlabeled and Noisy Demonstrations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#353089" title="Click to go to the Author Index">
             Xin, Jimmy
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#355329" title="Click to go to the Author Index">
             Zheng, Linus
            </a>
           </td>
           <td class="r">
            University of Texas, Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#350065" title="Click to go to the Author Index">
             Rahmani, Kia
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#298792" title="Click to go to the Author Index">
             Wei, Jiayi
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#195409" title="Click to go to the Author Index">
             Holtz, Jarrett
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#298412" title="Click to go to the Author Index">
             Dillig, Isil
            </a>
           </td>
           <td class="r">
            UT Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#127043" title="Click to go to the Author Index">
             Biswas, Joydeep
            </a>
           </td>
           <td class="r">
            University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab97" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation Learning (IL) is a promising paradigm for teaching robots to perform novel tasks using demonstrations. Most existing approaches for IL utilize neural networks (NN), however, these methods suffer from several well-known limitations: they 1) require large amounts of training data, 2) are hard to interpret, and 3) are hard to refine and adapt. There is an emerging interest in programmatic imitation learning
             <p>
              (PIL), which offers significant promise in addressing the above limitations. In PIL, the learned policy is represented in a programming
              <p>
               language, making it amenable to interpretation and adaptation to novel settings. However, state-of-the-art PIL algorithms assume access to action labels and struggle to learn from noisy real- world demonstrations. In this paper,
               <p>
                we propose PLUNDER, a novel PIL algorithm that integrates a probabilistic program synthesizer in an iterative Expectation-Maximization (EM) framework to address these shortcomings. Unlike existing PIL approaches, PLUNDER synthesizes probabilistic programmatic policies that are particularly well-suited for modeling the uncertainties inherent in real-world demonstrations. Our approach leverages an EM loop to simultaneously infer the missing action labels and the most likely probabilistic policy. We benchmark PLUNDER against several established IL techniques, and demonstrate its superiority across five challenging imitation learning tasks under noise. PLUNDER policies achieve 95% accuracy in matching the given demonstrations, outperforming the next best baseline by 19%. Additionally, policies generated by PLUNDER successfully complete the tasks 17% more frequently than the nearest baseline.
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_08">
             12:30-13:15, Paper ThINT1S.8
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod421" name="modify100" onclick="modify(100,421)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('100'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BonnBot-I Plus: A Bio-Diversity Aware Precise Weed Management Robotic Platform
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#266140" title="Click to go to the Author Index">
             Ahmadi, Alireza
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#217748" title="Click to go to the Author Index">
             Halstead, Michael Allan
            </a>
           </td>
           <td class="r">
            Bonn University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#279508" title="Click to go to the Author Index">
             Smitt, Claus
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#191378" title="Click to go to the Author Index">
             McCool, Christopher Steven
            </a>
           </td>
           <td class="r">
            University of Bonn
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab100" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this article, we focus on the critical tasks of plant protection in arable farms, addressing a modern challenge in agriculture: integrating ecological considerations into the operational strategy of precision weeding robots like BonnBot-I. This article presents the recent advancements in weed management algorithms and the real-world performance of BonnBot-I at the University of Bonn's Klein-Altendorf campus. We present a novel Rolling-view observation model for the BonnBot-Is weed monitoring section which leads to an average absolute weeding performance enhancement of 3.4%. Furthermore, for the first time, we show how precision weeding robots could consider bio-diversity-aware concerns in challenging weeding scenarios. We carried out comprehensive weeding experiments in sugar-beet fields, covering both weed-only and mixed crop-weed situations, and introduced a new dataset compatible with precision weeding. Our real-field experiments revealed that our weeding approach is capable of handling diverse weed distributions, with a minimal loss of only 11.66% attributable to intervention planning and 14.7% to vision system limitations highlighting required improvements of the vision system.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_09">
             12:30-13:15, Paper ThINT1S.9
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod422" name="modify119" onclick="modify(119,422)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('119'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Machine-Learning-Assisted Soft Fiber Optic Glove System for Sign Language Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#296824" title="Click to go to the Author Index">
             Zhu, Renjie
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#352811" title="Click to go to the Author Index">
             Fan, Dongliang
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#368653" title="Click to go to the Author Index">
             Lin, Jiang
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#240350" title="Click to go to the Author Index">
             Feng, Huijuan
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#152964" title="Click to go to the Author Index">
             WANG, Hongqiang
            </a>
           </td>
           <td class="r">
            Southern University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107733" title="Click to go to the Author Index">
             Dai, Jian
            </a>
           </td>
           <td class="r">
            School of Natural and Mathematical Sciences, King's College Lond
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab119" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#wearable_robotics" title="Click to go to the Keyword Index">
               Wearable Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#gesture__posture_and_facial_expressions" title="Click to go to the Keyword Index">
               Gesture, Posture and Facial Expressions
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Sign language recognition devices are effective approaches to breaking the communication barrier between signers and non-signers and exploring human-machine interactions. Wearable gloves have been developed for gesture recognition and virtual reality applications by employing flexible sensors for motion detection and machine learning for data analysis. However, most existing wearable devices present limited sign language translating capacity due to the sensors’ design and distribution. Here, we propose a cost-effective dual-hand soft fiber optic glove system consisting of multimode soft liquid-core fiber optic sensors, gyroscopes, wireless printed circuit boards, and batteries for sign language translation. In combination with machine learning technology, static and dynamic hand gestures of American Sign Language can be recognized by this glove system. The soft glove system exhibits a broad sign language range (10 numbers, 26 alphabets, 18 words, and 5 sentences), high recognition accuracy (98.6% for static gestures, 95% for dynamic gestures). Finally, we demonstrate its application for controlling the motion of a robot through hand gestures in the VR interface.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_10">
             12:30-13:15, Paper ThINT1S.10
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod423" name="modify121" onclick="modify(121,423)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('121'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Energy-Based Domain-Adaptive Segmentation with Depth Guidance
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#376696" title="Click to go to the Author Index">
             Zhu, Jinjing
            </a>
           </td>
           <td class="r">
            HKUST(GZ)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#388350" title="Click to go to the Author Index">
             Hu, Zhedong
            </a>
           </td>
           <td class="r">
            NCEPU,
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#147620" title="Click to go to the Author Index">
             Kim, Tae-Kyun
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#256235" title="Click to go to the Author Index">
             Wang, Lin
            </a>
           </td>
           <td class="r">
            HKUST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab121" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Recent endeavors have been made to leverage self-supervised depth estimation as guidance in unsupervised domain adaptation (UDA) for semantic segmentation. Prior arts, however, overlook the discrepancy between semantic and depth features, as well as the reliability of feature fusion, thus leading to suboptimal segmentation performance. To address this issue, we propose a novel UDA framework called SMART (croSs doMain semAntic segmentation based on eneRgy esTimation) that utilizes Energy-Based Models (EBMs) to obtain task-adaptive features and achieve reliable feature fusion for semantic segmentation with self-supervised depth estimation. Our framework incorporates two novel components: energy-based feature fusion (EB2F) and energy-based reliable fusion Assessment (RFA) modules. The EB2F module produces task-adaptive semantic and depth features by explicitly measuring and reducing their discrepancy using Hopfield energy for better feature fusion. The RFA module evaluates the reliability of the feature fusion using an energy score to improve the effectiveness of depth guidance. Extensive experiments on two datasets demonstrate that our method achieves significant performance gains over prior works, validating the effectiveness of our energy-based learning approach.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_11">
             12:30-13:15, Paper ThINT1S.11
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod424" name="modify127" onclick="modify(127,424)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('127'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MotionBEV: Attention-Aware Online LiDAR Moving Object Segmentation with Bird's Eye View Based Appearance and Motion Features
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#132076" title="Click to go to the Author Index">
             Zhou, Bo
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#362852" title="Click to go to the Author Index">
             Xie, Jiapeng
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#363003" title="Click to go to the Author Index">
             Pan, Yan
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#363000" title="Click to go to the Author Index">
             Wu, Jiajie
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#363001" title="Click to go to the Author Index">
             Lu, Chuanzhao
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab127" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Identifying moving objects is an essential capability for autonomous systems, as it provides critical information for pose estimation, navigation, collision avoidance, and static map construction. In this paper, we present MotionBEV, a fast and accurate framework for LiDAR moving object segmentation, which segments moving objects with appearance and motion features in the bird's eye view (BEV) domain. Our approach converts 3D LiDAR scans into a 2D polar BEV representation to improve computational efficiency. Specifically, we learn appearance features with a simplified PointNet and compute motion features through the height differences of consecutive frames of point clouds projected onto vertical columns in the polar BEV coordinate system. We employ a dual-branch network bridged by the Appearance-Motion Co-attention Module (AMCM) to adaptively fuse the spatio-temporal information from appearance and motion features. Our approach achieves state-of-the-art performance on the SemanticKITTI-MOS benchmark. Furthermore, to demonstrate the practical effectiveness of our method, we provide a LiDAR-MOS dataset recorded by a solid-state LiDAR, which features non-repetitive scanning patterns and a small field of view.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_12">
             12:30-13:15, Paper ThINT1S.12
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod425" name="modify128" onclick="modify(128,425)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('128'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Grasp in Cluttered Scene Using a Multi-Stage Deep Learning Model
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#386236" title="Click to go to the Author Index">
             Wei, Dujia
            </a>
           </td>
           <td class="r">
            Shenzhen Technology University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#386237" title="Click to go to the Author Index">
             Cao, Jianmin
            </a>
           </td>
           <td class="r">
            Shenzhen Technology University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#147033" title="Click to go to the Author Index">
             Gu, Ye
            </a>
           </td>
           <td class="r">
            Shenzhen Technology University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab128" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#perception_for_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Perception for Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Object grasping in cluttered scene is a practical robotic skill which has a wide range of applications. In this paper, we propose a novel maximum graspness metric which can help extract high-quality scene grasp points effectively. The graspness scores of a single-view point cloud are generated using the proposed interpolation approach. The graspness model is implemented using a compact encoder-decoder model which takes a depth image as input. On the other hand, the grasp point features are extracted. They are further grouped and sampled to predict approaching vectors and in-plane rotations of the grasp poses using residual point blocks. The proposed model is evaluated using a large scale benchmark GraspNet-1Billion dataset and can outperform prior state-ofthe-art method by a margin (+4.91 AP) on all camera types. Through real-world cluttered scenario testing, our approach achieves grasping successful rate of 89.60% using a UR-5 robotic arm and a RealSense camera.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_13">
             12:30-13:15, Paper ThINT1S.13
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod426" name="modify136" onclick="modify(136,426)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('136'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NeuralFloors: Conditional Street-Level Scene Generation from BEV Semantic Maps Via Neural Fields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#302584" title="Click to go to the Author Index">
             Musat, Valentina
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#196040" title="Click to go to the Author Index">
             De Martini, Daniele
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#179163" title="Click to go to the Author Index">
             Gadd, Matthew
            </a>
           </td>
           <td class="r">
            University of Oxford
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105828" title="Click to go to the Author Index">
             Newman, Paul
            </a>
           </td>
           <td class="r">
            Oxford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab136" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#computer_vision_for_transportation" title="Click to go to the Keyword Index">
               Computer Vision for Transportation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Semantic Bird’s Eye View (BEV) representations are a popular format, being easily interpretable and editable. However, synthesising ground-view images from BEVs is a difficult task as the system would need to learn both the mapping from BEV to Front View (FV) structure as well as to synthesise highly photo-realistic imagery, thus having to simultaneously consider both the geometry and appearance of the scene. We therefore present a factorised approach that tackles the problem in two stages: a first stage learns a BEV to FV transformation in the semantic space through a Neural Field, and a second stage leverages a Latent Diffusion Model (LDM) to synthesise images conditional on the output of the first stage. Our experiments show that this approach produces RGB images with a high perceptual quality that are also well aligned with their corresponding FV ground-truth.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_14">
             12:30-13:15, Paper ThINT1S.14
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod427" name="modify141" onclick="modify(141,427)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('141'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Vine Robots with Magnetic Skin for Surgical Navigations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#273299" title="Click to go to the Author Index">
             Davy, Joshua
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#334261" title="Click to go to the Author Index">
             Greenidge, Nikita Jasmine
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#271848" title="Click to go to the Author Index">
             Kim, Sukjun
            </a>
           </td>
           <td class="r">
            University of California, San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#391443" title="Click to go to the Author Index">
             Tinsley, Luke J.
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#270886" title="Click to go to the Author Index">
             Lloyd, Peter Robert
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#223931" title="Click to go to the Author Index">
             Chandler, James Henry
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#239954" title="Click to go to the Author Index">
             Harris, Russell
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#187439" title="Click to go to the Author Index">
             Morimoto, Tania K.
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101892" title="Click to go to the Author Index">
             Valdastri, Pietro
            </a>
           </td>
           <td class="r">
            University of Leeds
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab141" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Drawing inspiration from natural growth and movement strategies, vine robots possess exceptional passive shape-forming abilities, enabling them to deform around obstacles. However, an intrinsic lack of active steering limits their capacity to control their growth trajectory. This paper considers the external manipulation of such robots through the utilization of magnetically active materials embedded within the vine robot's skin. This results in a completely flexible, steerable, growing structure that can be actuated via the application of external magnetic fields and field gradients. We explore the principles of magnetically-guided vine robots and provide empirical evidence highlighting the efficacy of our proposed magnetic steering methodology. Due to the inverted internal structure, careful design of the magnetization direction of the robot has to be considered. Our design proposes an orthogonal magnetization strategy which successfully preserves a net positive magnetization. Our focus centers on a vine robot of 8 mm diameter, constructed from a polyethylene substrate coated with a silicone layer embedded with magnetic micro-particles. We demonstrate the ability of our robots to navigate complex environments and steer around large obstacles in a shear free manner via the simultaneous control of both the magnetic field and the growing pressure. Finally, we demonstrate our robot by performing the selective navigation of multiple bifurcations within a bronchial tree phantom.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_15">
             12:30-13:15, Paper ThINT1S.15
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod428" name="modify143" onclick="modify(143,428)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('143'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Domain Adaptive Detection of MAVs: A Benchmark and Noise Suppression Network (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#309349" title="Click to go to the Author Index">
             Zhang, Yin
            </a>
           </td>
           <td class="r">
            WestLake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#407547" title="Click to go to the Author Index">
             Deng, Jinhong
            </a>
           </td>
           <td class="r">
            The University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#291643" title="Click to go to the Author Index">
             Li, Wen
            </a>
           </td>
           <td class="r">
            University of Electronic Science and Technology of China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#171584" title="Click to go to the Author Index">
             Liu, Peidong
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#193378" title="Click to go to the Author Index">
             Zhao, Shiyu
            </a>
           </td>
           <td class="r">
            Westlake University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab143" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#aerial_systems__perception_and_autonomy" title="Click to go to the Keyword Index">
               Aerial Systems: Perception and Autonomy
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Visual detection of Micro Air Vehicles (MAVs) has attracted increasing attention in recent years due to its important application in various tasks. The existing methods for MAV detection assume that the training set and testing set have the same distribution. As a result, when deployed in new domains, the detectors would have a significant performance degradation due to domain discrepancy. In this paper, we study the problem of cross-domain MAV detection. The contributions of this paper are threefold. 1) We propose a Multi-MAV-Multi-Domain (M3D) dataset consisting of both simulation and realistic images. Compared to other existing datasets, the proposed one is more comprehensive in the sense that it covers rich scenes, diverse MAV types, and various viewing angles. A new benchmark for cross-domain MAV detection is proposed based on the proposed dataset. 2) We propose a Noise Suppression Network (NSN) based on the framework of pseudo-labeling and a large-to-small training procedure. To reduce the challenging pseudo-label noises, two novel modules are designed in this network. The first is a prior-based curriculum learning module for allocating adaptive thresholds for pseudo labels with different difficulties. The second is a masked copy-paste augmentation module for pasting truly-labeled MAVs on unlabeled target images and thus decreasing pseudo-label noises. 3) Extensive experimental results verify the superior performance of the proposed method compared to the state-of-the-art ones. In particular, it achieves mAP of 46.9%(+5.8%), 50.5%(+3.7%), and 61.5%(+11.3%) on the tasks of simulation-to-real adaptation, cross-scene adaptation, and cross-camera adaptation, respectively. The dataset is available at: https://github.com/WestlakeAerialRobotics/M3D.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_16">
             12:30-13:15, Paper ThINT1S.16
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod429" name="modify152" onclick="modify(152,429)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('152'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Aligning Human Intent from Imperfect Demonstrations with Confidence-Based Inverse Soft-Q Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#382868" title="Click to go to the Author Index">
             Bu, Xizhou
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#383861" title="Click to go to the Author Index">
             Li, Wenjuan
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#214164" title="Click to go to the Author Index">
             Liu, Zhengxiong
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#252433" title="Click to go to the Author Index">
             Ma, Zhiqiang
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#102722" title="Click to go to the Author Index">
             Huang, Panfeng
            </a>
           </td>
           <td class="r">
            Northwestern Polytechnical University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab152" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation learning attracts much attention for its ability to allow robots to quickly learn human manipulation skills through demonstrations. However, in the real world, human demonstrations often exhibit random behavior that is not intended by humans. Collecting high-quality human datasets is both challenging and expensive. Consequently, robots need to have the ability to learn behavioral policies that align with human intent from imperfect demonstrations. Previous work uses confidence scores to extract useful information from imperfect demonstrations, which relies on access to ground truth rewards or active human supervision. In this paper, we propose a transition-based method to obtain fine-grained confidence scores for data without the above efforts, which can increase the success rate of the baseline algorithm by 40.3% on average. We develop a generalized confidence-based imitation learning framework for guiding policy learning, called Confidence-based Inverse soft-Q Learning (CIQL), as shown in Fig.1. Based on this, we analyze two ways of processing noise and find that penalization is more aligned with human intent than filtering.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_17">
             12:30-13:15, Paper ThINT1S.17
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod430" name="modify156" onclick="modify(156,430)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('156'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Indoor and Outdoor 3D Scene Graph Generation Via Language-Enabled Spatial Ontologies
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#217509" title="Click to go to the Author Index">
             Strader, Jared
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#219275" title="Click to go to the Author Index">
             Hughes, Nathan
            </a>
           </td>
           <td class="r">
            MIT
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#339147" title="Click to go to the Author Index">
             Chen, William
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#238562" title="Click to go to the Author Index">
             Speranzon, Alberto
            </a>
           </td>
           <td class="r">
            Lockheed Martin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#119426" title="Click to go to the Author Index">
             Carlone, Luca
            </a>
           </td>
           <td class="r">
            Massachusetts Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab156" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes an approach to build 3D scene graphs in arbitrary indoor and outdoor environments. Such extension is challenging; the hierarchy of concepts that describe an outdoor environment is more complex than for indoors, and manually defining such hierarchy is time-consuming and does not scale. Furthermore, the lack of training data prevents the straightforward application of learning-based tools used in indoor settings. To address these challenges, we propose two novel extensions. First, we develop methods to build a spatial ontology defining concepts and relations relevant for indoor and outdoor robot operation. In particular, we use a Large Language Model (LLM) to build such an ontology, thus largely reducing the amount of manual effort required. Second, we leverage the spatial ontology for 3D scene graph construction using Logic Tensor Networks (LTN) to add logical rules, or axioms (e.g., “a beach contains sand”), which provide additional supervisory signals at training time thus reducing the need for labelled data, providing better predictions, and even allowing predicting concepts unseen at training time. We test our approach in a variety of datasets, including indoor, rural, and coastal environments, and show that it leads to a significant increase in the quality of the 3D scene graph generation with sparsely annotated data.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_18">
             12:30-13:15, Paper ThINT1S.18
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod431" name="modify162" onclick="modify(162,431)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('162'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              RoboBall Recap: Past, Current, and Future Inflatable Spherical Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#340796" title="Click to go to the Author Index">
             Jangale, Rishi
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#340728" title="Click to go to the Author Index">
             Oevermann, Micah
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#340798" title="Click to go to the Author Index">
             Jibrail, Joseph Garrett
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#378213" title="Click to go to the Author Index">
             Pravecek, Derek
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#340759" title="Click to go to the Author Index">
             Dravid, Meghali Prashant
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#413146" title="Click to go to the Author Index">
             Villanueva, Aaron
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#337083" title="Click to go to the Author Index">
             Ambrose, Robert
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab162" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This extended abstract describes the evolution of the spherical, soft robot series RoboBall. Starting with RoboBall I in 2003, this abstract iterates the learnings, changes, and goals of each robot up till RoboBall III in 2024. Potential applications and mission scopes are discussed, along with future work on this robot series.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_19">
             12:30-13:15, Paper ThINT1S.19
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod432" name="modify171" onclick="modify(171,432)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('171'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Commercialization of Robot Navigation Technology for Innovation for a Better Life
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105418" title="Click to go to the Author Index">
             Kim, Hyoung-Rock
            </a>
           </td>
           <td class="r">
            LG Electronics Co. Advanced Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#151640" title="Click to go to the Author Index">
             Noh, DongKi
            </a>
           </td>
           <td class="r">
            LG Electronics Inc
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#104112" title="Click to go to the Author Index">
             Baek, Seung-Min
            </a>
           </td>
           <td class="r">
            LG Electronics
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab171" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#engineering_for_robotic_systems" title="Click to go to the Keyword Index">
               Engineering for Robotic Systems
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#domestic_robotics" title="Click to go to the Keyword Index">
               Domestic Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#service_robotics" title="Click to go to the Keyword Index">
               Service Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             LG Electronics’ journey to commercialize robot navigation technology in various areas such as home, public spaces, and factories will be introduced in this paper. Technical challenges ahead in robot navigation to make an innovation for our better life will be discussed. With the vision on ‘Zero Labor Home’, the next smart home agent robot will bring us next innovation in our lives with the advances of spatial AI, i.e. combination of robot navigation and AI technology.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_20">
             12:30-13:15, Paper ThINT1S.20
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod433" name="modify175" onclick="modify(175,433)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('175'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Three-Dimensional Fluid-Directed Rigid Body Control on a Physical System
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#245867" title="Click to go to the Author Index">
             Teikmanis, Oskars
            </a>
           </td>
           <td class="r">
            Institute of Electronics and Computer Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410257" title="Click to go to the Author Index">
             Saltanovs, Rodions
            </a>
           </td>
           <td class="r">
            Institute of Electronics and Computer Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410251" title="Click to go to the Author Index">
             Freivalds, Karlis
            </a>
           </td>
           <td class="r">
            Institute of Electronics and Computer Science
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab175" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#engineering_for_robotic_systems" title="Click to go to the Keyword Index">
               Engineering for Robotic Systems
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#hydraulic_pneumatic_actuators" title="Click to go to the Keyword Index">
               Hydraulic/Pneumatic Actuators
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This article presents ongoing work on the "Flow Grid", a novel floor-standing device for fluid-directed, contactless manipulation of objects using air. It features a horizontal surface with 60 upward-facing nozzles connected to an air compressor and is controlled by a neural network. The design also integrates real-time depth camera measurements for the accurate tracking of objects across the device's surface. We describe key components and design decisions, providing insights into the device's intended use in evaluating machine learning algorithms for three-dimensional fluid-solid interactions, with potential applications in the non-prehensile manipulation of delicate objects.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_21">
             12:30-13:15, Paper ThINT1S.21
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod434" name="modify191" onclick="modify(191,434)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('191'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Precision Navigation for Robotic Inspections of Underground Gas Pipelines
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#210095" title="Click to go to the Author Index">
             van Teeffelen, Kees J.
            </a>
           </td>
           <td class="r">
            Saxion University of Applied Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#176401" title="Click to go to the Author Index">
             Sluiter, Victor IJzebrand
            </a>
           </td>
           <td class="r">
            Saxion University of Applied Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#142116" title="Click to go to the Author Index">
             Mersha, Abeje Yenehun
            </a>
           </td>
           <td class="r">
            Saxion University of Applied Science
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab191" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#autonomous_vehicle_navigation" title="Click to go to the Keyword Index">
               Autonomous Vehicle Navigation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_in_under_resourced_settings" title="Click to go to the Keyword Index">
               Robotics in Under-Resourced Settings
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work describes the design, implementation and validation of an autonomous gas leakage inspection robot. Navigation with centimeter level accuracy is achieved using RTK GNSS integrated using the ROS 2 and Nav2 frameworks. The proposed solution has been validated successfully in terms of navigation accuracy and gas detection capabilities. The approach has the potential to effectively address the increasing demand for inspections of the grid.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_23">
             12:30-13:15, Paper ThINT1S.23
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod435" name="modify210" onclick="modify(210,435)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Sample-Efficient Reinforcement Learning for Quadrotor Control through Curriculum Design and Training
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410488" title="Click to go to the Author Index">
             Lagos Suarez, Fausto Mauricio
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#341005" title="Click to go to the Author Index">
             Saradagi, Akshit
            </a>
           </td>
           <td class="r">
            Luleå University of Technology, Luleå, Sweden
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#243487" title="Click to go to the Author Index">
             Sumathy, Vidya
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#397977" title="Click to go to the Author Index">
             Kotpalliwar, Shruti
            </a>
           </td>
           <td class="r">
            Luleå Tekniska Universitet
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_24">
             12:30-13:15, Paper ThINT1S.24
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod436" name="modify215" onclick="modify(215,436)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('215'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Shortcomings of Force-From-Motion in Robot Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#291311" title="Click to go to the Author Index">
             Aljalbout, Elie
            </a>
           </td>
           <td class="r">
            Volkswagen AG
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#240056" title="Click to go to the Author Index">
             Frank, Felix
            </a>
           </td>
           <td class="r">
            Volkswagen Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107642" title="Click to go to the Author Index">
             van der Smagt, Patrick
            </a>
           </td>
           <td class="r">
            Volkswagen Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#151265" title="Click to go to the Author Index">
             Paraschos, Alexandros
            </a>
           </td>
           <td class="r">
            Volkswagen Group
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab215" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotic manipulation requires accurate motion and physical interaction control. However, current robot learning approaches focus on motion-centric action spaces that do not explicitly give the policy control over the interaction. In this paper, we discuss the repercussions of this choice and argue for more interaction-explicit action spaces in robot learning.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_25">
             12:30-13:15, Paper ThINT1S.25
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod437" name="modify216" onclick="modify(216,437)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('216'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Promoting Excellence in International Postgraduate Training in Field Robotics at the Universitat Jaume I (UJI)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#104058" title="Click to go to the Author Index">
             Sanz, Pedro J
            </a>
           </td>
           <td class="r">
            Jaume I
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410498" title="Click to go to the Author Index">
             Pino-Jarque, Andrea
            </a>
           </td>
           <td class="r">
            Jaume I
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410493" title="Click to go to the Author Index">
             López-Barajas, Salvador
            </a>
           </td>
           <td class="r">
            Jaume I
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410500" title="Click to go to the Author Index">
             Solis, Alejandro
            </a>
           </td>
           <td class="r">
            Universitat Jaume I
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#155066" title="Click to go to the Author Index">
             Marti, Jose Vicente
            </a>
           </td>
           <td class="r">
            Universitat Jaume I
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#103925" title="Click to go to the Author Index">
             Marin, Raul
            </a>
           </td>
           <td class="r">
            Jaume I University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab216" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#education_robotics" title="Click to go to the Keyword Index">
               Education Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Since its origin, IRS-Lab (UJI, Spain) has always been promoting excellence in robotics teaching, thanks to funding from the European Robotics Research Network (EURON, 2001-07) for the organization of international summer schools. The video presented focuses on the context of intelligent marine robotics (Erasmus Mundus Joint Master’s Degree MIR) [1]. And, specifically, in the study-track followed at UJI (i.e. underwater intervention missions). In short, the video is highlighting the main stages followed during the “MIR Robotics Challenge” (MRC) organized by our team (IRS-Lab) at UJI [2], with the intention of developing an interesting educational tool for the 1st year students (registered in UTLN, Toulon, France). All of them were later at UJI, during a week in June 2023 (MRC). So, the first stage was an offline training period, through the Unity simulator, including virtual robots, allowing students to familiarize themselves with the final real scenarios. In this context, the underwater effects were crucial, as well as the “Unity Robotics Hub” component, to allow interaction with the ROS system. Thanks to this training, the students were able to pass, during the last stage, the tests in real water tank conditions. The main value of this experience has been the high degree of motivation achieved by the students. Firstly, for the cooperative work within the teams and, furthermore, for the exciting work environment under realistic conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_26">
             12:30-13:15, Paper ThINT1S.26
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod438" name="modify247" onclick="modify(247,438)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('247'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BSP Control for Visual Pointing of Satellite Directional Antennas for Geostationary Earth Orbit Service Operations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#106001" title="Click to go to the Author Index">
             Bonsignorio, Fabio
            </a>
           </td>
           <td class="r">
            FER, University of Zagreb
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#108955" title="Click to go to the Author Index">
             Zereik, Enrica
            </a>
           </td>
           <td class="r">
            CNR - National Research Council
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab247" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#integrated_planning_and_control" title="Click to go to the Keyword Index">
               Integrated Planning and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present a novel approach for the visual driven pointing of satellite directional antennas based on a Belief Space Planning control (BSP). This work is part of a wider research program aiming at reducing the complexity and the cost of satellites dedicated to various tasks and to develop a system platform to manufacture them in orbit. BSP allows less accurate and cheaper antennas and satellite with respect to other control approaches. This will facilitate the 3D printing of their components and their assembly in orbit.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_27">
             12:30-13:15, Paper ThINT1S.27
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod439" name="modify248" onclick="modify(248,439)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('248'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data-Driven Hierarchical Open Set Recognition
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410584" title="Click to go to the Author Index">
             Hannum, Andrew
            </a>
           </td>
           <td class="r">
            University of Denver
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#368140" title="Click to go to the Author Index">
             Conway, Max
            </a>
           </td>
           <td class="r">
            University of Denver
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#179273" title="Click to go to the Author Index">
             Lopez, Mario Alberto
            </a>
           </td>
           <td class="r">
            University of Denver
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#385014" title="Click to go to the Author Index">
             Harrison, Andre
            </a>
           </td>
           <td class="r">
            U.S. Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab248" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Open set recognition (OSR) is crucial for robust perception in robotics and computer vision, especially as robots operate in dynamic real-world environments. This paper proposes a novel data-driven hierarchical approach to OSR that relates unknown class samples to known classes without requiring manually obtained relational information.
             <p>
              The method uses constrained agglomerative clustering to build a hierarchy of known classes in an embedding space. New samples are classified by finding the best matching node in this hierarchy. The approach is demonstrated on the Animals with Attributes 2 (AwA2) dataset, achieving an AUC ROC score of 0.82 and an average utility score of 0.85, comparable to state-of-the-art OSR methods.
              <p>
               The key advantages of this approach are: 1. It provides additional relational information for unknown classes. 2. It generates hierarchies automatically from training data, avoiding manual creation and potential human biases.
               <p>
                The paper introduces two classification approaches: score-based and traversal-based. It also proposes a new metric called Concentration Centrality (CC) to measure the consistency of hierarchical classification models.
                <p>
                 Evaluation metrics include standard accuracy measures (AUC ROC, F1 Score, etc.), a utility metric for known classes, and the new Class Concentration Centrality (CCC) metric for unseen classes. The CCC metric measures the consistency of unknown class placements in the hierarchy.
                 <p>
                  The proposed method does not outperform existing models in accuracy, but achieves comparable accuracy to existing state-of-the-art approaches while providing additional and consistent information on unknown classes. It does not require supplementary information beyond typical supervised model requirements, making it advantageous for data-driven applications.
                  <p>
                   Future work will focus on improving accuracy, further validating the Concentration Centrality metric, and running experiments following the Large-Scale Open-Set Classification Protocols for ImageNet.
                  </p>
                 </p>
                </p>
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_28">
             12:30-13:15, Paper ThINT1S.28
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod440" name="modify252" onclick="modify(252,440)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('252'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Autonomous Taking in the Laundry with Deep Predictive Learning Using Tactility, Dual Arms and Eyes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#285628" title="Click to go to the Author Index">
             Ichiwara, Hideyuki
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd. / Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#243493" title="Click to go to the Author Index">
             Ito, Hiroshi
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#202288" title="Click to go to the Author Index">
             Yamamoto, Kenjiro
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410030" title="Click to go to the Author Index">
             Noguchi, Naoaki
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab252" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#cognitive_control_architectures" title="Click to go to the Keyword Index">
               Cognitive Control Architectures
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#sensorimotor_learning" title="Click to go to the Keyword Index">
               Sensorimotor Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We presented a live demonstration of an end-to-end imitation learning method for the task of taking in the laundry at the 2024 IEEE International Conference on Robotics and Automation. Our approach is based on deep predictive learning with real-time multimodal prediction, leveraging a visual attention model. We extended the model to utilize bimanual manipulation, binocular vision, and tactile sensing. The model was trained using 16 demonstration data points collected within one hour, resulting in a single policy capable of retrieving laundry items of various types and at various positions. Additionally, the model performed the task in a timeframe comparable to that of a human taking in the laundry.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_29">
             12:30-13:15, Paper ThINT1S.29
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod441" name="modify256" onclick="modify(256,441)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Exploration and Scene Graph Construction in Unstructured Environments Using Structural Semantic Topometric Map
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#343828" title="Click to go to the Author Index">
             Fredriksson, Scott
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#340572" title="Click to go to the Author Index">
             Valdes Saucedo, Mario Alberto
            </a>
           </td>
           <td class="r">
            Lulea University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#341005" title="Click to go to the Author Index">
             Saradagi, Akshit
            </a>
           </td>
           <td class="r">
            Luleå University of Technology, Luleå, Sweden
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_30">
             12:30-13:15, Paper ThINT1S.30
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod442" name="modify261" onclick="modify(261,442)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('261'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Action Anticipation Using EEG Signals for Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#408297" title="Click to go to the Author Index">
             Vieira, Rodrigo
            </a>
           </td>
           <td class="r">
            University of Lisbon - Instituto Superior Técnico
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#127311" title="Click to go to the Author Index">
             Moreno, Plinio
            </a>
           </td>
           <td class="r">
            IST-ID
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#408298" title="Click to go to the Author Index">
             Vourvopoulos, Athanasios
            </a>
           </td>
           <td class="r">
            Instituto Superior Técnico (IST)/Universidade De Lisboa
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab261" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#intention_recognition" title="Click to go to the Keyword Index">
               Intention Recognition
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Electroencephalographic (EEG) signals have the potential of improving response time of robots while executing collaborative tasks. We study how EEG detects action intention before it begins, with high temporal resolution. Modern Deep Learning techniques, particularly Convolutional Neural Networks (CNNs), can handle multi-modal data and enable end-to-end time series classification. This allows for extracting useful features from EEG data with low latency. This study evaluates different CNN-based approaches for action anticipation from EEG signals, achieving up to 80.90% accuracy on a motor imagery (MI) HRI dataset. These results are also validated in a realistic pilot experiment, where classifiers accurately predict actions several hundred milliseconds before they occur.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_31">
             12:30-13:15, Paper ThINT1S.31
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod443" name="modify515" onclick="modify(515,443)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('515'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning the Ego-Motion of an Underwater Imaging Sonar: A Comparative Experimental Evaluation of Novel CNN and RCNN Approaches
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#368382" title="Click to go to the Author Index">
             Muñoz, Bastián
            </a>
           </td>
           <td class="r">
            Pontificia Universidad Católica De Chile
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#123330" title="Click to go to the Author Index">
             Troni, Giancarlo
            </a>
           </td>
           <td class="r">
            Monterey Bay Aquarium Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab515" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This research addresses the challenge of estimating the ego-motion of a forward-looking sonar (FLS) through deep neural networks (DNNs) and their application in autonomous underwater robots. Over the last two decades, analytical methods have been developed to perform odometry estimation using FLS data. While these methods can be effective, they are often computationally intensive, complex to implement, or rely on simplifying assumptions restricting their widespread application. Inspired by works on the optical domain, we propose two novel deep-learning approaches to estimate the FLS ego-motion. The first approach employs a convolutional neural network (CNN) to estimate motion from FLS images directly. The second approach leverages sequential image information using a recurrent convolutional neural network (RCNN). We quantitatively evaluate their performance by training and testing on synthetic and field data. Results show that both methods can learn to estimate ego-motion on both data types and that including sequential information can improve performance. This work presents the first usage of a RCNN for this task, advances toward real-world applications by fine-tuning the models based on field data, and the first quantitative evaluation of these kinds of methods for acoustic odometry using field data.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_32">
             12:30-13:15, Paper ThINT1S.32
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod444" name="modify268" onclick="modify(268,444)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('268'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Field-Ready Autonomous Robot: Door Opening and Passing through Using Deep Predictive Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#243493" title="Click to go to the Author Index">
             Ito, Hiroshi
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#285628" title="Click to go to the Author Index">
             Ichiwara, Hideyuki
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd. / Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#202288" title="Click to go to the Author Index">
             Yamamoto, Kenjiro
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410030" title="Click to go to the Author Index">
             Noguchi, Naoaki
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab268" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Robotics applications of deep learning are advancing rapidly, enabling autonomous robots to operate in diverse environments. Deep reinforcement learning allows robots to learn novel behaviors through trial and error, but challenges such as complex reward design and reality gaps remain. Recently, imitation learning has emerged as a key technology that simplifies the process by training models to mimic expert operations. Deep predictive learning, inspired by predictive coding theory, predicts the robot's sensorimotor information and minimizes prediction errors for stable operations. This paper presents a robotic system that efficiently reduces the cost of robot motion teaching and model learning. We show that deep predictive learning can be applied to a robot to reduce the task execution time by about half and the number of teaching motions by about 1/4 compared to previous study. Our robot system was continuously demonstrated for four days at the exhibition booth of ICRA 2024 in Yokohama, showing its robust performance under real-world conditions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_33">
             12:30-13:15, Paper ThINT1S.33
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod445" name="modify269" onclick="modify(269,445)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('269'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leveraging Swarm Capabilities to Assist Other Systems
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#307107" title="Click to go to the Author Index">
             Kegeleirs, Miquel
            </a>
           </td>
           <td class="r">
            Université Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#303018" title="Click to go to the Author Index">
             Garzón Ramos, David
            </a>
           </td>
           <td class="r">
            University of Bristol
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#379356" title="Click to go to the Author Index">
             Legarda Herranz, Guillermo
            </a>
           </td>
           <td class="r">
            Université Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#340869" title="Click to go to the Author Index">
             Gharbi, Ilyes
            </a>
           </td>
           <td class="r">
            Université Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#376603" title="Click to go to the Author Index">
             Szpirer, Jeanne
            </a>
           </td>
           <td class="r">
            IRIDIA, Université Libre De Bruxelles, Brussels, Belgium
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#384296" title="Click to go to the Author Index">
             Hasselmann, Ken
            </a>
           </td>
           <td class="r">
            Royal Military Academy
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#165078" title="Click to go to the Author Index">
             Garattoni, Lorenzo
            </a>
           </td>
           <td class="r">
            Toyota Motor Europe
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#165079" title="Click to go to the Author Index">
             Francesca, Gianpiero
            </a>
           </td>
           <td class="r">
            Université Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#138052" title="Click to go to the Author Index">
             Birattari, Mauro
            </a>
           </td>
           <td class="r">
            Université Libre De Bruxelles
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab269" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Swarm perception is the ability of the swarm to leverage the perception capabilities of each individual and create a collective understanding of the environment. Swarm perception is particularly valuable for quickly providing access to a large amount of data acquired in large unknown environments. Most studies emphasizing swarm perception treat the swarm as an isolated system of interest. But certain scenarios, such as search and rescue operations, might not derive substantial advantage from deploying a robot swarm as an autonomous solution. However, the assistance provided by a swarm could prove invaluable to human rescuers. By leveraging swarm capabilities such as swarm perception, a robot swarm could act as an efficient support in an heterogeneous system comprising other robots and/or human operators. Tasks such as target identification &amp; tracking, scouting, or monitoring/surveillance could benefit from this approach. We present a speculative application for a people identification and tracking scenario in an office environment: a swarm of Mercator robots explores and maps an office environment while collecting localization data on encountered people. By sharing this data, the swarm helps a Toyota Human Support Robot to deliver a letter to one specific person.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_34">
             12:30-13:15, Paper ThINT1S.34
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod446" name="modify273" onclick="modify(273,446)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('273'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automated Design and Execution of Expressive 6DOF Arm Motion with Blender and GPT
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#237589" title="Click to go to the Author Index">
             Mercader, Alexandra Léna Victoria
            </a>
           </td>
           <td class="r">
            École De Technologie Supérieure De Montréal
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab273" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#gesture__posture_and_facial_expressions" title="Click to go to the Keyword Index">
               Gesture, Posture and Facial Expressions
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In the field of human-machine communication, robot expressivity is key to smooth and natural interaction with humans. Although human motion expressivity has been extensively analyzed, particularly through dance notation such as Laban's theory, its application in robotics remains insufficiently explored. This research employs a 6-degree-of-freedom (6DOF) robotic arm to elucidate the range of expressive capabilities that can be derived from this theoretical framework. We demonstrate that fundamental movements such as object manipulation can be interpreted variably through the modulation of specific kinematic parameters, including direction of movement, spatial envelope, gripper orientation, and the dynamics of internal and external forces. These adjustments can effectively convey nuanced information about the robot's operational status and internal state, such as indicating readiness, processing activity, or the need for human intervention, thereby enhancing the clarity and effectiveness of human-robot interactions. To facilitate this exploration, we used the motion generation of a customized GPT describing movement thanks to the knowledge of Movement-Observation-Analysis (MOA). This method enables movement creation based on either user input or GPT. The generated sequence is imported into Blender, where it can be further modified and refined using animation software to emphasize the critical role of expressivity in robotic motion design. This demonstrates that robots can transcend basic functional tasks to exhibit expressive behaviors, significantly enhancing the quality and intuitiveness of human-robot interactions and collaborations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_35">
             12:30-13:15, Paper ThINT1S.35
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod447" name="modify277" onclick="modify(277,447)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('277'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              HARMONIC: A Framework for Explanatory Cognitive Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#275715" title="Click to go to the Author Index">
             Oruganti, Sanjay
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410623" title="Click to go to the Author Index">
             Nirenburg, Sergei
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410624" title="Click to go to the Author Index">
             McShane, Marjorie
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410625" title="Click to go to the Author Index">
             English, Jesse
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410626" title="Click to go to the Author Index">
             Roberts, Michael
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410627" title="Click to go to the Author Index">
             Arndt, Christian
            </a>
           </td>
           <td class="r">
            Rensselaer Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab277" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#cognitive_control_architectures" title="Click to go to the Keyword Index">
               Cognitive Control Architectures
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#natural_dialog_for_hri" title="Click to go to the Keyword Index">
               Natural Dialog for HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present HARMONIC, a framework for implementing cognitive robots that transforms general-purpose robots into trusted teammates capable of complex decision-making, natural communication and human-level explanation. The framework supports interoperability between a strategic (cognitive) layer for high-level decision-making and a tactical (robot) layer for low-level control and execution. We describe the core features of the framework and our initial implementation, in which HARMONIC was deployed on a simulated UGV and drone involved in a multi-robot search and retrieval task.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_36">
             12:30-13:15, Paper ThINT1S.36
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod448" name="modify280" onclick="modify(280,448)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             In This Article, We Present a Multi-Hierarchical Graph Construction and Planning Framework That Addresses the Task of Visual Inspection, Coupled with the Semantic Characterization of Structures in an Unknown Environment. Ensuring Up-To-Date Situational Knowledge Is Crucial for the Coordination and Planning of Search and Rescue Teams. by Leveraging High-Level Information, Gathered from Inspected Structures, Strategic Plans Can Be Tailor-Made to Suit Structures of Interest. in Line with This Vision, We Present the Layered Semantic Graph (LSG) Framework, Which Integrates Spatial and Semantic Knowledge of an Environment As Abstract Layers of a Hierarchical Nested Graph. Grounding the Abstract Concepts in High-Level Planner Inputs and Instantaneous Visual Information, the Proposed LSG Provides a Real-Time Graph Synthesis and Decision-Making Capability. Moreover, the Nested Architecture Is Exploited to Address Fast Path-Planning Capability on Multi-Hierarchical Nested Graphs. the Performance of the Proposed Framework Is Validated through Simulations in Large-Scale Environments Along with Experimental Evaluations Using Aerial Platforms
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#315260" title="Click to go to the Author Index">
             Kottayam Viswanathan, Vignesh
            </a>
           </td>
           <td class="r">
            Lulea University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#305363" title="Click to go to the Author Index">
             Satpute, Sumeet
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#199367" title="Click to go to the Author Index">
             Kanellakis, Christoforos
            </a>
           </td>
           <td class="r">
            LTU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_37">
             12:30-13:15, Paper ThINT1S.37
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod449" name="modify283" onclick="modify(283,449)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('283'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Artificial Cognition, from the Ground Up
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#252913" title="Click to go to the Author Index">
             Pasquali, Dario
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410651" title="Click to go to the Author Index">
             Miceli, Carmine
            </a>
           </td>
           <td class="r">
            COgNiTive Architecture for Collaborative Technologies, Istituto
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#281625" title="Click to go to the Author Index">
             Belgiovine, Giulia
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#148808" title="Click to go to the Author Index">
             Rea, Francesco
            </a>
           </td>
           <td class="r">
            Istituto Italiano Di Tecnologia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#108313" title="Click to go to the Author Index">
             Mastrogiovanni, Fulvio
            </a>
           </td>
           <td class="r">
            University of Genoa
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107990" title="Click to go to the Author Index">
             Sandini, Giulio
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology - Center for Human Technologies
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#108900" title="Click to go to the Author Index">
             Sciutti, Alessandra
            </a>
           </td>
           <td class="r">
            Italian Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab283" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#developmental_robotics" title="Click to go to the Keyword Index">
               Developmental Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#long_term_interaction" title="Click to go to the Keyword Index">
               Long term Interaction
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We present our work-in-progress efforts toward enabling the humanoid robot iCub to collect and learn from unsupervised first-hand experiences autonomously. Such personal, situated, embodied, and developmental-inspired Artificial Cognition would be crucial to enable social robots to dynamically adapt and interact in everyday life scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_38">
             12:30-13:15, Paper ThINT1S.38
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod450" name="modify284" onclick="modify(284,450)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('284'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Addressing the Challenges of Underwater Motion Planning towards Enabling Autonomous Aquaculture Operations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#192519" title="Click to go to the Author Index">
             Xanthidis, Marios
            </a>
           </td>
           <td class="r">
            SINTEF Ocean
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#296277" title="Click to go to the Author Index">
             Amundsen, Herman Biørn
            </a>
           </td>
           <td class="r">
            NTNU
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#384023" title="Click to go to the Author Index">
             Haugaløkken, Bent
            </a>
           </td>
           <td class="r">
            SINTEF Ocean
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#204300" title="Click to go to the Author Index">
             Evjemo, Linn Danielsen
            </a>
           </td>
           <td class="r">
            SINTEF Fisheries and Aquaculture
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#132933" title="Click to go to the Author Index">
             Alexis, Kostas
            </a>
           </td>
           <td class="r">
            NTNU - Norwegian University of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#138305" title="Click to go to the Author Index">
             Kelasidi, Eleni
            </a>
           </td>
           <td class="r">
            SINTEF Ocean
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab284" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Motion planning is among the most fundamental problems any autonomous system should solve in order to execute tasks safely. Despite recent advancements in robotics enabling the introduction of mobile robots in industrial settings, autonomous operations in the underwater domain remains a challenging problem. This is particularly true in industrial aquaculture settings, where autonomous underwater vehicles (AUVs) are expected to operate in environments with lack of static reference, deformable moving obstacles, uncertainty, disturbances from ocean waves, and currents. From a motion planning perspective, the goal of this work is to illuminate the unique combination of problems limiting aquaculture automation, identify minimum necessary requirements, and mention recent developments towards mitigating some of these challenges to improve efficiency and safety. Finally, future prospects and research directions are included towards realizing autonomous systems in the challenging domain of aquaculture robotics and automation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_39">
             12:30-13:15, Paper ThINT1S.39
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod451" name="modify285" onclick="modify(285,451)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('285'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Perspectives on Using Multi-Modal Large Language Models for Physical Human-Robot Interaction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#351022" title="Click to go to the Author Index">
             Rohrmüller, Martin
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-Universität Erlangen-Nürnberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410683" title="Click to go to the Author Index">
             Ren, Yongxu
            </a>
           </td>
           <td class="r">
            Friedrich-Alexander-Universität Erlangen-Nürnberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410668" title="Click to go to the Author Index">
             Schlumbom, Paul-Ruben
            </a>
           </td>
           <td class="r">
            University of Waikato
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410681" title="Click to go to the Author Index">
             Bifet, Albert
            </a>
           </td>
           <td class="r">
            Artificial Intelligence Institute, University of Waikato
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#153085" title="Click to go to the Author Index">
             Beckerle, Philipp
            </a>
           </td>
           <td class="r">
            FAU Erlangen-Nürnberg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#195835" title="Click to go to the Author Index">
             Dwivedi, Anany
            </a>
           </td>
           <td class="r">
            University of Auckland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab285" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#control_architectures_and_programming" title="Click to go to the Keyword Index">
               Control Architectures and Programming
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper explores the utilization of large language models (LLMs) in human-robot interaction (HRI). We analyze different methods of including LLMs in the HRI frameworks, broadly categorizing them into two approaches based on their directness in generating robot actions: integrated language-based HRI that employs LLMs to directly generate low-level actions, and modular language-based HRI, that uses LLMs as a semantic planning or decision-making module. We evaluate the advantages and disadvantages of each approach, considering practical applications in HRI scenarios. Our analysis highlights the pros and cons of using each approach, considering their generalizability, data efficiency, scalability, and complexity. We also highlight the potential and future perspective of advanced LLM-based approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_40">
             12:30-13:15, Paper ThINT1S.40
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod452" name="modify288" onclick="modify(288,452)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('288'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              High-Speed Obstacle Avoidance of a Large-Scale Underactuated Autonomous Underwater Vehicle under a Finite Field of View (I)
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#284848" title="Click to go to the Author Index">
             Yu, Lin
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#404727" title="Click to go to the Author Index">
             Qiao, Lei
            </a>
           </td>
           <td class="r">
            Shanghai Jiao Tong University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#198849" title="Click to go to the Author Index">
             Shen, Chao
            </a>
           </td>
           <td class="r">
            Carleton University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab288" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#marine_robotics" title="Click to go to the Keyword Index">
               Marine Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#collision_avoidance" title="Click to go to the Keyword Index">
               Collision Avoidance
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#underactuated_robots" title="Click to go to the Keyword Index">
               Underactuated Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper addresses the problem of high-speed waypoint tracking and real-time obstacle avoidance for large-scale underactuated autonomous underwater vehicles (AUVs) in the vertical plane. Specifically, a robust nonlinear model predictive control (RNMPC) scheme is proposed, considering different types of constraints including the scale of the AUV, the finite field of view of the sensor, the input saturation, the physical limits on system state, and the influence of the vertical underactuated velocity. To navigate in the completely unknown environment with nonconvex obstacles, a dynamic sensing and collision avoidance scheme is proposed so that the collision avoidance can be properly formulated into convex constraints in the RNMPC optimization problem. Recursive feasibility and closed-loop stability are proved rigorously. Through the high-fidelity simulations with graph and data visualization techniques, the proposed algorithm has higher waypoint tracking accuracy, safer obstacle avoidance ability, and better multiple constraints handling capability than the existing dynamic virtual AUV technique.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_41">
             12:30-13:15, Paper ThINT1S.41
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod453" name="modify308" onclick="modify(308,453)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('308'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              AICOR Learning Hub - Video Description
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#357124" title="Click to go to the Author Index">
             Syrbe, Jörn
            </a>
           </td>
           <td class="r">
            University of Bremen, Institute for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#357129" title="Click to go to the Author Index">
             Dziomba, Leonie
            </a>
           </td>
           <td class="r">
            University of Bremen, Institute for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410752" title="Click to go to the Author Index">
             Zhan, Yanxiang
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#297951" title="Click to go to the Author Index">
             Kümpel, Michaela
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#357125" title="Click to go to the Author Index">
             Wenzl, Petra
            </a>
           </td>
           <td class="r">
            Institute for Artificial Intelligence, University of Bremen, Bre
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410750" title="Click to go to the Author Index">
             Wünsch, Timm
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410753" title="Click to go to the Author Index">
             Marks, Elias
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#103133" title="Click to go to the Author Index">
             Beetz, Michael
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab308" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#education_robotics" title="Click to go to the Keyword Index">
               Education Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#cognitive_modeling" title="Click to go to the Keyword Index">
               Cognitive Modeling
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The next generation of AI-based robots must have advanced cognitive abilities to navigate the physical and social world effectively. At the same time, future roboticists must have a deep understanding of cognitive robotics to equip these robots. That is why our Institute for Artificial Intelligence is committed to making our cognitive robotics research transparent, comprehensible, and usable for both researchers and learners. Our efforts are centralized in the AICOR VRB, a virtual build- ing featuring an educational floor centered on the IntEL4CoRo project. This platform integrates media, higher education, and didactic concepts into a learning-centered structure at the module and program levels. We leverage instructional design to convert curriculum content and competencies into teaching materials, to foster learners’ independent problem-solving skills by changing the teacher role to supervisors enabling independent research. The APKIPE model guides learners through five phases: The arrival, prior knowledge, informing, processing, and evaluation phases are all part of a rich simulated environment for practical cognitive robotics experience. This educational approach not only facilitates understanding of cognition-enabled robotic systems but also promotes holistic awareness by supporting the UN’s Sustainability Development Goals, encouraging learners to align their projects with these goals. The IntEL4CoRo project makes research accessible and promotes collaboration in AI-powered robotics and cognition-enabled robotics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_42">
             12:30-13:15, Paper ThINT1S.42
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod454" name="modify312" onclick="modify(312,454)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('312'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Weakly Supervised Video Object Segmentation for Robotic Surgery through Semi-Decoupled Distillation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#308679" title="Click to go to the Author Index">
             Liao, Guiqiu
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#193550" title="Click to go to the Author Index">
             Jogan, Matjaz
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410831" title="Click to go to the Author Index">
             Koushik, Sai
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#196377" title="Click to go to the Author Index">
             Eaton, Eric
            </a>
           </td>
           <td class="r">
            University of Pennsylvania
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#206060" title="Click to go to the Author Index">
             Hashimoto, Daniel
            </a>
           </td>
           <td class="r">
            MGH
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab312" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Weakly supervised video object segmentation (WSVOS) relies solely on binary video-level presence labels to segment objects in frames, which is challenging given complex spatiotemporal interactions of objects in real-life applications such as robotic surgery. This paper introduces a semi-decoupled distillation approach to disentangle spatiotemporal knowledge and train networks to generate class activation maps for video segmentation. Segmentation performance is further improved using teacher-student networks that learn spatial and temporal aspects of data. We test our method on instrument segmentation in surgical video where it outperforms state-of-the-art methods that use either frame-level or video-level labels. This research holds promise to scale up datasets by reducing manual annotation efforts.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_43">
             12:30-13:15, Paper ThINT1S.43
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod455" name="modify313" onclick="modify(313,455)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('313'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Predubot: An Integrated Learning Platform for Preschool Computational Thinking
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410758" title="Click to go to the Author Index">
             Coiro, Francisca
            </a>
           </td>
           <td class="r">
            Not Apply
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#179440" title="Click to go to the Author Index">
             Solis, Miguel A.
            </a>
           </td>
           <td class="r">
            Universidad Andrés Bello
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#199635" title="Click to go to the Author Index">
             Nettle, Cristóbal
            </a>
           </td>
           <td class="r">
            Centro De Innovación Y Robótica
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410761" title="Click to go to the Author Index">
             Chila, Anibal
            </a>
           </td>
           <td class="r">
            Not Apply
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab313" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#education_robotics" title="Click to go to the Keyword Index">
               Education Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_in_under_resourced_settings" title="Click to go to the Keyword Index">
               Robotics in Under-Resourced Settings
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#art_and_entertainment_robotics" title="Click to go to the Keyword Index">
               Art and Entertainment Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Educational robotics represents a novel and attractive scenario for stimulating different abilities frequently related to science, technology, engineering, and math. There are various methods available based on the particular skill being addressed and the required resources, such as physical equipment or computational instruments.
             <p>
              Both commercial platforms and open-source educational kits have significantly reduced the coding skills needed to specify desired behavior on a physical agent. This can be achieved through programming using block sequences or by incorporating a smartphone or tablet for more intuitive interactions. This video shows the implementation of a new open-source robot, designed to help preschool children develop their computational thinking skills. It has a microcontroller unit that is integrated into the robot and does not need any extra equipment like computers for programming, since it translates physical blocks printed on PLA into logical instructions for locomotion. The approach shares similarities with Arduino programming due to its structural architecture, which enables an easy transition to more advanced platforms as children grow older.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_44">
             12:30-13:15, Paper ThINT1S.44
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod456" name="modify314" onclick="modify(314,456)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('314'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              MIRE: Mixed Reality Based Interactive Platform for Robotics Education
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410763" title="Click to go to the Author Index">
             Pareek, Vishakha
            </a>
           </td>
           <td class="r">
            TIH, iHub Drishti, IIT Jodhpur
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410766" title="Click to go to the Author Index">
             Sharma, Shreyansh
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology Jodhpur
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#340457" title="Click to go to the Author Index">
             Dubey, Richa
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology, Jodhpur
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410770" title="Click to go to the Author Index">
             Singh, Vibhor
            </a>
           </td>
           <td class="r">
            TIH-Ihub Drishti, IIT Jodhpur
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#172205" title="Click to go to the Author Index">
             Chaudhury, Santanu
            </a>
           </td>
           <td class="r">
            IIT Delhi
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab314" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#education_robotics" title="Click to go to the Keyword Index">
               Education Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents an innovative Mixed Reality (MR) platform termed MIRE, which is designed to introduce school students to the fundamentals of robotics. MIRE is a unique, interactive, and engaging educational platform that makes an introduction to robotics both engaging and accessible, particularly for school students. Whereas the cost, setup, and maintenance of a physical robot are typically expensive, MIRE provides holistic information about four different types of robots in a single, cost-effective, interactive setup. MIRE is particularly beneficial where access to expensive hardware setups and advanced educational resources is limited.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_45">
             12:30-13:15, Paper ThINT1S.45
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod457" name="modify326" onclick="modify(326,457)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('326'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Extraction of Culture Representations from Multimodal Datasets
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#361817" title="Click to go to the Author Index">
             Gjaci, Ariel
            </a>
           </td>
           <td class="r">
            University of Genoa
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#250569" title="Click to go to the Author Index">
             Schmuck, Viktor
            </a>
           </td>
           <td class="r">
            King's College London, United Kingdom
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#108312" title="Click to go to the Author Index">
             Sgorbissa, Antonio
            </a>
           </td>
           <td class="r">
            University of Genova
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#158663" title="Click to go to the Author Index">
             Celiktutan, Oya
            </a>
           </td>
           <td class="r">
            King's College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab326" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#multi_modal_perception_for_hri" title="Click to go to the Keyword Index">
               Multi-Modal Perception for HRI
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study addresses the challenge of embedding culturally adaptive behaviors in artificial agents by analyzing cultural elements in a multimodal interaction dataset. Our approach begins by identifying cultural differences among three distinct groups using high-level textual and gestural features. We then trained and evaluated various Fully Connected Neural Network (FCNN) models for culture classification to determine if their performance was sufficient for using their weights in culture representation. We employed both subject-dependent and subject-independent data splits in our analysis. To address speaker variability, we enhanced the FCNN models with a speaker classification layer and applied adversarial learning techniques to develop speaker-invariant features. We compared the performance of these improved models against the standard FCNN setup and a multi-task learning framework. Preliminary results indicate that FCNN classifiers achieve high accuracy with subject-dependent data but face challenges with subject-independent data, highlighting difficulties in generalizing to unseen speakers. While adversarial learning partially mitigates these issues, further improvements are necessary.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_46">
             12:30-13:15, Paper ThINT1S.46
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod458" name="modify337" onclick="modify(337,458)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('337'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards Collaborative Semantic Visual Navigation Via Large Language Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#276203" title="Click to go to the Author Index">
             Yu, Bangguo
            </a>
           </td>
           <td class="r">
            University of Groningen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#237992" title="Click to go to the Author Index">
             Li, Kailai
            </a>
           </td>
           <td class="r">
            University of Groningen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#169824" title="Click to go to the Author Index">
             Kasaei, Hamidreza
            </a>
           </td>
           <td class="r">
            University of Groningen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#166691" title="Click to go to the Author Index">
             Cao, Ming
            </a>
           </td>
           <td class="r">
            University of Groningen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab337" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Target-driven visual navigation in an unknown environment plays crucial roles towards reaching high-performance autonomy and human-machine interactions for intelligent robots. Most existing approaches for mapless visual navigation focus on single-robot operations, which often lack efficiency and robustness in complex environments. Meanwhile, policy learning for multi-robot collaboration is resource-demanding. To address these challenges, we propose Co-NavGPT, an innovative multi-robot framework integrating large language models (LLMs) as a global planner for collaborative visual navigation. We conduct experiments in synthetic environments for evaluation. Numerical results show superior performance of Co-NavGPT over existing approaches in terms of success rate and efficiency, not requiring the learning procedure and yet demonstrating great potential of exploiting LLMs in multi-robot collaboration. We open-source our current implementation at https://github.com/ybgdgh/Co-NavGPT.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_47">
             12:30-13:15, Paper ThINT1S.47
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod459" name="modify354" onclick="modify(354,459)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('354'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Point Cloud Forecasting Using Temporal Attention
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#363878" title="Click to go to the Author Index">
             Dasgupta, Soham
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology Roorkee
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410846" title="Click to go to the Author Index">
             Aphale, Kshitij
            </a>
           </td>
           <td class="r">
            Indian Institute of Technology, Jodhpur
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#113942" title="Click to go to the Author Index">
             Sharma, Avinash
            </a>
           </td>
           <td class="r">
            International Institute of Information Technology,
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab354" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#computer_vision_for_automation" title="Click to go to the Keyword Index">
               Computer Vision for Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Point cloud forecasting is crucial for the success of motion planning algorithms. However, due to its complex nature and challenges in integrating with existing architectures, it remains an extremely difficult and interesting problem. In this extended abstract, we propose a novel attention-based architecture for point cloud forecasting and report improved quantitative and qualitative results in comparison to the SOTA method on publicly available datasets while offering comparatively faster inference speeds.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_48">
             12:30-13:15, Paper ThINT1S.48
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod460" name="modify366" onclick="modify(366,460)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('366'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic Exploration Using Generalized Behavioral Entropy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#192667" title="Click to go to the Author Index">
             Suresh, Aamodh
            </a>
           </td>
           <td class="r">
            US Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#103595" title="Click to go to the Author Index">
             Nieto-Granda, Carlos
            </a>
           </td>
           <td class="r">
            DEVCOM U.S. Army Research Laboratory
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#113333" title="Click to go to the Author Index">
             Martinez, Sonia
            </a>
           </td>
           <td class="r">
            UC San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab366" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#probability_and_statistical_methods" title="Click to go to the Keyword Index">
               Probability and Statistical Methods
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#planning_under_uncertainty" title="Click to go to the Keyword Index">
               Planning under Uncertainty
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work presents and evaluates a novel strategy for robotic exploration that leverages human models of uncertainty perception. To do this, we introduce a measure of uncertainty that we term “Behavioral entropy”, which builds on Prelec’s probability weighting from Behavioral Economics. We show that the new operator is an admissible generalized entropy, analyze its theoretical properties and compare it with other common formulations such as Shannon’s and Renyi’s. In particular, we discuss how the new formulation is more expressive in the sense of measures of sensitivity and perceptiveness to uncertainty introduced here. Then we use Behavioral entropy to define a new type of utility function that can guide a frontier based environment exploration process. The approach’s benefits are illustrated and compared in a Proof-of-Concept and ROS unity simulation environment with a Clearpath Warthog robot. We show that the robot equipped with Behavioral entropy explores faster than Shannon and Renyi entropies.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_49">
             12:30-13:15, Paper ThINT1S.49
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod461" name="modify402" onclick="modify(402,461)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('402'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Software Development Framework for Multi-Robot Systems with High-Level Specification
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#254225" title="Click to go to the Author Index">
             Kang, Woosuk
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411255" title="Click to go to the Author Index">
             Yoon, Kyonghwan
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#350570" title="Click to go to the Author Index">
             Jeong, EunJin
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#254228" title="Click to go to the Author Index">
             Ha, Soonhoi
            </a>
           </td>
           <td class="r">
            Seoul National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab402" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#software_tools_for_robot_programming" title="Click to go to the Keyword Index">
               Software Tools for Robot Programming
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#embedded_systems_for_robotic_and_automation" title="Click to go to the Keyword Index">
               Embedded Systems for Robotic and Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In this paper, we present HiSARM (High-level Specification, Automatic Code Generation, and Retargetable Deployment for Multi-robot Systems), a novel software development framework designed to enhance swarm functionalities for real-world applications. HiSARM supports flexible and user-friendly high-level mission specifications and automates the entire process from code generation to deployment and execution, improving user convenience and productivity. Key features of our framework in the context of swarm robotics, such as robust leader election, dynamic grouping, and decentralized decision-making, enhance user productivity when developing swarm applications. Preliminary experiments have demonstrated its capability to handle complex missions and consistency between simulation and real-world operations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_50">
             12:30-13:15, Paper ThINT1S.50
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod462" name="modify407" onclick="modify(407,462)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('407'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automatic In-Vehicle Occupancy Detection Using Low Cost Mm-Wave Radar
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411303" title="Click to go to the Author Index">
             Mohan, Anand
            </a>
           </td>
           <td class="r">
            Malaviya National Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411301" title="Click to go to the Author Index">
             Meena, Hemant
            </a>
           </td>
           <td class="r">
            Malaviya National Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411304" title="Click to go to the Author Index">
             Srivastava, Abhishek
            </a>
           </td>
           <td class="r">
            IIIT Hyderabad
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab407" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             These days, autonomous vehicles are equipped with a variety of sensors that allow them to sense their environment and control several functions. In particular, if a child is left alone in a parked car, the passenger occupancy detection system can detect it. Traditionally, camera sensors have been used to detect vehicle occupancy. Cameras perform well in bright light but struggle in low light. To address these limitations, we opted to detect vehicle occupancy using millimeter-wave radar. Radar operates accurately regardless of lighting conditions. Point cloud images from our experiments were used for the proposed work. After generating 3D point cloud images, two filters, top view, and front view, were used to improve vehicle occupancy detection. These filters transformed 3D images into 2D ones. The top-view filter was found to be more effective than the front-view filter. Several machine learning algorithms were then used to detect vehicle seat occupancy. Logistic Regression produced the best results, with an accuracy of 95%. Compared to existing methodologies, our model significantly improves vehicle occupancy detection accuracy. Our machine learning model outperforms deep learning alternatives in terms of accuracy, processing time, and dataset size.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_51">
             12:30-13:15, Paper ThINT1S.51
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod463" name="modify414" onclick="modify(414,463)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('414'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Extended Abstract: Smart Auditory Item Locator (SAIL): A LoRa-Based Solution for Visually Impaired Users
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#408445" title="Click to go to the Author Index">
             Agarwal, Arav
            </a>
           </td>
           <td class="r">
            The Internation School of Bangalore
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#408444" title="Click to go to the Author Index">
             Pokhali, Roohi
            </a>
           </td>
           <td class="r">
            ZAS Academy
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab414" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#hardware_software_integration_in_robotics" title="Click to go to the Keyword Index">
               Hardware-Software Integration in Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#ai_enabled_robotics" title="Click to go to the Keyword Index">
               AI-Enabled Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#physically_assistive_devices" title="Click to go to the Keyword Index">
               Physically Assistive Devices
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Our proposed Smart Auditory Item Locator (SAIL) leverages LoRa technology's reliability, range, and low power consumption to assist visually impaired users in locating items like keys and wallets. The system comprises a primary node, resembling a TV remote, and client nodes, both equipped with LoRa transmitters and receivers. Users activate client nodes by pressing buttons on the primary node, which respond with RSSI signal strength. A machine learning model correlates RSSI with distance, dynamically adjusting buzzer volume for intuitive proximity feedback. Tested in a multi-floor residence, SAIL demonstrated robust indoor connectivity, effectively guiding users despite walls and ceilings.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_52">
             12:30-13:15, Paper ThINT1S.52
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod464" name="modify422" onclick="modify(422,464)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('422'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              The Swarm Garden
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#286427" title="Click to go to the Author Index">
             Alhafnawi, Merihan
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411347" title="Click to go to the Author Index">
             Stein-Montalvo, Lucia
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411350" title="Click to go to the Author Index">
             Bendarkawi, Jad
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411348" title="Click to go to the Author Index">
             Tafesse, Yenet
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411351" title="Click to go to the Author Index">
             Chow, Vicky
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411353" title="Click to go to the Author Index">
             Adriaenssens, Sigrif
            </a>
           </td>
           <td class="r">
            Princeton University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#111044" title="Click to go to the Author Index">
             Nagpal, Radhika
            </a>
           </td>
           <td class="r">
            Harvard University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab422" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#swarm_robotics" title="Click to go to the Keyword Index">
               Swarm Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#art_and_entertainment_robotics" title="Click to go to the Keyword Index">
               Art and Entertainment Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#building_automation" title="Click to go to the Keyword Index">
               Building Automation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             At the intersection of swarm robotics and architecture, we created the Swarm Garden, a novel responsive system to be deployed on façades. The Swarm Garden is an adaptive shading system made of a swarm of robotic modules that respond to humans and the environment while creating beautiful spaces. In this video, we showcase 35 robotic modules that we designed and built for The Swarm Garden. The modules are equipped with sensors that enable human-swarm interaction and environmental interaction. We further created a wearable device as an additional mode of interaction that the modules respond to. In the video, we showcase the swarm and show snippets of an exhibition we hosted with our swarm. The attendees interacted with the modules and attended a performance where a dancer wore the wearable and the modules responded to her dance. We further show how the Swarm Garden is envisioned to be deployed on façades, providing a shading mechanism as well as creating a beautiful space.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_53">
             12:30-13:15, Paper ThINT1S.53
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod465" name="modify213" onclick="modify(213,465)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('213'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards a Scalable Control Scheme for Bio-Inspired Shotcrete Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#240945" title="Click to go to the Author Index">
             Wu, Rui
            </a>
           </td>
           <td class="r">
            École Polytechnique Fédérale De Lausanne
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#266690" title="Click to go to the Author Index">
             Gholami, Soheil
            </a>
           </td>
           <td class="r">
            École Polytechnique Fédérale De Lausanne (EPFL)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#388508" title="Click to go to the Author Index">
             Bonato, Tristan
            </a>
           </td>
           <td class="r">
            École Polytechnique Fédérale De Lausanne, Learning Algorithms An
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#362010" title="Click to go to the Author Index">
             Munier, Louis
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#106862" title="Click to go to the Author Index">
             Billard, Aude
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab213" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_and_automation_in_construction" title="Click to go to the Keyword Index">
               Robotics and Automation in Construction
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Shotcrete is widely employed to maintain tunnels and large concrete structures, traditionally executed by skilled nozzle operators. Despite their expertise, operators still face challenges related to their well-being, health, and task efficiency. We developed a scalable framework for robotic shotcrete applications that can be applied to targets of varying complexity, including those partially filled with concrete, to assist nozzlemen. We take a learning from demonstration approach to model expert nozzle men. This framework facilitates spraying complex geometries, such as curved surfaces found in tunnels and includes surface finishing capabilities via interaction control. Validation experiments were conducted using a 6-degree-of-freedom velocity-based control robot in simulations and real-world.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_54">
             12:30-13:15, Paper ThINT1S.54
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod466" name="modify461" onclick="modify(461,466)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('461'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Semantic-Geometric Task Graphs from Bimanual Human Demonstrations
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#317861" title="Click to go to the Author Index">
             Herbert, Franziska
            </a>
           </td>
           <td class="r">
            TU Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#205320" title="Click to go to the Author Index">
             Prasad, Vignesh
            </a>
           </td>
           <td class="r">
            TU Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#190438" title="Click to go to the Author Index">
             Koert, Dorothea
            </a>
           </td>
           <td class="r">
            Technische Universitaet Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#168538" title="Click to go to the Author Index">
             Chalvatzaki, Georgia
            </a>
           </td>
           <td class="r">
            Technische Universität Darmstadt
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab461" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#semantic_scene_understanding" title="Click to go to the Keyword Index">
               Semantic Scene Understanding
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Understanding how humans execute and sequence their actions is essential for learning robot skills from demonstrations of long-horizon tasks. An efficient approach is to decompose such tasks into smaller sub-tasks composed of atomic skills and some objects, and properly sequenced to complete the overall task. We present a novel Graph-based neural architecture for learning task graphs from human demonstrations. In particular, we train our network to predict the next actions, object-action saliency, and the subsequent evolution of the scene via motion forecasting, i.e., capturing high-level task dynamics. We present some initial results showing the efficacy of our method on various bimanual tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_55">
             12:30-13:15, Paper ThINT1S.55
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod467" name="modify463" onclick="modify(463,467)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('463'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Using Sensorised Tendons to Achieve Proprioceptive Pose Estimation in Continuum Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411434" title="Click to go to the Author Index">
             De Freitas Ferrari, Murilo
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#142995" title="Click to go to the Author Index">
             Witkowski, Mark
            </a>
           </td>
           <td class="r">
            Imperial College
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#268924" title="Click to go to the Author Index">
             Brown, Joshua
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab463" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#soft_sensors_and_actuators" title="Click to go to the Keyword Index">
               Soft Sensors and Actuators
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Sensing the shape of a continuum robot is an open challenge in soft robotics. The infinite flexibility of these robots makes them difficult to instrument, and the enclosed environments in which they operate makes many traditional sensors unsuitable. This work proposes that the sensorisation of conductive tendons could enable proprioceptive pose estimation for continuum robots. A working prototype sensor is installed on a small continuum robot, and its position and orientation estimates compared against visual odometry from fiducial markers.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_56">
             12:30-13:15, Paper ThINT1S.56
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod468" name="modify469" onclick="modify(469,468)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('469'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Future Aspects in Human Action Recognition: Exploring Emerging Techniques and Ethical Influences
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#129384" title="Click to go to the Author Index">
             Gasteratos, Antonios
            </a>
           </td>
           <td class="r">
            Democritus University of Thrace
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#344028" title="Click to go to the Author Index">
             Moutsis, Stavros
            </a>
           </td>
           <td class="r">
            Democritus University of Thrace
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#219809" title="Click to go to the Author Index">
             Tsintotas, Konstantinos A.
            </a>
           </td>
           <td class="r">
            Democritus University of Thrace
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#118557" title="Click to go to the Author Index">
             Aloimonos, Yiannis
            </a>
           </td>
           <td class="r">
            University of Maryland
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab469" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#recognition" title="Click to go to the Keyword Index">
               Recognition
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The following abstract is not included in the paper.
             <p>
              Visual-based human action recognition (HAR) remains a challenging task due to the inclusion of temporal analysis. While various techniques have been released to handle the time domain, data generated by event-based cameras, which include the time domain in their output, could optimize the HAR systems in terms of performance and latency. In addition, inappropriate datasets constitute another difficulty in creating high-performance systems, which could be addressed by including realistic synthetic videos. Alternatively, a different solution would be for the systems to rely on techniques that do not directly depend on data, such as reinforcement learning. Finally, the ethical concerns and boundaries are likely to influence and guide the development of HAR frameworks.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_57">
             12:30-13:15, Paper ThINT1S.57
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod469" name="modify481" onclick="modify(481,469)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('481'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SkeleTree: Multi-View 3D Plant Skeleton Extraction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#293488" title="Click to go to the Author Index">
             Marri, Samhita
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#293495" title="Click to go to the Author Index">
             Sivakumar, Arun Narenthiran
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#182532" title="Click to go to the Author Index">
             Uppalapati, Naveen Kumar
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#133090" title="Click to go to the Author Index">
             Chowdhary, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab481" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#rgb_d_perception" title="Click to go to the Keyword Index">
               RGB-D Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Detecting plant skeletons is a crucial task for performing various tasks like harvesting, pruning, and phenotyping by not only identifying the target regions for each of these tasks but also reasoning about collision avoidance. However, it is a challenging task due to the complex nature of the plant environments due to severe occlusion. In this work, we propose SkeleTree which predicts the skeleton of a plant that is occluded with dense foliage using RGB and depth images in multiple views. By making use of the rich features of images extracted from DINOv2, we propose predicting branch nodes and performing multi-view fusion from different views to further predict the skeleton of the plant.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_58">
             12:30-13:15, Paper ThINT1S.58
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod470" name="modify483" onclick="modify(483,470)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('483'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Leveraging Non-Prehensile Tactile Data for Object Retraction in Constrained Clutter Using Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#287452" title="Click to go to the Author Index">
             Brouwer, Dane
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#378409" title="Click to go to the Author Index">
             Citron, Joshua
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101950" title="Click to go to the Author Index">
             Cutkosky, Mark
            </a>
           </td>
           <td class="r">
            Stanford University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab483" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#force_and_tactile_sensing" title="Click to go to the Keyword Index">
               Force and Tactile Sensing
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#domestic_robotics" title="Click to go to the Keyword Index">
               Domestic Robotics
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Retracting objects from dense collections of movable objects can be difficult for humans, let alone robots. A key sensing modality that assists humans during these tasks is the tactile sensing present on the non-prehensile surfaces of our hands and arms. We propose an investigation of the role of non-prehensile tactile sensing for training robots to retract objects in constrained clutter. We have built hardware and simulation environments that closely mimic each other and utilize custom triaxial tactile sensors. We use imitation learning to train policies both with and without tactile information on 259 demonstrations gathered in simulation and compare their performance. Preliminary results indicate that non-prehensile tactile information assists navigating to the target object despite object jamming. Implementing these learned policies on the hardware setup is ongoing work.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_59">
             12:30-13:15, Paper ThINT1S.59
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod471" name="modify487" onclick="modify(487,471)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('487'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning to Turn: Diffusion Imitation for Robust Row Turning in Under-Canopy Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#293495" title="Click to go to the Author Index">
             Sivakumar, Arun Narenthiran
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#326312" title="Click to go to the Author Index">
             Thangeda, Pranay
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#323901" title="Click to go to the Author Index">
             Fang, Yixiao
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#253889" title="Click to go to the Author Index">
             Valverde Gasparino, Mateus
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#341043" title="Click to go to the Author Index">
             Cuaran, Jose
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#217244" title="Click to go to the Author Index">
             Ornik, Melkior
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#133090" title="Click to go to the Author Index">
             Chowdhary, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab487" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Under-canopy agricultural robots require robust navigation capabilities to enable full autonomy but struggle with tight row turning between crop rows due to degraded GPS reception, visual aliasing, occlusion, and complex vehicle dynamics. We propose an imitation learning approach using diffusion policies to learn row turning behaviors from demonstrations provided by human operators or privileged controllers. Simulation experiments in a corn field environment show potential in learning this task with only visual observations and velocity states. However, challenges remain in maintaining control within rows and handling varied initial conditions, highlighting areas for future improvement.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_60">
             12:30-13:15, Paper ThINT1S.60
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod472" name="modify492" onclick="modify(492,472)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('492'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Data-Centric Mapping for Autonomous Broadacre Farming
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#210679" title="Click to go to the Author Index">
             Hedayatpour, Mojtaba
            </a>
           </td>
           <td class="r">
            University of Regina
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#233006" title="Click to go to the Author Index">
             Loo, Shing Yan
            </a>
           </td>
           <td class="r">
            University of Alberta, Universiti Putra Malaysia
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411437" title="Click to go to the Author Index">
             Rahman, Parash
            </a>
           </td>
           <td class="r">
            Mojow
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#217731" title="Click to go to the Author Index">
             Scheideman, Sean
            </a>
           </td>
           <td class="r">
            University of Alberta
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411425" title="Click to go to the Author Index">
             Tata, Ganesh
            </a>
           </td>
           <td class="r">
            Mojow Autonomous Solutions
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab492" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mapping is one of the main components in developing an autonomous mobile robot, and it is especially helpful in autonomous navigation tasks. We have developed an autonomous tractor for broadacre farming, which performs many farming operations autonomously. As part of our autonomous navigation stack, we introduce our data-centric approach to the mapping problem. In the context of broadacre farming, we try to reduce any navigation task to a mapping problem first. Every navigation task starts by accurately mapping the environment around the tractor. Once accurate maps and representations of the tractor's surroundings are created, state-of-the-art motion planning and control strategies are used to navigate the tractor on the map. Our proposed approach considers both local and global mapping problems. The data-centric mapping pipeline consists of three main components: calibration and sensor fusion, perception, and mapping, each using state-of-the-art methods. Our perception system, the focus of this paper, heavily relies on our high-quality datasets developed for specific applications such as seeding, harrowing, harvesting and spraying. For data collection and dataset creation, we use EYEBOX™ hardware equipped with cameras, LiDARs, Radars, GNSS and other sensors. We discuss some of the challenges for data collection and for creating quality datasets. We present the key broadacre farming applications we focus on and explain how our data-centric mapping approach is helping us solve the problems. Later we describe how the perception system can be extended to support new navigation tasks. We also demonstrate, with examples, how the system performance improves as it is trained with more data from real operations without interrupting the work in progress. As our datasets expand in diversity, our machine-learning models become more robust to previously unseen situations. Our success in using this data-centric approach has prompted us to create a wide variety of datasets for key farming operations that constitute the majority of efforts at the farm. We also provide insights into our quality datasets, with more than 1,000,000 annotated frames created for those key farming applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_61">
             12:30-13:15, Paper ThINT1S.61
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod473" name="modify494" onclick="modify(494,473)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('494'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Towards a Real-World Dataset of Deformable Objects for Robotic Manipulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411392" title="Click to go to the Author Index">
             Obrist, Jan
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#257047" title="Click to go to the Author Index">
             Zamora Mora, Miguel Angel
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#265984" title="Click to go to the Author Index">
             Zheng, Hehui
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#272932" title="Click to go to the Author Index">
             Zarate, Juan Jose
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#164300" title="Click to go to the Author Index">
             Katzschmann, Robert Kevin
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#147028" title="Click to go to the Author Index">
             Coros, Stelian
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab494" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#data_sets_for_robot_learning" title="Click to go to the Keyword Index">
               Data Sets for Robot Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Advancing robotic manipulation of deformable objects can enable automation of repetitive tasks across multiple industries, from food processing to textiles and healthcare. Yet robots struggle with the high dimensionality of deformable objects and their complex dynamics. While data-driven methods have shown potential for solving manipulation tasks, their application in the domain of deformable objects has been constrained by the lack of data. To address this, we propose PokeFlex, a pilot dataset featuring real-world 3D mesh data of actively deformed objects, together with the corresponding forces applied by a robotic arm, using a simple poking strategy. Deformations are captured with a professional volumetric capture system that allows for complete 360-degree reconstruction. The PokeFlex dataset consists of five deformable objects with varying stiffness and shapes. Additionally, we leverage the PokeFlex dataset to train a vision model for online 3D mesh reconstruction from a single image and a template mesh. We refer readers to the supplementary material for demos and examples of our dataset.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint1s_62">
             12:30-13:15, Paper ThINT1S.62
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod474" name="modify510" onclick="modify(510,474)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('510'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Generative Design of Structural Parts of Industrial In-Mold Labeling Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411489" title="Click to go to the Author Index">
             COLAK, OGUZ
            </a>
           </td>
           <td class="r">
            Eskisehir Technical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411490" title="Click to go to the Author Index">
             Atipler, Huseyin Enes
            </a>
           </td>
           <td class="r">
            Eskisehir Technical University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411463" title="Click to go to the Author Index">
             HACIOĞLU, ERDEM
            </a>
           </td>
           <td class="r">
            TEKKAN PLASTİK SANAYİ Ve TİCARET A.Ş
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411483" title="Click to go to the Author Index">
             ASLAN, Yesim
            </a>
           </td>
           <td class="r">
            Tekkan Plastik
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411468" title="Click to go to the Author Index">
             Mutlu, ismail
            </a>
           </td>
           <td class="r">
            Tekkan Plastik
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410074" title="Click to go to the Author Index">
             EGE, Mucahit
            </a>
           </td>
           <td class="r">
            Istanbul Gedik University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab510" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#engineering_for_robotic_systems" title="Click to go to the Keyword Index">
               Engineering for Robotic Systems
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#industrial_robots" title="Click to go to the Keyword Index">
               Industrial Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This study presents a novel approach to the design optimization of structural components for industrial in-mold labeling (IML) robots using generative design techniques. IML robots play a crucial role in the automation of labeling processes within the manufacturing industry, particularly in sectors such as packaging, automotive, and consumer goods. The structural integrity and performance of these robots are paramount for ensuring precise and efficient labeling operations. Generative design, a paradigm that leverages computational algorithms and iterative optimization processes, offers a promising avenue for enhancing the structural efficiency of robotic components. By encoding design constraints, performance objectives, and material properties into the generative design framework, researchers and developers can explore vast design spaces to discover innovative solutions that fulfill functional requirements while minimizing weight and material usage.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thlu_br">
             <b>
              ThLU_BR
             </b>
             Rotterdam + Port
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod475" name="modifyThLU_BR" onclick="modsession(77,475)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thlu_br" title="Click to go to the Program at a Glance">
             <b>
              Lunch Break 4
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thint2s">
             <b>
              ThINT2S
             </b>
             Rotterdam + Port
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod476" name="modifyThINT2S" onclick="modsession(79,476)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thint2s" title="Click to go to the Program at a Glance">
             <b>
              Interactive Session 8
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_00">
             , Paper ThINT2S.0
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod477" name="modify53" onclick="modify(53,477)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('53'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Scalarizing Multi-Objective Robot Planning Problems Using Weighted Maximization
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#218541" title="Click to go to the Author Index">
             Wilde, Nils
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#116598" title="Click to go to the Author Index">
             Smith, Stephen L.
            </a>
           </td>
           <td class="r">
            University of Waterloo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#142433" title="Click to go to the Author Index">
             Alonso-Mora, Javier
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab53" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#motion_and_path_planning" title="Click to go to the Keyword Index">
               Motion and Path Planning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             When designing a motion planner for autonomous robots there are usually multiple objectives to be considered. However, a cost function that yields the desired trade-off between objectives is not easily obtainable. A common technique across many applications is to use a weighted sum of relevant objective functions and then carefully adapt the weights. However, this approach may not find all relevant trade-offs even in simple planning problems. Thus, we study an alternative method based on a weighted maximum of objectives. Such a cost function is more expressive than the weighted sum, and we show how it can be deployed in both continuous- and discrete-space motion planning problems. We propose a novel path planning algorithm for the proposed cost function and establish its correctness, and present heuristic adaptations that yield a practical runtime. In extensive simulation experiments, we demonstrate that the proposed cost function and algorithm are able to find a wider range of trade-offs between objectives (i.e., Pareto-optimal solutions) for various planning problems, showcasing its advantages in practice.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_01">
             14:15-15:00, Paper ThINT2S.1
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod478" name="modify2" onclick="modify(2,478)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('2'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Run to the Source: Effective Reproducibility of Robotics Code Repositories
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#102932" title="Click to go to the Author Index">
             Cervera, Enric
            </a>
           </td>
           <td class="r">
            Jaume-I University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab2" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#software__middleware_and_programming_environments" title="Click to go to the Keyword Index">
               Software, Middleware and Programming Environments
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#software_tools_for_benchmarking_and_reproducibility" title="Click to go to the Keyword Index">
               Software Tools for Benchmarking and Reproducibility
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#software_tools_for_robot_programming" title="Click to go to the Keyword Index">
               Software Tools for Robot Programming
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In recent years the robotics community has actively embraced the open paradigm, and research articles are commonly enriched with the inclusion of a source code repository of software. However, the reproducibility of such code is not straightforward, and it may become increasingly difficult with the evolution of software. There is a need for providing not only the source code but also an executable version with all the necessary library dependencies. A solution based on software containers is presented in this paper, with some unique advantages: first, the executable package is automatically generated from the last version of the source code; second, it is archived in the same cloud service that hosts the code repository; third, it integrates seamlessly with the development workflow of the research code; finally, it does not consume any local computing resources from the researcher. The executable code can then be downloaded and run by other users, with the only requirement of installing a specific software for running containers. This paper presents the complete workflow, which is then applied to some illustrative examples of source code repositories of articles published at robotics conferences.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_02">
             14:15-15:00, Paper ThINT2S.2
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod479" name="modify10" onclick="modify(10,479)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('10'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Embodied Intelligence: Bionic Robot Controller Integrating Environment Perception, Autonomous Planning, and Motion Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#150013" title="Click to go to the Author Index">
             GAN, Yahui
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#384387" title="Click to go to the Author Index">
             Zhang, Bo
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#347626" title="Click to go to the Author Index">
             Shao, Jiawei
            </a>
           </td>
           <td class="r">
            Southeast University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#385172" title="Click to go to the Author Index">
             Han, Zao
            </a>
           </td>
           <td class="r">
            Nanjing Yingqi Intelligent Technology Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#385171" title="Click to go to the Author Index">
             Li, Ang
            </a>
           </td>
           <td class="r">
            Nanjing Yingqi Intelligent Technology Co., Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#140675" title="Click to go to the Author Index">
             Dai, Xianzhong
            </a>
           </td>
           <td class="r">
            South-East University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab10" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#task_planning" title="Click to go to the Keyword Index">
               Task Planning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#cooperating_robots" title="Click to go to the Keyword Index">
               Cooperating Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper proposes a bionic robot controller equipped with intelligent perception and autonomous motion planning modules to address the manufacturing industry's requirements for small-batch, customized, and autonomous task. The controller integrates three key components: robot motion control module, vision perception module, and autonomous planning module. Taking multi-object rearrangement problem as an example, a dual-robot collaborative system is established for validation of the controller. The controller employs a stereoscopic vision algorithm for object recognition and localization. Leveraging the task sequence planning and RRT-Growth-Angle algorithm improved in this paper, the controller autonomously plans collision-free trajectories for the robots, facilitating their movement from the starting point to the grasping position and further to the placement location. The motion control algorithm collaboratively controls dual-robot, ensuring the precise and stable rearrangement of multiple objects into predefined positions. The results affirm that the bionic robot controller effectively mimics the three essential components of human-like functionality—perceiving, pondering, and acting—enabling it to autonomously and intelligently tackle complex tasks, which verifies the viability of the method.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_03">
             14:15-15:00, Paper ThINT2S.3
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod480" name="modify25" onclick="modify(25,480)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('25'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Speed Planning Based on Terrain-Aware Constraint Reinforcement Learning in Rugged Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#316985" title="Click to go to the Author Index">
             Yang, Andong
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#284456" title="Click to go to the Author Index">
             Li, Wei
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology, Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#207125" title="Click to go to the Author Index">
             Hu, Yu
            </a>
           </td>
           <td class="r">
            Institute of Computing Technology Chinese Academy of Sciences
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab25" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Speed planning in rugged terrain poses challenges due to various constraints, such as traverse efficiency, dynamics, safety, and smoothness. This paper introduces a framework based on Constrained Reinforcement Learning (CRL) that considers all these constraints. In addition, extracting the terrain information as a constraint to be added to the CRL is also a barrier. In this paper, a terrain constraint extraction module is designed to quantify the semantic and geometric attributes of the terrain by estimating maximum safe speed. All networks are trained on simulators or datasets and eventually deployed on a real mobile robot. To continuously improve the planning performance and mitigate the error caused by the simulator-reality gap, we propose a feedback structure for detecting and preserving critical experiences during the testing process. The experiments in the simulator and the real robot demonstrate that our method can reduce the frequency of dangerous status by 45% and improve up to 71% smoothness.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_04">
             14:15-15:00, Paper ThINT2S.4
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod481" name="modify111" onclick="modify(111,481)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('111'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning-Based Minimally-Sensed Fault-Tolerant Adaptive Flight Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#236434" title="Click to go to the Author Index">
             O'Connell, Michael
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#385254" title="Click to go to the Author Index">
             Cho, Joshua
            </a>
           </td>
           <td class="r">
            California Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#267936" title="Click to go to the Author Index">
             Anderson, Matthew
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#118282" title="Click to go to the Author Index">
             Chung, Soon-Jo
            </a>
           </td>
           <td class="r">
            Caltech
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab111" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#failure_detection_and_recovery" title="Click to go to the Keyword Index">
               Failure Detection and Recovery
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Many multirotor aircraft use redundant configurations to maintain control in the event of an actuator failure. Due to the redundancy of the system, fault isolation is inherently difficult and further compounded by complex interacting aerodynamics of the propellers, wings, and body. This paper presents a novel sparse failure identification and control correction method that does not require direct fault sensing, and instead utilizes only thev ehicle's dynamic response. The method couples an L1 regularized representation of the failure with a deep neural network to effectively isolate faults and improve tracking control in highly dynamic environments with unmodeled aerodynamic effects and unknown actuator failures. The method also includes a control re-allocation scheme which corrects for the identified faults while maximizing control authority and maintaining nominal performance characteristics. Experimental results demonstrate the method's ability to maintain control of a multirotor aircraft by isolating motor failures and reallocating control, improving position tracking by 48 % over the baseline. This paper contributes to the development of robust fault detection and control strategies for over-actuated aircraft.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_05">
             14:15-15:00, Paper ThINT2S.5
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod482" name="modify502" onclick="modify(502,482)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('502'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Enhanced Visual Under-Canopy Robotic Navigation Using mmWave Radar
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411484" title="Click to go to the Author Index">
             Mihigo, Aganze
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#293495" title="Click to go to the Author Index">
             Sivakumar, Arun Narenthiran
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#313807" title="Click to go to the Author Index">
             Sie, Emerson
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#331230" title="Click to go to the Author Index">
             Ngui, Isaac
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411486" title="Click to go to the Author Index">
             Kindratenko, Andrew
            </a>
           </td>
           <td class="r">
            University of Illinois Champaign-Urbana
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105498" title="Click to go to the Author Index">
             Morales, Marco
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign &amp; Instituto Tecnológico
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#133090" title="Click to go to the Author Index">
             Chowdhary, Girish
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#102662" title="Click to go to the Author Index">
             Amato, Nancy
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab502" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_and_automation_in_agriculture_and_forestry" title="Click to go to the Keyword Index">
               Robotics and Automation in Agriculture and Forestry
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose to explore the integration of millimeter-wave (mmWave) radar with camera-based systems to enhance under-canopy navigation for agricultural robots. While vision-based navigation has significantly improved, it still faces challenges in dense vegetation and adverse conditions. Our study aims to leverage mmWave radar's ability to operate in adverse weather conditions to complement systems like CropFollow++. We plan to collect synchronized radar and visual data, train models on radar-generated heatmaps to identify semantic keypoints, and integrate these predictions with visual navigation systems. By combining the strengths of both sensors, we aim to develop a more robust navigation system for challenging agricultural settings.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_06">
             14:15-15:00, Paper ThINT2S.6
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod483" name="modify47" onclick="modify(47,483)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('47'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Workspace Nonlinear Disturbance Observer for Robust Position Control of Flexible Joint Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#291840" title="Click to go to the Author Index">
             Lee, Deokjin
            </a>
           </td>
           <td class="r">
            Daegu Gyeongbuk Institute of Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#238340" title="Click to go to the Author Index">
             Back, Juhoon
            </a>
           </td>
           <td class="r">
            Kwangwoon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#129982" title="Click to go to the Author Index">
             Oh, Sehoon
            </a>
           </td>
           <td class="r">
            DGIST
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab47" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#flexible_robotics" title="Click to go to the Keyword Index">
               Flexible Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#robust_adaptive_control" title="Click to go to the Keyword Index">
               Robust/Adaptive Control
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Control of tasks by Flexible Joint Robots (FJR) is highly susceptible to disturbances, coupling, and vibrations that result from joint elasticity. A possible solution to this problem is a Disturbance Observer (DOB), which can address these issues in the joint space. However, this approach has limitations for workspace motion control, as the effect of nonlinear link dynamics and workspace coupling force is not considered. For these reasons, a novel DOB design for FJR in the workspace is deemed necessary, yet most research has focused on the joint DOB design. In this paper, a novel approach to modeling and control of the FJR in the workspace is proposed. A robust workspace nonlinear DOB for FJR is developed, taking into consideration the full dynamics of FJR as well as various disturbances that deteriorate the workspace position control, such as motor/link side disturbance, inertia variation, and workspace coupling force. In addition, the singularity problem that arises from modeling FJR for workspace DOB is accounted for by the proposed method. The effectiveness of the proposed method is demonstrated through simulations and experiments, and its stability is also verified.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_07">
             14:15-15:00, Paper ThINT2S.7
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod484" name="modify58" onclick="modify(58,484)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('58'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#309962" title="Click to go to the Author Index">
             Liu, Gaoyuan
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#277905" title="Click to go to the Author Index">
             De Winter, Joris
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#335336" title="Click to go to the Author Index">
             Durodié, Yuri
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#310312" title="Click to go to the Author Index">
             Steckelmacher, Denis
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#190419" title="Click to go to the Author Index">
             Nowé, Ann
            </a>
           </td>
           <td class="r">
            VUB
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101955" title="Click to go to the Author Index">
             Vanderborght, Bram
            </a>
           </td>
           <td class="r">
            Vrije Universiteit Brussel
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab58" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Task and motion planning (TAMP) for robotics manipulation necessitates long-horizon reasoning involving versatile actions and skills. While deterministic actions can be crafted by sampling or optimizing with certain constraints, planning actions with uncertainty, i.e., probabilistic actions, remains a challenge for TAMP. On the contrary, Reinforcement Learning (RL) excels in acquiring versatile, yet short-horizon, manipulation skills that are robust with uncertainties. In this letter, we design a method that integrates RL skills into TAMP pipelines. Besides the policy, a RL skill is defined with data-driven logical components that enable the skill to be deployed by symbolic planning. A plan refinement sub-routine is designed to further tackle the inevitable effect uncertainties. In the experiments, we compare our method with baseline hierarchical planning from both TAMP and RL fields and illustrate the strength of the method. The results show that by embedding RL skills, we extend the capability of TAMP to domains with probabilistic skills, and improve the planning efficiency compared to the previous methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_08">
             14:15-15:00, Paper ThINT2S.8
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod485" name="modify77" onclick="modify(77,485)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('77'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Identification and Control of Soft Robots Using the Koopman Operator
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#303140" title="Click to go to the Author Index">
             Dahdah, Steven
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#140525" title="Click to go to the Author Index">
             Forbes, James Richard
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#187525" title="Click to go to the Author Index">
             Vasudevan, Ram
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#205650" title="Click to go to the Author Index">
             Bruder, Daniel
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab77" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This submission summarizes recent research applying Koopman operator theory to the identification and control of soft robotic manipulators. While soft robots are uniquely suited to work in unstructured environments or alongside humans thanks to their natural compliance. This same compliance makes them difficult to model from first principles, which poses a significant control challenge and limits their practical deployment in these settings. Machine learning tools can help model these robots from data, but most methods do not lend themselves well to analysis and optimal control. The Koopman operator allows nonlinear systems to be represented as infinite-dimensional linear systems, which can then be approximated from data. The linearity of the Koopman operator allows modes to be analyzed, stability to be assessed, and controllers and observers to be designed using standard tools. The publications summarized experimentally demonstrate the suitability of the Koopman operator for modelling and controlling soft robots while highlighting the deep well of related research directions still unexplored.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_09">
             14:15-15:00, Paper ThINT2S.9
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod486" name="modify83" onclick="modify(83,486)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('83'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Comprehensive Gradient Computation Framework of PCS Model for Soft Robot Simulation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#273063" title="Click to go to the Author Index">
             Ishigaki, Taiki
            </a>
           </td>
           <td class="r">
            The University of Tokyo
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#114052" title="Click to go to the Author Index">
             Ayusawa, Ko
            </a>
           </td>
           <td class="r">
            National Institute of Advanced Industrial Science and Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#117307" title="Click to go to the Author Index">
             Yamamoto, Ko
            </a>
           </td>
           <td class="r">
            University of Tokyo
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab83" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#modeling__control__and_learning_for_soft_robots" title="Click to go to the Keyword Index">
               Modeling, Control, and Learning for Soft Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#dynamics" title="Click to go to the Keyword Index">
               Dynamics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#simulation_and_animation" title="Click to go to the Keyword Index">
               Simulation and Animation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Dynamic simulation of soft and flexible bodies is important for the motion planning and control of soft robot systems. This study presents a fast real-time simulation of the piecewise constant strain (PCS) model proposed in soft robotics research to calculate the dynamics of a beam or rod structure. We extend the theory of comprehensive motion transformation matrix (CMTM) to the PCS model, which allows us to systematically calculate the gradients in the equations of motion. Using the dynamic gradient, we perform a dynamic simulation of the PCS model via implicit integration. Compared to explicit integration, implicit integration allows us to use a larger time-step width for the integration without a flexible deformation divergence, which decreases the computational time. We present several examples of dynamic simulations, including a relatively rigid material such as carbon-fiber-reinforced plastics used in a leaf-spring-type sports prosthesis.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_10">
             14:15-15:00, Paper ThINT2S.10
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod487" name="modify105" onclick="modify(105,487)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('105'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Dual-Hemispherical Photometric Visual Servoing
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#179017" title="Click to go to the Author Index">
             Crombez, Nathan
            </a>
           </td>
           <td class="r">
            Université De Technologie De Belfort-Montbéliard
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#277973" title="Click to go to the Author Index">
             Buisson, Jocelyn
            </a>
           </td>
           <td class="r">
            Université De Technologie De Belfort Montbéliard
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#271001" title="Click to go to the Author Index">
             André, Antoine N.
            </a>
           </td>
           <td class="r">
            AIST
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#113960" title="Click to go to the Author Index">
             Caron, Guillaume
            </a>
           </td>
           <td class="r">
            CNRS
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab105" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#visual_servoing" title="Click to go to the Keyword Index">
               Visual Servoing
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#omnidirectional_vision" title="Click to go to the Keyword Index">
               Omnidirectional Vision
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             It is well established that wide field of view cameras, as well as using the whole photometric information contained in images, offer many advantages for visual servoing. Therefore, we propose to extend the photometric visual servoing to full spherical cameras. More precisely, we are dealing with 360-degree optical rigs composed of two wide-angle lenses oriented in opposite directions that capture everything around the device in one acquisition. The photometric visual feature coupled to dual-hemispherical acquisitions that contain the whole surrounding scene provide useful complementary information, showing large convergence domains, straighter camera trajectories than with a single hemispherical camera, and high accuracy. We report thorough simulations and several challenging real experiments using a 6 degrees-of-freedom robotic arm controlled from dual-hemispherical acquisitions.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_11">
             14:15-15:00, Paper ThINT2S.11
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod488" name="modify110" onclick="modify(110,488)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('110'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              On Efficient and Flexible Autonomous Robotic Insertion Assembly in the Presence of Uncertainty
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#321227" title="Click to go to the Author Index">
             Cao, Shichen
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101783" title="Click to go to the Author Index">
             Xiao, Jing
            </a>
           </td>
           <td class="r">
            Worcester Polytechnic Institute (WPI)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab110" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#compliant_assembly" title="Click to go to the Keyword Index">
               Compliant Assembly
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#contact_modeling" title="Click to go to the Keyword Index">
               Contact Modeling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a general approach for autonomous tight-clearance assembly of complex-shaped parts using a general-purpose robot manipulator equipped with a force/torque (F/T) sensor. Such autonomous assembly is challenging in the presence of relative part pose uncertainty due to inaccuracies in part and robot modeling, sensing and
             <p>
              perception, and robot motion control, especially when this uncertainty exceeds the clearance between assembly components. Our approach uses a geometry-invariant representation of each assembly part and integrates a learned uncertainty prediction model that is general and independent of specific parts into search and optimization to efficiently determine
              <p>
               the best estimate of any contact configuration caused by misalignment between assembly parts and to ensure successful assembly. Our approach allows the manipulator to autonomously pick and assemble rigid parts at
               <p>
                different workspace locations and accommodates different insertion directions from pickup directions. Our extensive experimental results demonstrate the effectiveness and efficiency of this approach in achieving successful assemblies of complex parts even though the pose uncertainty can be more than 10 times the task clearance between the assembled parts.
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_12">
             14:15-15:00, Paper ThINT2S.12
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod489" name="modify116" onclick="modify(116,489)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Structural Design of PEDOT: PSS Toward Fabricating Multi-Stimuli-Responsive Soft Actuator
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#407155" title="Click to go to the Author Index">
             Yousefian, Hatef
            </a>
           </td>
           <td class="r">
            The University of British Columbia (UBC)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#407627" title="Click to go to the Author Index">
             Isari, Ali Akbar
            </a>
           </td>
           <td class="r">
            The University of British Columbia (UBC)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#407625" title="Click to go to the Author Index">
             Ghaffarkhah, Ahmadreza
            </a>
           </td>
           <td class="r">
            The University of British Columbia (UBC)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#407695" title="Click to go to the Author Index">
             Arjmand, Mohammad
            </a>
           </td>
           <td class="r">
            The University of British Columbia (UBC)
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_13">
             14:15-15:00, Paper ThINT2S.13
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod490" name="modify132" onclick="modify(132,490)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('132'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Material Scrunching Enables Working Channels in Miniaturized Vine-Inspired Robots
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#209776" title="Click to go to the Author Index">
             Girerd, Cedric
            </a>
           </td>
           <td class="r">
            LIRMM, Univ Montpellier, CNRS, Montpellier, France
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#325742" title="Click to go to the Author Index">
             Alvarez, Anna
            </a>
           </td>
           <td class="r">
            University of California Santa Barbara
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#114825" title="Click to go to the Author Index">
             Hawkes, Elliot Wright
            </a>
           </td>
           <td class="r">
            University of California, Santa Barbara
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#187439" title="Click to go to the Author Index">
             Morimoto, Tania K.
            </a>
           </td>
           <td class="r">
            University of California San Diego
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab132" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#surgical_robotics__steerable_catheters_needles" title="Click to go to the Keyword Index">
               Surgical Robotics: Steerable Catheters/Needles
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#biologically_inspired_robots" title="Click to go to the Keyword Index">
               Biologically-Inspired Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#medical_robots_and_systems" title="Click to go to the Keyword Index">
               Medical Robots and Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A new subclass of soft robot, known as tip-extending or ``vine'' robots, consists of long inflatable devices that move through the environment by extending from the tip. A key requirement for many applications of these robots is a working channel - a hollow tube through the core of the robot for passing tools, sensors, fluids, etc. While working channels have been proposed in a few vine robots, it remains an open challenge to create miniaturized vine robots (diameter &lt; 1 cm) with working channels that enable continuous access through the core. In this paper, we analyze the growth models of current vine robot designs and show that the working channel greatly increases required pressure to grow at small scales due to internal friction. Based on this insight, we propose the concept of storing scrunched material at the tip of the vine robot to circumvent this frictional force. We validate our models and demonstrate this concept via prototypes down to diameters of 2.3 mm. Overall, this work enables the creation of miniaturized vine robots with working channels, which significantly enhances their practicality and potential for impact in applications such as minimally invasive surgery.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_14">
             14:15-15:00, Paper ThINT2S.14
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod491" name="modify154" onclick="modify(154,491)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('154'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Logic Learning from Demonstrations for Multi-Step Manipulation Tasks in Dynamic Environments
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#290156" title="Click to go to the Author Index">
             Zhang, Yan
            </a>
           </td>
           <td class="r">
            Idiap Research Institute; EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#246669" title="Click to go to the Author Index">
             Xue, Teng
            </a>
           </td>
           <td class="r">
            Idiap Research Institute and EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#307828" title="Click to go to the Author Index">
             Razmjoo, Amirreza
            </a>
           </td>
           <td class="r">
            Idiap Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107089" title="Click to go to the Author Index">
             Calinon, Sylvain
            </a>
           </td>
           <td class="r">
            Idiap Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab154" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Learning from Demonstration (LfD) stands as an efficient framework for imparting human-like skills to robots. Nevertheless, designing an LfD framework capable of seamlessly imitating, generalizing, and reacting to disturbances for long-horizon manipulation tasks in dynamic environments remains a challenge. To tackle this challenge, we present Logic Dynamic Movement Primitives (Logic-DMP), which combines Task and Motion Planning (TAMP) with an optimal control formulation of DMP, allowing us to incorporate motion-level via-point specifications and to handle task-level variations or disturbances in dynamic environments. We conduct a comparative analysis of our proposed approach against several baselines, evaluating its generalization ability and reactivity across three long-horizon manipulation tasks. Our experiment demonstrates the fast generalization and reactivity of Logic- DMP for handling task-level variants and disturbances in long- horizon manipulation tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_15">
             14:15-15:00, Paper ThINT2S.15
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod492" name="modify160" onclick="modify(160,492)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('160'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Impact Robustness vs. Torque Bandwidth: A Design Guide for Differential Elastic Actuators
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#305412" title="Click to go to the Author Index">
             Shu, Anton Leonhard
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#296192" title="Click to go to the Author Index">
             Raschel, Clara Maria
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#183929" title="Click to go to the Author Index">
             Keppler, Manuel
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#139453" title="Click to go to the Author Index">
             Wedler, Armin
            </a>
           </td>
           <td class="r">
            DLR - German Aerospace Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#116285" title="Click to go to the Author Index">
             Görner, Martin
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab160" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#compliant_joint_mechanism" title="Click to go to the Keyword Index">
               Compliant Joint/Mechanism
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#mechanism_design" title="Click to go to the Keyword Index">
               Mechanism Design
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#compliance_and_impedance_control" title="Click to go to the Keyword Index">
               Compliance and Impedance Control
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#differential_elastic_actuator" title="Click to go to the Keyword Index">
               Differential Elastic Actuator
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Differential Elastic Actuators (DEAs) connect a motor and a spring via differential gears to a shared output shaft, offering a more compact solution for creating mechanical robust systems than Series Elastic Actuators (SEAs), with superior torque transmission at high frequencies. The key to maximizing DEA performance lies in the careful selection of stiffness, inertia, and damping values to meet specific requirements for performance and durability. We introduce a DEA design guide that utilizes open-loop torque-bandwidth for performance evaluation and the magnitude of impact-induced gear torque for robustness evaluation. This approach enables determining DEA parameters using closed-form equations, eliminating the need for simulations or extensive expert knowledge. The effectiveness of our method is confirmed through experiments with a re-configurable DEA prototype.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_16">
             14:15-15:00, Paper ThINT2S.16
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod493" name="modify169" onclick="modify(169,493)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('169'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Gafro: Geometric Algebra for Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#226444" title="Click to go to the Author Index">
             Löw, Tobias
            </a>
           </td>
           <td class="r">
            Idiap Research Institute, EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#306727" title="Click to go to the Author Index">
             Abbet, Philip
            </a>
           </td>
           <td class="r">
            Idiap Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107089" title="Click to go to the Author Index">
             Calinon, Sylvain
            </a>
           </td>
           <td class="r">
            Idiap Research Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab169" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#software_tools_for_robot_programming" title="Click to go to the Keyword Index">
               Software Tools for Robot Programming
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#optimization_and_optimal_control" title="Click to go to the Keyword Index">
               Optimization and Optimal Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Geometry is a fundamental part of robotics and there have been various frameworks of representation over the years. Recently, geometric algebra has gained attention for its property of unifying many of those previous ideas into one algebra. While there are already efficient open-source implementations of geometric algebra available, none of them is targeted at robotics applications. We want to address this shortcoming with our library gafro. This article presents an overview of the implementation details as well as a tutorial of gafro, an efficient c++ library targeting robotics applications using geometric algebra. The library focuses on using conformal geometric algebra. Hence, various geometric primitives are available for computation as well as rigid body transformations. The modeling of robotic systems is also an important aspect of the library. It implements various algorithms for calculating the kinematics and dynamics of such systems as well as objectives for optimisation problems. The software stack is completed by python bindings in pygafro and a ROS interface in gafro_ros}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_17">
             14:15-15:00, Paper ThINT2S.17
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod494" name="modify179" onclick="modify(179,494)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('179'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Design of a Variable Curvature Soft Continuum Robot with Fast Continuous Stiffness Adjustment
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#315409" title="Click to go to the Author Index">
             Wang, Xinran
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#409683" title="Click to go to the Author Index">
             Mandal, Naini
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#408942" title="Click to go to the Author Index">
             Howells, James
            </a>
           </td>
           <td class="r">
            Imperial College London
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#138488" title="Click to go to the Author Index">
             Rojas, Nicolas
            </a>
           </td>
           <td class="r">
            The AI Institute
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab179" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_applications" title="Click to go to the Keyword Index">
               Soft Robot Applications
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#dexterous_manipulation" title="Click to go to the Keyword Index">
               Dexterous Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Variable curvature control of soft continuum robots is an open challenge. We propose a novel method that involves a variable stiffness growing spine to control the bending curvature of a soft continuum robot. This design enriches the workspace of the continuum robots and provides more usability by adjusting stiffness continuously within 1 second. This work sets the stage for future developments in the field, enabling the creation of highly flexible and responsive variable curvature soft continuum robots. Experiments have been performed to validate the accuracy of the growing spine and demonstrate the robot’s ability to achieve variable curvatures.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_18">
             14:15-15:00, Paper ThINT2S.18
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod495" name="modify182" onclick="modify(182,495)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('182'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-Modal MPPI and Active Inference for Reactive Task and Motion Planning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#385000" title="Click to go to the Author Index">
             Zhang, Yuezhe
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#257078" title="Click to go to the Author Index">
             Pezzato, Corrado
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#309938" title="Click to go to the Author Index">
             Trevisan, Elia
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#310551" title="Click to go to the Author Index">
             Salmi, Chadi
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#122669" title="Click to go to the Author Index">
             Hernández, Carlos
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#142433" title="Click to go to the Author Index">
             Alonso-Mora, Javier
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab182" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#task_and_motion_planning" title="Click to go to the Keyword Index">
               Task and Motion Planning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#manipulation_planning" title="Click to go to the Keyword Index">
               Manipulation Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Task and Motion Planning (TAMP) has made strides in complex manipulation tasks, yet the execution robustness of the planned solutions remains overlooked. In this work, we propose a method for reactive TAMP to cope with runtime uncertainties and disturbances. We combine an Active Inference planner (AIP) for adaptive high-level action selection and a novel Multi-Modal Model Predictive Path Integral controller (M3P2I) for low-level control. This results in a scheme that simultaneously adapts both high-level actions and low-level motions. The AIP generates alternative symbolic plans, each linked to a cost function for M3P2I. The latter employs a physics simulator for diverse trajectory rollouts, deriving optimal control by weighing the different samples according to their cost. This idea enables blending different robot skills for fluid and reactive plan execution, accommodating plan adjustments at both the high and low levels to cope, for instance, with dynamic obstacles or disturbances that invalidate the current plan. We have tested our approach in simulations and real-world scenarios.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_19">
             14:15-15:00, Paper ThINT2S.19
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod496" name="modify512" onclick="modify(512,496)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('512'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Using Classical Vision and 3-D Registration for a Successful Pose Estimation of a Bell Pepper
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#367308" title="Click to go to the Author Index">
             De Los Rios Alatorre, Gustavo
            </a>
           </td>
           <td class="r">
            ITESM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#399058" title="Click to go to the Author Index">
             Nieto Gutierrez, Nezih
            </a>
           </td>
           <td class="r">
            Tecnológico De Monterrey
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#396938" title="Click to go to the Author Index">
             Murra López, Arturo José
            </a>
           </td>
           <td class="r">
            Tecnológico De Monterrey
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#110518" title="Click to go to the Author Index">
             Munoz, Luis Alberto
            </a>
           </td>
           <td class="r">
            The Robotics Institute of Yucatán
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411501" title="Click to go to the Author Index">
             Mendez Meraz, Armando Enrico
            </a>
           </td>
           <td class="r">
            ITESM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411502" title="Click to go to the Author Index">
             Duran, Ian
            </a>
           </td>
           <td class="r">
            ITESM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411504" title="Click to go to the Author Index">
             Serna, Cesar
            </a>
           </td>
           <td class="r">
            ITESM
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#158522" title="Click to go to the Author Index">
             ESCOBEDO-CABELLO, Jesus-Arturo
            </a>
           </td>
           <td class="r">
            INRIA Rhone-Alpes
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab512" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a robust computer visionbased method for the detection and 3D pose estimation of semi-deformable objects, a crucial challenge in robotics due to their non-standardized characteristics and high susceptibility to occlusions and viewpoint changes. Our solution aims to empower vision-based robotics systems by strengthening their understanding of these objects. Initially developed on a Windows 11 system, the method was adapted to Ubuntu enabling deployment on a Jetson Nano 2GB and testing in ROS2 Gazebo simulations. The system demonstrated consistent performance across different platforms and sensors, including the Azure Kinect DK and a simulated Intel D435-i. We specifically tested the ability of the system to accurately estimate the 3D pose of bell peppers in different angles, showcasing its potential for practical applications, such as robotic harvesting.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_20">
             14:15-15:00, Paper ThINT2S.20
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod497" name="modify187" onclick="modify(187,497)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('187'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Hilti SLAM Challenge 2023: Benchmarking Single + Multi-Session SLAM across Sensor Constellations in Construction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#359909" title="Click to go to the Author Index">
             Nair, Ashish Devadas
            </a>
           </td>
           <td class="r">
            Hilti Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#275322" title="Click to go to the Author Index">
             Kindle, Julien
            </a>
           </td>
           <td class="r">
            ETH Zurich
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#403777" title="Click to go to the Author Index">
             Levchev, Plamen
            </a>
           </td>
           <td class="r">
            Hilti
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105662" title="Click to go to the Author Index">
             Scaramuzza, Davide
            </a>
           </td>
           <td class="r">
            University of Zurich
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab187" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#data_sets_for_slam" title="Click to go to the Keyword Index">
               Data Sets for SLAM
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#slam" title="Click to go to the Keyword Index">
               SLAM
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#performance_evaluation_and_benchmarking" title="Click to go to the Keyword Index">
               Performance Evaluation and Benchmarking
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Simultaneous Localization and Mapping systems are a key enabler for positioning in both handheld and robotic applications. The Hilti SLAM Challenges organized over the past years have been successful at benchmarking some of the world’s best SLAM Systems with high accuracy. However, more capabilities of these systems are yet to be explored, such as platform agnosticism across varying sensor suites and multisession SLAM. These factors indirectly serve as an indicator of robustness and ease of deployment in real-world applications. There exists no dataset plus benchmark combination publicly available, which considers these factors combined. The Hilti SLAM Challenge 2023 Dataset and Benchmark addresses this issue. Additionally, we propose a novel fiducial marker design for a pre-surveyed point on the ground to be observable from an off-the-shelf LiDAR mounted on a robot, and an algorithm to estimate its position at mm-level accuracy. Results from the challenge show an increase in overall participation, single-session SLAM systems getting increasingly accurate, successfully operating across varying sensor suites, but relatively few participants performing multi-session SLAM.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_21">
             14:15-15:00, Paper ThINT2S.21
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod498" name="modify203" onclick="modify(203,498)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('203'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              On the Role of the Action Space in Robot Manipulation Learning and Sim-To-Real Transfer
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#291311" title="Click to go to the Author Index">
             Aljalbout, Elie
            </a>
           </td>
           <td class="r">
            Volkswagen AG
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#240056" title="Click to go to the Author Index">
             Frank, Felix
            </a>
           </td>
           <td class="r">
            Volkswagen Group
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#175184" title="Click to go to the Author Index">
             Karl, Maximilian
            </a>
           </td>
           <td class="r">
            Volkswagen AG
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107642" title="Click to go to the Author Index">
             van der Smagt, Patrick
            </a>
           </td>
           <td class="r">
            Volkswagen Group
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab203" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We study the choice of action space in robot manipulation learning and sim-to-real transfer. We define metrics that assess the performance, and examine the emerging properties in the different action spaces. We train over 250 reinforcement learning~(RL) agents in simulated reaching and pushing tasks, using 13 different control spaces. The choice of spaces spans combinations of common action space design characteristics. We evaluate the training performance in simulation and the transfer to a real-world environment. We identify good and bad characteristics of robotic action spaces and make recommendations for future designs. Our findings have important implications for the design of RL algorithms for robot manipulation tasks, and highlight the need for careful consideration of action spaces when training and transferring RL agents for real-world robotics.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_22">
             14:15-15:00, Paper ThINT2S.22
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod499" name="modify204" onclick="modify(204,499)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('204'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Assistive Robotic Manipulation with Scalable Autonomy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#228941" title="Click to go to the Author Index">
             Quere, Gabriel
            </a>
           </td>
           <td class="r">
            DLR
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#217914" title="Click to go to the Author Index">
             Iskandar, Maged
            </a>
           </td>
           <td class="r">
            German Aerospace Center - DLR
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410330" title="Click to go to the Author Index">
             Spielmann, Simon
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410331" title="Click to go to the Author Index">
             Welser, Miriam
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#256405" title="Click to go to the Author Index">
             Bustamante, Samuel
            </a>
           </td>
           <td class="r">
            German Aeroespace Center (DLR), Robotics and Mechatronics Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#367214" title="Click to go to the Author Index">
             Jung, Sebastian
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410332" title="Click to go to the Author Index">
             Schiel, Felix
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#204572" title="Click to go to the Author Index">
             Hagengruber, Annette
            </a>
           </td>
           <td class="r">
            German Aerospace Center
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#137731" title="Click to go to the Author Index">
             Vogel, Jörn
            </a>
           </td>
           <td class="r">
            German Aerospace Center (DLR)
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab204" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#rehabilitation_robotics" title="Click to go to the Keyword Index">
               Rehabilitation Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#physically_assistive_devices" title="Click to go to the Keyword Index">
               Physically Assistive Devices
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#mobile_manipulation" title="Click to go to the Keyword Index">
               Mobile Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Mobile manipulation aids enable people with physical disabilities to manipulate their environment. However, controlling a robotic manipulator with many degrees of freedom is challenging, especially with a low-dimensional interface such as a 3D joystick. In this video we present our solution for more self-reliant control of wheelchair-mounted manipulators. In our robotic system EDAN, we integrated scalable autonomy, a holistic approach which provides manipulation assistance by offering three levels of autonomy: "direct control", "shared control" and "supervised autonomy". In addition, we coordinate the movement of the arm with that of the wheelchair, increasing the reachability of the arm for tasks that require a large range of motion. This video demonstrates the capabilities of our system in a realistic home-like environment. A series of tasks are performed with varying levels of autonomy, allowing intuitive operation at the user's preferred pace. The demonstration in this video is with an unimpaired user. Future work will include evaluation with users from the target group.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_23">
             14:15-15:00, Paper ThINT2S.23
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod500" name="modify224" onclick="modify(224,500)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('224'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robot Painting: Art for Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#339311" title="Click to go to the Author Index">
             Schaldenbrand, Peter
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#218263" title="Click to go to the Author Index">
             Chen, Gerry
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#398871" title="Click to go to the Author Index">
             Misra, Vihaan
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410520" title="Click to go to the Author Index">
             Chen, Yunhao Lorie
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107102" title="Click to go to the Author Index">
             Goldberg, Ken
            </a>
           </td>
           <td class="r">
            UC Berkeley
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#179403" title="Click to go to the Author Index">
             Oh, Jean
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab224" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#art_and_entertainment_robotics" title="Click to go to the Keyword Index">
               Art and Entertainment Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This standalone video presents a brief history of robot painting projects with the intention of educating viewers about the specific, core robotics challenges that people developing robot painters face. We focus on four robotics challenges: controls, the simulation-to-reality gap, generative intelligence, and human-robot interaction. We show how various projects tackle these challenges with quotes from experts in the field. Video clips from projects, such as Harold Cohen's AARON robot in 1968, to Ken Goldberg's robot art in 1990, through the FRIDA robot's contemporary artwork, and many more, walk viewers through the history of robot painting while highlighting contributions made in the field of robotics. We demonstrate through entertaining videos that robot painting is not a toy domain, it touches upon all core robotics challenges, making it not only one of the more enjoyable spaces to work, given its aesthetic nature, but fruitful in terms of possibilities to make great contributions to general robotics and artificial intelligence research.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_24">
             14:15-15:00, Paper ThINT2S.24
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod501" name="modify231" onclick="modify(231,501)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('231'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Tractable Bayesian Dynamics Priors from Differentiable Physics for Learning and Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#291140" title="Click to go to the Author Index">
             Watson, Joe
            </a>
           </td>
           <td class="r">
            Technical University Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#313079" title="Click to go to the Author Index">
             Hahner, Benedikt
            </a>
           </td>
           <td class="r">
            TU Darmstadt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#246352" title="Click to go to the Author Index">
             Belousov, Boris
            </a>
           </td>
           <td class="r">
            German Research Center for Artificial Intelligence - DFKI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#104326" title="Click to go to the Author Index">
             Peters, Jan
            </a>
           </td>
           <td class="r">
            Technische Universität Darmstadt
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab231" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#reinforcement_learning" title="Click to go to the Keyword Index">
               Reinforcement Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#model_learning_for_control" title="Click to go to the Keyword Index">
               Model Learning for Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Statistical model-based reinforcement learning methods should enable efficient data-driven policy optimization for robotic systems. However, the success of these approaches relies on how well the learned dynamics model generalizes outside of its training data distribution, which is difficult to ensure in practice with `black-box' models unless inductive biases are incorporated. We demonstrate how a differentiable simulation model can be used to synthesize a tractable Gaussian process prior using the linearized Laplace approximation, a principled approximate inference technique for Gaussian posteriors. Using this statistical model as an inductive bias, we can perform exploration and decision-making in an informed way using posterior sampling, performing reinforcement learning for control, and active learning for system identification. In the case of simulation-to-reality mismatch, we empirically investigate how this physics-informed prior is best used, comparing architecture- and objective-based approaches.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_25">
             14:15-15:00, Paper ThINT2S.25
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod502" name="modify276" onclick="modify(276,502)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('276'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic Intervention for Water Leak Search in North Area at CERN
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#193696" title="Click to go to the Author Index">
             Buonocore, Luca Rosario
            </a>
           </td>
           <td class="r">
            CERN
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#223665" title="Click to go to the Author Index">
             Veiga Almagro, Carlos
            </a>
           </td>
           <td class="r">
            CERN
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#374549" title="Click to go to the Author Index">
             Rodriguez-Nogueira, Jose
            </a>
           </td>
           <td class="r">
            CERN
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410621" title="Click to go to the Author Index">
             Romagnoli, Enrico
            </a>
           </td>
           <td class="r">
            CERN
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410620" title="Click to go to the Author Index">
             Lendaro, Jerome
            </a>
           </td>
           <td class="r">
            CERN
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#184518" title="Click to go to the Author Index">
             DI CASTRO, Mario
            </a>
           </td>
           <td class="r">
            CERN, European Organization for Nuclear Research
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab276" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#robotics_in_hazardous_fields" title="Click to go to the Keyword Index">
               Robotics in Hazardous Fields
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The CERNBot_NA is a specialized robotic platform permanently stationed in European Organization for Nuclear Research's (CERN) North Area experimental cavern, developed for emergency inspection and maintenance tasks. Unlike other systems like the TIM in the Large Hadron Collider (LHC) and MIRA in the Super Proton Synchrotron (SPS), CERNBot_NA's charging station is located on the surface, necessitating its capability to operate elevators to access the underground area. This robot, part of the omnidirectional CERNBot family, was deployed to address a detected water leak in the cooling system. The rapid response, within 20 minutes, involved the robot descending via an elevator to perform a visual inspection, crucial for confirming the issue and planning preventive measures. Teleoperated from a control room, the robot provides comprehensive environmental feedback through onboard and external cameras, ensuring full operational awareness. In the absence of external cameras, proprioception is maintained via a Human-Robot Interface (HRI). Once in the area, an automatic guillotine door allowed the robot to enter. It is equipped with sensors for dose rate measurement and lighting, enabling operation even in dark conditions. Such robotic interventions are vital in minimizing system downtime and improving machine availability, significantly reducing intervention time and enhancing safety by allowing immediate responses to detected faults.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_26">
             14:15-15:00, Paper ThINT2S.26
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod503" name="modify279" onclick="modify(279,503)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('279'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              An Earth Rover Dataset Recorded at the ICRA@40 Party
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#333339" title="Click to go to the Author Index">
             Zhang, Qi
            </a>
           </td>
           <td class="r">
            University of Glasgow
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#365629" title="Click to go to the Author Index">
             Lin, Zhihao
            </a>
           </td>
           <td class="r">
            University of Glasgow
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#113692" title="Click to go to the Author Index">
             Visser, Arnoud
            </a>
           </td>
           <td class="r">
            Universiteit Van Amsterdam
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab279" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#data_sets_for_robotic_vision" title="Click to go to the Keyword Index">
               Data Sets for Robotic Vision
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The ICRA conference is celebrating its 40th anniversary in Rotterdam in September 2024, with as highlight the Happy Birthday ICRA Party at the iconic Holland America Line Cruise Terminal. One month later the IROS conference will take place, which will include the Earth Rover Challenge. In this challenge open-world autonomous navigation models are studied truly open-world settings.
             <p>
              As part of the Earth Rover Challenge several real-world navigation sets in several cities world-wide, like Auckland, Australia and Wuhan, China. The only dataset recorded in the Netherlands is the small village Oudewater. The proposal is to record a dataset with the robot used in the Earth Rover Challenge in Rotterdam, in front of the Holland America Line Cruise Terminal, before the festivities of the Happy Birthday ICRA Party start.
              <p>
               See:
               <a href="https://github.com/SlamMate/vSLAM-on-FrodoBots-2K">
                https://github.com/SlamMate/vSLAM-on-FrodoBots-2K
               </a>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_27">
             14:15-15:00, Paper ThINT2S.27
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod504" name="modify291" onclick="modify(291,504)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('291'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Interdimensional Knowledge Transfer for Semantic Segmentation on LiDAR Point Clouds
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#393261" title="Click to go to the Author Index">
             HA, SeongHeon
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#393260" title="Click to go to the Author Index">
             Kim, YeoGyeong
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#240968" title="Click to go to the Author Index">
             Park, Jinsun
            </a>
           </td>
           <td class="r">
            Pusan National University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab291" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#object_detection__segmentation_and_categorization" title="Click to go to the Keyword Index">
               Object Detection, Segmentation and Categorization
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#ai_based_methods" title="Click to go to the Keyword Index">
               AI-Based Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             3D semantic segmentation is a task to classify a set of 3D points in a scene into semantically meaningful objects. Most existing approaches solely rely on single-domain information such as point clouds acquired by LiDAR sensors. However, the amount of information acquired from a single domain information is inherently limited. The limitation can be alleviated by complementary knowledge exchange between multi-modal sensors such as RGB and LiDAR sensors, but this leads to increased computational complexity inevitably. To address the problem, we propose a novel Interdimensional Alignment Module (IAM) for knowledge transfer between 2D and 3D information. The proposed IAM aims to exploit multi-modal information during the training by combining complementary 2D and 3D information based on knowledge distillation with a teacher-student structure while utilizing single-domain information during the inference stage to keep the computational cost low. Specifically, the teacher network utilizes 3D point clouds projected onto the paired 2D image to extract geometrically aligned key-value pairs from each modality. After that, the extracted global context vectors are exchanged between modalities to generate cross-modal attention-based 2D-3D fused features. The prediction from the fused features is adopted as unidirectional guidance to the lightweight student network, therefore, only 3D information is required for the inference. We also design a FocalKLD loss to effectively deal with the class imbalance problem during the distillation. This leads to significantly improved performance compared to previous single-domain segmentation methods. Experimental results on SemanticKITTI and nuScenes datasets show that our model is effective and superior to previous methods.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_28">
             14:15-15:00, Paper ThINT2S.28
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod505" name="modify298" onclick="modify(298,505)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('298'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Multi-LED Classification As Pretext for Robot Heading Estimation
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#374333" title="Click to go to the Author Index">
             Carlotti, Nicholas
            </a>
           </td>
           <td class="r">
            Dalle Molle Institute for Artificial Intelligence (IDSIA)
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#235964" title="Click to go to the Author Index">
             Nava, Mirko
            </a>
           </td>
           <td class="r">
            IDSIA
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#155801" title="Click to go to the Author Index">
             Giusti, Alessandro
            </a>
           </td>
           <td class="r">
            IDSIA USI-SUPSI
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab298" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#multi_robot_systems" title="Click to go to the Keyword Index">
               Multi-Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a self-supervised approach for visual robot detection and heading estimation by learning to estimate the states (OFF or ON) of four independent robot-mounted LEDs. Experimental results show a median image-space position error of 14 px and relative heading MAE of 17◦, versus a supervised upperbound scoring 10 px and 8◦, respectively.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_29">
             14:15-15:00, Paper ThINT2S.29
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod506" name="modify311" onclick="modify(311,506)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Generalization of Task Parameterized Dynamical Systems Using Gaussian Process Transportation
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#269151" title="Click to go to the Author Index">
             Franzese, Giovanni
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#204329" title="Click to go to the Author Index">
             Prakash, Ravi
            </a>
           </td>
           <td class="r">
            Indian Institute of Science
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#179240" title="Click to go to the Author Index">
             Della Santina, Cosimo
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#117613" title="Click to go to the Author Index">
             Kober, Jens
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_30">
             14:15-15:00, Paper ThINT2S.30
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod507" name="modify318" onclick="modify(318,507)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('318'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task Learning Using Actionable Knowledge Graphs
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#297951" title="Click to go to the Author Index">
             Kümpel, Michaela
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#325299" title="Click to go to the Author Index">
             Vyas, Abhijit
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#372601" title="Click to go to the Author Index">
             Hassouna, Vanessa
            </a>
           </td>
           <td class="r">
            University Bremen, Institute for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#103133" title="Click to go to the Author Index">
             Beetz, Michael
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab318" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#cognitive_modeling" title="Click to go to the Keyword Index">
               Cognitive Modeling
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Developing versatile household robots that are capable of adapting to diverse tasks, objects, and environments is a significant challenge. Traditional imitation learning approaches, which optimise parameters within specific task domains, often struggle with generalisation and require extensive training data. These approaches can underperform when encountering states not covered by expert demonstrations. To address this, we propose integrating actionable knowledge graphs with task demonstrations to enhance robots' task learning capabilities. This method enables robots to understand and perform task variations more effectively by linking semantic information to logged memories of task demonstrations.
             <p>
              We demonstrate this approach using a fruit-cutting scenario, where participants perform various cutting actions in a VR environment. The linked actionable knowledge graph classifies these actions into specific tasks like halving, slicing, or dicing, which robots can then execute accurately by using the included semantics to parameterise their action plan. This research aims to advance robotic capabilities by enabling true task learning, making robots more adaptable and efficient in performing a wider range of household tasks autonomously.
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_31">
             14:15-15:00, Paper ThINT2S.31
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod508" name="modify331" onclick="modify(331,508)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Generative AI in Multimodal Systems: Enhancing Scene Understanding in Visually Degraded Environments with Active Range Sensors
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#409735" title="Click to go to the Author Index">
             Kyuroson, Alexander
            </a>
           </td>
           <td class="r">
            Lulea University of Technology, Robotics and AI
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#330779" title="Click to go to the Author Index">
             Mukherjee, Moumita
            </a>
           </td>
           <td class="r">
            Lulea University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#269884" title="Click to go to the Author Index">
             Banerjee, Avijit
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#281347" title="Click to go to the Author Index">
             Koval, Anton
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_32">
             14:15-15:00, Paper ThINT2S.32
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod509" name="modify335" onclick="modify(335,509)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('335'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Robotic Space Simulator: A Full-Motion Simulator for In-Space Robotics
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410798" title="Click to go to the Author Index">
             Hilburn, Eddie
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#237118" title="Click to go to the Author Index">
             Pettinger, Adam
            </a>
           </td>
           <td class="r">
            The University of Texas at Austin
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#412847" title="Click to go to the Author Index">
             Wilkinson, Emily
            </a>
           </td>
           <td class="r">
            Texas A&amp;M University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab335" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#parallel_robots" title="Click to go to the Keyword Index">
               Parallel Robots
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             As launch costs decrease, the number of applications for space-based platforms is growing. Correspondingly, the demand for in-space servicing of existing spacecraft is increasing, with many methodologies expected to feature robotics. However, testing of robotic servicing systems in space -- particularly when this testing requires close proximity to or direct interaction with other orbital objects -- carries great risk of unintentional collisions and space debris. While software simulations are valuable tools, the realism gap is a major obstacle to deploying these robotic systems. Physical simulation platforms for interaction between spacecraft are highly desirable as an intermediate validation.
             <p>
              In this report, we introduce the Robotic Space Simulator (RSS), which uses two 7-DoF Gough-Stewart platforms to fuse accurately simulated centroidal dynamics with real force-sensorized contact interaction and realistic visuals. We discuss the primary characteristics of the RSS, give results of our initial validation of the system, including contact between the simulated spacecraft, and discuss our next steps for the system.
              <p>
               The objective of the RSS is to provide a means to simulate interaction between two spacecraft in a micro-gravity environment, including contact dynamics between the spacecraft via robotic manipulator. Early testing of the system has validated the design and capabilities of the RSS to achieve this objective. The video accompanying this report depicts a grappling interaction between two simulated spacecraft, each with a mass of 2100kg. Prior to contact, a teleoperator inputs forces to the spacecraft center of mass via joystick control. These inputs accumulate as momentum for the spacecraft until it contacts the objective spacecraft. Following contact, we measure the forces on each platform, and show they are equal in magnitude and opposite in direction.
               <p>
                Finally, we discuss future research objectives to further improve the RSS, including high-fidelity modeling and comprehensive 14-DoF control to maximize the simulator workspace.
               </p>
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_33">
             14:15-15:00, Paper ThINT2S.33
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod510" name="modify336" onclick="modify(336,510)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('336'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task-Parameter Nexus for Learning Task-Specific Parameters in Model-Based Control
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#231782" title="Click to go to the Author Index">
             Cheng, Sheng
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#383910" title="Click to go to the Author Index">
             Gu, Yuliang
            </a>
           </td>
           <td class="r">
            UIUC
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#385361" title="Click to go to the Author Index">
             Tao, Ran
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#193952" title="Click to go to the Author Index">
             Wang, Shenlong
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#325400" title="Click to go to the Author Index">
             Wang, Xiaofeng
            </a>
           </td>
           <td class="r">
            University of South Carolina
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#151876" title="Click to go to the Author Index">
             HOVAKIMYAN, NAIRA
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab336" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#machine_learning_for_robot_control" title="Click to go to the Keyword Index">
               Machine Learning for Robot Control
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#motion_control" title="Click to go to the Keyword Index">
               Motion Control
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#aerial_systems__mechanics_and_control" title="Click to go to the Keyword Index">
               Aerial Systems: Mechanics and Control
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This extended abstract presents the Task-Parameter Nexus (TPN), a learning-based approach to the online determination of the (near-)optimal control parameters of model-based controllers (MBCs) for trajectory tracking tasks. In TPN, a deep neural network is introduced to generate the control parameters for any given trajectory at runtime. To train this network, we introduce a systematic approach to build the training dataset so that this dataset is rich enough to cover a wide range of trajectories to be tracked. For each trajectory in the bank, we autotune the optimal control parameters offline and use them as labels. With this dataset, the TPN is trained and evaluated on the quadrotor platform. It is shown in simulation experiments that the TPN can predict near-optimal control parameters for a spectrum of tracking tasks, demonstrating its robust generalization capabilities.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_34">
             14:15-15:00, Paper ThINT2S.34
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod511" name="modify346" onclick="modify(346,511)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('346'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Neuromorphic Navigation: Integrating Event-Based Vision and Physics-Driven Planning on a Parrot Bebop2 Quadrotor
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#391577" title="Click to go to the Author Index">
             Joshi, Amogh
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#337662" title="Click to go to the Author Index">
             Sanyal, Sourav
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#299278" title="Click to go to the Author Index">
             Roy, Kaushik
            </a>
           </td>
           <td class="r">
            Purdue University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab346" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#neurorobotics" title="Click to go to the Keyword Index">
               Neurorobotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             In autonomous aerial navigation, real-time and energy-efficient obstacle avoidance remains a significant challenge, especially in dynamic and complex indoor environments. This work presents a novel integration of neuromorphic event cameras with physics-driven planning algorithms implemented on a Parrot Bebop2 quadrotor. Neuromorphic event cameras, characterized by their high dynamic range and low latency, offer significant advantages over traditional frame-based systems, particularly in poor lighting conditions or during high-speed maneuvers. We use a DVS camera with a shallow Spiking Neural Network (SNN) for event-based object detection of a moving ring in real time in an indoor lab. Further, we enhance drone control with physics-guided empirical knowledge inside a neural network training mechanism, to predict energy-efficient flight paths to fly through the moving ring. This integration results in a real-time, low-latency navigation system capable of dynamically responding to environmental changes while minimizing energy consumption. We detail our hardware setup, control loop, and modifications necessary for real-world applications, including the challenges of sensor integration without burdening the flight capabilities. Experimental results demonstrate the effectiveness of our approach in achieving robust, collision-free, and energy- efficient flight paths, showcasing the potential of neuromorphic vision and physics-driven planning in enhancing autonomous navigation systems.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_35">
             14:15-15:00, Paper ThINT2S.35
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod512" name="modify348" onclick="modify(348,512)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('348'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Cable-Driven Upper Limb Rehabilitation Robot with Muscle-Synergy-Based Myoelectric Controller
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#241728" title="Click to go to the Author Index">
             Xie, Chenglin
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#280018" title="Click to go to the Author Index">
             Lyu, Yueling
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#241810" title="Click to go to the Author Index">
             Li, Guoxin
            </a>
           </td>
           <td class="r">
            Tongji University, Shanghai, China
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#119913" title="Click to go to the Author Index">
             Tong, Kai Yu
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#274572" title="Click to go to the Author Index">
             Xia, Haisheng
            </a>
           </td>
           <td class="r">
            Tongji University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#209085" title="Click to go to the Author Index">
             Song, Rong
            </a>
           </td>
           <td class="r">
            Sun Yat-Sen University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101805" title="Click to go to the Author Index">
             Li, Zhijun
            </a>
           </td>
           <td class="r">
            University of Science and Technology of China
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab348" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#rehabilitation_robotics" title="Click to go to the Keyword Index">
               Rehabilitation Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#neurorobotics" title="Click to go to the Keyword Index">
               Neurorobotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#physical_human_robot_interaction" title="Click to go to the Keyword Index">
               Physical Human-Robot Interaction
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#muscle_synergy_based_myoelectric_controller" title="Click to go to the Keyword Index">
               Muscle-Synergy-Based Myoelectric Controller
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Surface electromyography (sEMG) signal has been used in upper limb rehabilitation robots (ULRR). However, existing ULRR based on myoelectric controllers suffered from limited generalization ability in estimating 3D motion intention. This study proposed a muscle-synergy-inspired approach to enhance the generalization ability of the myoelectric controller of a cable-driven ULRR. Low-dimensional commands were extracted from sEMG signals based on an EMG-to-muscle activation model and non-negative matrix factorization. The extracted commands were used to estimate the 3D human force. Two different trajectory tracking tasks were selected to test the generalization ability. The system was trained based on training sets where participants performed one task. Then the system was tested using testing sets where participants performed the other task. Finally, the system was verified on real-time robotic control experiment. Results showed the proposed controller achieved better force estimating accuracy, better trajectory tracking accuracy and lower interaction force than the myoelectric controller without considering muscle synergies, which meant the proposed controller yielded better generali
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_36">
             14:15-15:00, Paper ThINT2S.36
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod513" name="modify349" onclick="modify(349,513)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('349'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NARF24: Estimating Articulated Object Structure for Implicit Rendering
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#278306" title="Click to go to the Author Index">
             Lewis, Stanley
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#372200" title="Click to go to the Author Index">
             Gao, Tom
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#106068" title="Click to go to the Author Index">
             Jenkins, Odest Chadwicke
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab349" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#visual_learning" title="Click to go to the Keyword Index">
               Visual Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#representation_learning" title="Click to go to the Keyword Index">
               Representation Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Articulated objects and their representations pose a difficult problem for robots. These objects require not only representations of geometry and texture, but also of the various connections and joint parameters that make up each articulation. We propose a method that learns a common Neural Radiance Field (NeRF) representation across a small number of collected scenes. This representation is combined with a parts-based image segmentation to produce an implicit- space part localization, from which the connectivity and joint parameters of the articulated object can be estimated, thus enabling configuration-conditioned rendering.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_37">
             14:15-15:00, Paper ThINT2S.37
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod514" name="modify359" onclick="modify(359,514)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('359'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Cable Traversing Bimanual Robotic Manipulators for Expandable Range Application
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#222424" title="Click to go to the Author Index">
             Cheng, Hung Hon
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#194194" title="Click to go to the Author Index">
             Hughes, Josie
            </a>
           </td>
           <td class="r">
            EPFL
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab359" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#telerobotics_and_teleoperation" title="Click to go to the Keyword Index">
               Telerobotics and Teleoperation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This paper presents a teleportation system with floating robotic arms that traverse parallel cables to perform long-distance manipulation. The system benefits from the cable-based infrastructure, which is easy to set up and cost-effective with expandable workspace range. Unlike mobile and drone-based robots, the proposed system overcomes challenges of uneven terrain and limited operation duration. It provides long-distance manipulation, useful in large field applications such as agriculture and variable shoulder width bi-manual manipulation. The system's parameters are demonstrated with its reachable workspace. The dual-arm setup allows for handling complex tasks through long-distance teleoperation. Experiments, both outdoor and indoor, imitating the harvesting process, demonstrate its usability and potential applications.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_38">
             14:15-15:00, Paper ThINT2S.38
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod515" name="modify360" onclick="modify(360,515)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Deployment of an Aerial Multi-Agent System for Automated Task Execution in Large-Scale Underground Mining Environments
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#325318" title="Click to go to the Author Index">
             Dahlquist, Niklas
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#325063" title="Click to go to the Author Index">
             Stathoulopoulos, Nikolaos
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#299617" title="Click to go to the Author Index">
             Nordström, Samuel
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#270770" title="Click to go to the Author Index">
             Lindqvist, Björn
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105882" title="Click to go to the Author Index">
             Nikolakopoulos, George
            </a>
           </td>
           <td class="r">
            Luleå University of Technology
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_39">
             14:15-15:00, Paper ThINT2S.39
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod516" name="modify361" onclick="modify(361,516)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('361'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Adaptive Motion Generation Using Uncertainty-Driven Foresight Prediction
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#321189" title="Click to go to the Author Index">
             Hiruma, Hyogo
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#243493" title="Click to go to the Author Index">
             Ito, Hiroshi
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab361" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#embodied_cognitive_science" title="Click to go to the Keyword Index">
               Embodied Cognitive Science
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#cognitive_control_architectures" title="Click to go to the Keyword Index">
               Cognitive Control Architectures
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Uncertainty of environments has long been a difficult characteristic to handle, when performing real-world robot tasks. This is because the uncertainty produces unexpected observations that cannot be covered by manual scripting. Learning based robot controlling methods are a promising approach for generating flexible motions against unknown situations, but still tend to suffer under uncertainty due to its deterministic nature. In order to adaptively perform the target task under such conditions, the robot control model must be able to accurately understand the possible uncertainty, and to exploratively derive the optimal action that minimizes such uncertainty. This paper extended a existing predictive learning based robot control method, which employ foresight prediction using dynamic internal simulation. The foresight module samples multiple future prediction using the dynamics that a Recurrent Neural Network (RNN) has internally structured. Each foresight starts with a hidden state with random noise applied, so that multiple possible futures can be explored. The foresights are scored by the predicted expected variance, or uncertainty, where the one with the lowest value is selected for actual robot controlling. The adaptiveness of the model was evaluated on a door opening task. The door can be opened either by pushing, pulling, or sliding, but robot cannot visually distinguish which way, and is required to adapt on the fly. The results showed that the proposed model adaptively diverged its motion through interaction with the door, whereas conventional methods failed to stably diverge. The models were analyzed on Lyapunov exponents of RNN hidden states which reflect the possible divergence at each time step during task execution. The result indicated that the foresight module biased the model to consider future consequences, which lead to embedding uncertainties at the policy of the robot controller, rather than the resultant observation. This is beneficial for implementing adaptive behaviors, which indices derivation of diverse motion during exploration.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_40">
             14:15-15:00, Paper ThINT2S.40
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod517" name="modify369" onclick="modify(369,517)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('369'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Wheel Slip Decomposition and State Estimation for Exploration Rovers
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#191078" title="Click to go to the Author Index">
             Lou, Qingfeng
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#115939" title="Click to go to the Author Index">
             Kovecses, Jozsef
            </a>
           </td>
           <td class="r">
            McGill University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab369" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#space_robotics_and_automation" title="Click to go to the Keyword Index">
               Space Robotics and Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#sensor_fusion" title="Click to go to the Keyword Index">
               Sensor Fusion
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Building on the foundational concepts of odometry and terramechanics, this paper aims to enhance rover state estimation by exclusively leveraging wheel encoder systems. It addresses identified gaps in current methodologies with two main objectives. The first is to introduce a novel wheel slip decomposition technique that considers the interactions among different wheels through rover kinematics. The second objective is to develop three distinct estimation frameworks that operate at both the kinematics and dynamics levels to effectively determine wheel slip. This approach was experimentally validated using a six-wheeled rover prototype.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_41">
             14:15-15:00, Paper ThINT2S.41
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod518" name="modify391" onclick="modify(391,518)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('391'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Project Food Angel: Development and Preliminary Observations of a Food Delivery Robot for the Homeless
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410983" title="Click to go to the Author Index">
             Hong, Ethan
            </a>
           </td>
           <td class="r">
            Geffen Academy at UCLA
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab391" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#design_and_human_factors" title="Click to go to the Keyword Index">
               Design and Human Factors
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Food Angel is a food delivery robot to help with the problems of “food insecurity” and “homelessness.” Food insecurity [1, 2] refers to the condition of not having access to sufficient food, or food of an adequate quality, to meet one's basic needs. Together with homelessness, these are prominent issues globally, especially in the urban areas around the world [3]. Currently, food banks and other charity organizations provide food for the needed, however due to the lack of manpower and stigma surrounding the homeless including fear and concerns for safety and hygiene, food insecurity for the homeless is still a significant problem that needs to be addressed [4]. Utilizing autonomous wheeled robots for this application may seem to be a good approach [5, 6], especially with a number of successful commercial robotic delivery services [7, 8]. However, besides technical considerations such as range, payload, operation time, autonomy, etc. [9] there are a number of important aspects that still need to be investigated, such as how the general public and the receiving end may feel about using robots for such application, or human-robot interaction issues such as how to communicate the intent of the robot to the homeless. Feedback from early interviews indicate that technology (such as monitor screens with messages, speakers with sound cues or voice announcements, or an automatically opening top) is not preferred by the homeless. Thus the first prototype of Food Angel uses a simple cardboard box with a clear handwritten sign for communication. Through this work, we would like to test what is possible using robotics technology, to establish a design guideline for future robots for this application, and to bring awareness about the problem of food insecurity. This video presents our first prototype of Food Angel and the preliminary results and observations from its first deployment.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_42">
             14:15-15:00, Paper ThINT2S.42
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod519" name="modify400" onclick="modify(400,519)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('400'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              CAMEL: Learning Cost Maps Made Easy
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#294813" title="Click to go to the Author Index">
             Viswanath, Kasi
            </a>
           </td>
           <td class="r">
            Texas A&amp;m University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101954" title="Click to go to the Author Index">
             PB, Sujit
            </a>
           </td>
           <td class="r">
            IISER Bhopal
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#102948" title="Click to go to the Author Index">
             Saripalli, Srikanth
            </a>
           </td>
           <td class="r">
            Texas A&amp;M
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab400" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#field_robots" title="Click to go to the Keyword Index">
               Field Robots
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#vision_based_navigation" title="Click to go to the Keyword Index">
               Vision-Based Navigation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Cost maps are essential for robotic vehicles to plan collision-free paths. Each cell in the map carries a cost that represents sensed environmental information, which is often determined manually through trial and error. In off-road environments, the diverse range of features makes it difficult to manually assign accurate cost values to each feature. Furthermore, different manually assigned cost values can result in varying paths for the same environment, which is undesirable. This paper addresses the challenge of learning cost map values from the sensed environment to enhance robust vehicle path planning. We propose a novel framework called CAMEL, which uses a deep learning approach to learn parameters from demonstrations, resulting in an adaptive and robust cost map for path planning. A unique discrete Fourier-based planner is employed to learn the cost map. CAMEL is evaluated through simulations on a ground rover, demonstrating flexible and robust vehicle motion without collisions in unstructured off-road terrains.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_43">
             14:15-15:00, Paper ThINT2S.43
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod520" name="modify406" onclick="modify(406,520)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('406'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Goal-Oriented Humanoid Bimanual Dough Rolling Using Dynamic Heterogeneous Graph Based on Human Demonstration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#273261" title="Click to go to the Author Index">
             Liu, Junjia
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#344104" title="Click to go to the Author Index">
             Li, Chenzui
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#272510" title="Click to go to the Author Index">
             Wang, Shixiong
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#231411" title="Click to go to the Author Index">
             Dong, Zhipeng
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#399284" title="Click to go to the Author Index">
             Li, Ting Yan Andrew
            </a>
           </td>
           <td class="r">
            The Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107089" title="Click to go to the Author Index">
             Calinon, Sylvain
            </a>
           </td>
           <td class="r">
            Idiap Research Institute
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#132532" title="Click to go to the Author Index">
             Chen, Fei
            </a>
           </td>
           <td class="r">
            T-Stone Robotics Institute, the Chinese University of Hong Kong
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab406" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#bimanual_manipulation" title="Click to go to the Keyword Index">
               Bimanual Manipulation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_in_grasping_and_manipulation" title="Click to go to the Keyword Index">
               Deep Learning in Grasping and Manipulation
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Soft object manipulation poses significant challenges for robots, requiring effective techniques for state representation and manipulation policy learning. State representation involves capturing the dynamic changes in the environment, while manipulation policy learning focuses on establishing the relationship between robot actions and state transformations to achieve specific goals. To address these challenges, this research paper introduces a novel approach: a dynamic heterogeneous graph-based model for learning goal-oriented soft object manipulation policies. The proposed model utilizes graphs as a unified representation for both states and policy learning. By leveraging the dynamic graph, crucial information regarding object dynamics and manipulation policies can be extracted. Furthermore, the model facilitates the integration of demonstrations, enabling guided policy learning. To evaluate the efficacy of our approach, we designed a dough rolling task and conducted experiments using both a differentiable simulator and a real-world humanoid robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_44">
             14:15-15:00, Paper ThINT2S.44
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod521" name="modify419" onclick="modify(419,521)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('419'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Ergonomic Motion Planning for Human-Robot Collaboration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#331230" title="Click to go to the Author Index">
             Ngui, Isaac
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#237863" title="Click to go to the Author Index">
             Motes, James
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#266010" title="Click to go to the Author Index">
             Lee, Hannah
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#105498" title="Click to go to the Author Index">
             Morales, Marco
            </a>
           </td>
           <td class="r">
            University of Illinois at Urbana-Champaign &amp; Instituto Tecnológ
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#102662" title="Click to go to the Author Index">
             Amato, Nancy
            </a>
           </td>
           <td class="r">
            University of Illinois Urbana-Champaign
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab419" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This work proposes an ergonomic planning framework for human-robot collaborative tasks. The planner employs a hierarchical approach to combat large search spaces while performing local ergonomic optimizations at each stage. The planner will produce robot trajectories that optimize for ergonomics and are usable in real-world tasks.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_45">
             14:15-15:00, Paper ThINT2S.45
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod522" name="modify423" onclick="modify(423,522)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('423'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              SE3ET: SE(3)-Equivariant Transformer for Low-Overlap Point Cloud Registration
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#270466" title="Click to go to the Author Index">
             Lin, Chien Erh
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#250121" title="Click to go to the Author Index">
             Zhu, Minghan
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#131371" title="Click to go to the Author Index">
             Ghaffari, Maani
            </a>
           </td>
           <td class="r">
            University of Michigan
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab423" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_for_visual_perception" title="Click to go to the Keyword Index">
               Deep Learning for Visual Perception
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#localization" title="Click to go to the Keyword Index">
               Localization
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#deep_learning_methods" title="Click to go to the Keyword Index">
               Deep Learning Methods
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Partial point cloud registration is a challenging problem in robotics, especially when the robot undergoes a large transformation, causing a significant initial pose error and a low overlap between measurements. This work proposes exploiting equivariant learning from 3D point clouds to improve registration robustness. We propose SE3ET, an SE(3)-equivariant registration framework that employs equivariant point convolution and equivariant transformer designs to learn expressive and robust geometric features. We tested the proposed registration method on indoor and outdoor benchmarks where the point clouds are under arbitrary transformations and low overlapping ratios. We also provide generalization tests and run-time performance.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_46">
             14:15-15:00, Paper ThINT2S.46
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod523" name="modify427" onclick="modify(427,523)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('427'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              NICOL Learns to Imitate Using Diffusion
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#396630" title="Click to go to the Author Index">
             Spisak, Josua
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#200086" title="Click to go to the Author Index">
             Kerzel, Matthias
            </a>
           </td>
           <td class="r">
            Uni Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#110222" title="Click to go to the Author Index">
             Wermter, Stefan
            </a>
           </td>
           <td class="r">
            University of Hamburg
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#353085" title="Click to go to the Author Index">
             Wenhao, Lu
            </a>
           </td>
           <td class="r">
            Hamburg University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab427" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Our video shows how our robot NICOL cite{1} learns to imitate human poses through the use of a diffusion model. While NICOL is already capable of many things cite{3} cite{4}, the ability to directly copy humans holds a lot of potential.	The robot is positioned in front of a table, opposing a human. The video starts by showing how the training data is collected by a human imitating the robot's arm movements, allowing our diffusion model to learn the relation between robotic and human movements as expected by a human. Afterwards, the video transitions to a role switch where the robot imitates the human, by directly mapping the seen rgb images to joint values. Finally, we show our approach's generalisability by imitating two demonstrations from different human demonstrators. The model had not seen either human during the training, so the human is new to it during the inference process. Despite the different physiology of the humans, the model is able to imitate them. The video is focused on the practical application rather than the technical details of our model and the practical use of diffusion models for robotics cite{2}.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_47">
             14:15-15:00, Paper ThINT2S.47
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod524" name="modify436" onclick="modify(436,524)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('436'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Task-Critical Motion Patterns in Task Planning and Motion Control for Household Manipulation Tasks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#372601" title="Click to go to the Author Index">
             Hassouna, Vanessa
            </a>
           </td>
           <td class="r">
            University Bremen, Institute for Artificial Intelligence
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#346522" title="Click to go to the Author Index">
             Huerkamp, Malte
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#103133" title="Click to go to the Author Index">
             Beetz, Michael
            </a>
           </td>
           <td class="r">
            University of Bremen
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab436" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#cognitive_control_architectures" title="Click to go to the Keyword Index">
               Cognitive Control Architectures
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Autonomous household robots face challenges beyond simple pick-and-place tasks, requiring solutions for diverse actions such as pouring, mixing, and cutting. This paper introduces the concept of Task-Critical Motion Patterns (TCMPs) to improve the adaptability and robustness of generalized task plans for these actions. A TCMP describes the essential motion pattern of a task through all variations. Integrated within a cognitive robot control architecture, TCMPs leverage constraint-based motion control for flexible execution of known tasks in new environments. An example of a mixing task illustrates the practical application and benefits of TCMPs.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_48">
             14:15-15:00, Paper ThINT2S.48
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod525" name="modify437" onclick="modify(437,525)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('437'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Explaining and Preventing Robot Failures through Causal Models
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#280410" title="Click to go to the Author Index">
             Diehl, Maximilian
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#151485" title="Click to go to the Author Index">
             Ramirez-Amaro, Karinne
            </a>
           </td>
           <td class="r">
            Chalmers University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab437" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#acceptability_and_trust" title="Click to go to the Keyword Index">
               Acceptability and Trust
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#human_centered_robotics" title="Click to go to the Keyword Index">
               Human-Centered Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Future robots are expected to assist humans with daily tasks such as setting the table and washing dishes. However, operating in human environments, these robots are prone to errors. Humans often find it challenging to understand why these failures occur, especially when robots rely on black-box decision-making methods. This lack of transparency reduces trust and effectiveness in human-robot interactions and limits humans' ability to assist robots in recovering from failures. To address this challenge, we have developed methods that use causal models of robot actions to explain why these failures happen. In this extended abstract, we outline our approach to explaining, predicting, and preventing robot failures. Our method initially learns the cause-effect relationships between task executions and their outcomes as a causal Bayesian network (CBN). Based on this CBN, our method provides contrastive explanations for task failures and predicts and prevents future failures. To avoid the need to relearn the causal model for new tasks, we also propose methods to transfer CBN parameter priors from related tasks based on the semantic similarity of the two CBNs. Finally, we present our current work on extending the methods to robot navigation scenarios, where we use a causal model to predict when humans are likely dissatisfied with the robot trajectory (e.g., low perceived robot competence) and to provide alternative robot trajectories for improved performance ratings.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_50">
             14:15-15:00, Paper ThINT2S.50
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod526" name="modify447" onclick="modify(447,526)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('447'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              BerryTwist: A New Gripper for Blackberry Harvesting
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#380270" title="Click to go to the Author Index">
             Elfferich, Johannes Frederik
            </a>
           </td>
           <td class="r">
            TU Delft - 3mE
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#292827" title="Click to go to the Author Index">
             Shahabishalghouni, E.
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#179240" title="Click to go to the Author Index">
             Della Santina, Cosimo
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#107921" title="Click to go to the Author Index">
             Dodou, Dimitra
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab447" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#agricultural_automation" title="Click to go to the Keyword Index">
               Agricultural Automation
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#grippers_and_other_end_effectors" title="Click to go to the Keyword Index">
               Grippers and Other End-Effectors
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#soft_robot_materials_and_design" title="Click to go to the Keyword Index">
               Soft Robot Materials and Design
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             The agricultural sector faces labor shortages, especially in fruit harvesting. Addressing this, we introduce BerryTwist, a novel soft robotic gripper designed for harvesting blackberries. The gripper uses a fabric tube mechanism powered by motorized twisting to handle fruits securely and with minimal damage. The results have shown an 82% success rate in detaching and a 95% success rate in releasing blackberries, highlighting its potential for highly efficient agricultural automation.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_51">
             14:15-15:00, Paper ThINT2S.51
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod527" name="modify462" onclick="modify(462,527)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('462'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Multi-Reference Frame Skills from Demonstration with Task-Parameterized Gaussian Processes
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#340254" title="Click to go to the Author Index">
             Ramirez Montero, Mariano
            </a>
           </td>
           <td class="r">
            Delft University of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#269151" title="Click to go to the Author Index">
             Franzese, Giovanni
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#117613" title="Click to go to the Author Index">
             Kober, Jens
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#179240" title="Click to go to the Author Index">
             Della Santina, Cosimo
            </a>
           </td>
           <td class="r">
            TU Delft
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab462" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_demonstration" title="Click to go to the Keyword Index">
               Learning from Demonstration
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#probabilistic_inference" title="Click to go to the Keyword Index">
               Probabilistic Inference
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             A challenge in Learning from Demonstration is to generate representations that can generalize to unseen situations. This work proposes to learn such a representation for multi-reference frame skills without using task-specific heuristics. Local policies are first learned by fitting the relative skills with respect to each frame using Gaussian Processes (GPs). Then, another GP, which determines the relevance of each frame for every time step, is trained in a self-supervised manner. The uncertainty quantification capability of GPs is exploited to stabilize the local policies and to train the frame relevance in a fully Bayesian way. We validate the method through a dataset of multi-frame tasks generated in simulation and on real-world experiments with a robotic manipulation pick-and-place re-shelving task. We evaluate the performance of our method with two metrics: how close the trajectories get to each of the task goals and the Frechet distance to expert trajectories. In both metrics, the proposed method outperforms the state-of-the-art baseline, Task-Parameterised Gaussian Mixture Model (TPGMM).
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_52">
             14:15-15:00, Paper ThINT2S.52
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod528" name="modify465" onclick="modify(465,528)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             Vision-Based Detection and Localization Algorithm for Apple Harvesting Robot
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410929" title="Click to go to the Author Index">
             Etchart, Antoine
            </a>
           </td>
           <td class="r">
            École Nationale d'Ingénieurs de Tarbes
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410963" title="Click to go to the Author Index">
             ten Hagen, Gerwin Michaël
            </a>
           </td>
           <td class="r">
            Saxion
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410926" title="Click to go to the Author Index">
             Jansen, Luuk
            </a>
           </td>
           <td class="r">
            Saxion
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410982" title="Click to go to the Author Index">
             Wolf, Kaj
            </a>
           </td>
           <td class="r">
            Saxion
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410930" title="Click to go to the Author Index">
             Elders, Kas
            </a>
           </td>
           <td class="r">
            Saxion
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410861" title="Click to go to the Author Index">
             Mirtajadini, Seyed Hojat
            </a>
           </td>
           <td class="r">
            Saxion University of Applied Sciences
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#397789" title="Click to go to the Author Index">
             Najafi, Esmaeil
            </a>
           </td>
           <td class="r">
            Saxion University of Applied Sciences
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_53">
             14:15-15:00, Paper ThINT2S.53
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod529" name="modify504" onclick="modify(504,529)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('504'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              ”Can(’t) Touch Me!”: Towards Investigating Socially Assistive Robot Coaching with Physical HRI
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411488" title="Click to go to the Author Index">
             Moiş, Bogdan Ionut
            </a>
           </td>
           <td class="r">
            Faculty of Electrical Engineering, Mathematics and Computer Scie
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#172614" title="Click to go to the Author Index">
             Schneider, Sebastian
            </a>
           </td>
           <td class="r">
            University of Twente
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab504" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#haptics_and_haptic_interfaces" title="Click to go to the Keyword Index">
               Haptics and Haptic Interfaces
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#touch_in_hri" title="Click to go to the Keyword Index">
               Touch in HRI
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Physical activity (PA) is vital for a healthy lifestyle, offering benefits such as reducing cardiovascular risks, enhancing cognitive functions, and alleviating depression and anxiety symptoms. Effective PA often relies on proper execution to maximize benefits and prevent injuries. Augmented feedback, delivered through visual, auditory, or haptic modalities, significantly enhances motor task learning. A multimodal feedback system can leverage these benefits, although more comparative studies are needed, especially on haptic feedback, as existing research often focuses on simple motor tasks.
             <p>
              Tutorials and coaches commonly aid in learning physical exercises, with personal coaches providing more effective feedback than general tutorials. Technological advancements have introduced socially assistive robots (SARs) as potential individual coaches, capable of offering tailored feedback when finding a compatible human coach is challenging due to individual differences. While studies have explored audio and visual feedback from SARs for PA coaching, the effectiveness of haptic feedback remains under-researched.
              <p>
               To address this gap, we propose research equipping SARs with haptic feedback capabilities for squatting exercises, which are complex enough to benefit from precise guidance. Our studies will investigate two methods: users wearing haptic devices that deliver feedback and robots directly guiding users through physical Human-Robot Interaction (pHRI). The feasibility and motivational effects of these methods need exploration, as current literature does not sufficiently address this trade-off. Our research aims to determine the effectiveness of haptic feedback in enhancing exercise performance through SARs
              </p>
             </p>
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_54">
             14:15-15:00, Paper ThINT2S.54
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod530" name="modify477" onclick="modify(477,530)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('477'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Learning Active Search with Graph Neural Networks
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#269666" title="Click to go to the Author Index">
             Hu, Yafei
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#341266" title="Click to go to the Author Index">
             Gupta, Tejus
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#104304" title="Click to go to the Author Index">
             Scherer, Sebastian
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#147934" title="Click to go to the Author Index">
             Schneider, Jeff
            </a>
           </td>
           <td class="r">
            Carnegie Mellon University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab477" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#autonomous_agents" title="Click to go to the Keyword Index">
               Autonomous Agents
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Active search is an important problem in robotics in which robots actively search in a partially observable environment to find task-relevant information, such as Object Of Interest (OOI). Previous Thompson-sampling-based methods pose certain limits such as computational efficiency, which hinders the real-world robotic deployments. In this paper, we present Learning Active Search (LAS) with Graph Neural Network (GNN), an imitation learning-based alternative to the Thompson Sampling-based methods. We represent the robots' belief space of the environment as a graph and feed the graph into a GNN to output the reward function, as well the searching action, which is the optimal waypoint in the belief space.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_55">
             14:15-15:00, Paper ThINT2S.55
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod531" name="modify478" onclick="modify(478,531)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('478'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Safe Human-Robot Interactions with Continuous and Differentiable Distance Fields
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#395607" title="Click to go to the Author Index">
             Ali, Usama
            </a>
           </td>
           <td class="r">
            Technische Hochschule Würzburg Schweinfurt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#395608" title="Click to go to the Author Index">
             Mueller, Adrian
            </a>
           </td>
           <td class="r">
            Technical University Wuerzburg Schweinfurt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#238113" title="Click to go to the Author Index">
             Sukkar, Fouad
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#260181" title="Click to go to the Author Index">
             Wu, Lan
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#103699" title="Click to go to the Author Index">
             Kaupp, Tobias
            </a>
           </td>
           <td class="r">
            Technical University of Applied Sciences Würzburg-Schweinfurt
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#103428" title="Click to go to the Author Index">
             Vidal-Calleja, Teresa A.
            </a>
           </td>
           <td class="r">
            University of Technology Sydney
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab478" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#safety_in_hri" title="Click to go to the Keyword Index">
               Safety in HRI
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#human_aware_motion_planning" title="Click to go to the Keyword Index">
               Human-Aware Motion Planning
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#reactive_and_sensor_based_planning" title="Click to go to the Keyword Index">
               Reactive and Sensor-Based Planning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Human-robot collaboration applications require safe and reactive planning. Euclidean distance fields (EDF) are a promising representation of such dynamic scenes due to their ability to reason about free space and the readily available distance to collision costs. A key challenge for the commonly used discrete EDF representations, however, is the need for differentiable distance fields to produce smooth collision costs and efficient updates of dynamic objects. In this paper, we propose to use a Gaussian Process (GP) distance field-based framework that enables both, differentiable distance fields and fast dynamic scene updates. Moreover, we combine this framework with the Riemannian Motion Policies as a local reactive planner to enable safe human-robot interactions. We design a collision avoidance policy that models the repulsive motion using the distance and gradient fields from our GP. We show our reactive planner in an experiment with a UR5e interacting safely and smoothly with a human.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_56">
             14:15-15:00, Paper ThINT2S.56
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod532" name="modify506" onclick="modify(506,532)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('506'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              A Framework for Inferring Belief States in Partially-Observable Human-Robot Teams
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#303196" title="Click to go to the Author Index">
             Kolb, Jack
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#197904" title="Click to go to the Author Index">
             Feigh, Karen
            </a>
           </td>
           <td class="r">
            Georgia Institute of Technology
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab506" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#human_robot_teaming" title="Click to go to the Keyword Index">
               Human-Robot Teaming
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#human_robot_collaboration" title="Click to go to the Keyword Index">
               Human-Robot Collaboration
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             We propose a framework for robots to estimate the belief state of a human teammate in real-time in 3D partially-observable domains. Recent research has shown potential for robots to use a predicted belief state -- or team mental model -- to inform downstream planning and coordination tasks in human-robot teams. However, no works have applied mental models to realistic domains where the robot operates in 3D space and does not have perfect observability over the environment. Our framework leverages recent advancements in scene graph construction towards addressing the open problem of estimating a human user's situation awareness. We aim to evaluate the framework on a 3D simulation platform and a real-world robot.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_57">
             14:15-15:00, Paper ThINT2S.57
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod533" name="modify482" onclick="modify(482,533)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('482'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Automating Live Music: The Development of Waseda's Wind Instrument Playing Robot
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#202023" title="Click to go to the Author Index">
             Lin, Jia-Yeu
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#102287" title="Click to go to the Author Index">
             Takanishi, Atsuo
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab482" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#art_and_entertainment_robotics" title="Click to go to the Keyword Index">
               Art and Entertainment Robotics
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#social_hri" title="Click to go to the Keyword Index">
               Social HRI
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#humanoid_robot_systems" title="Click to go to the Keyword Index">
               Humanoid Robot Systems
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             This video explores Waseda University's innovative journey in developing wind instrument-playing robots, from automated performance to interactive musical engagement. The history of robot musicians begins with WABOT-2, the world's first personal robot capable of playing a keyboard, built by Waseda University in the early 1980s. Since 1990, our team has focused on creating robots that can play wind instruments, driven by a desire to understand and replicate the complexities of wind instruments performance. The video showcases the evolution of the Waseda flutist and saxophonist robots, highlighting their transformation from basic mechanical designs to sophisticated systems capable of interacting with human musicians. Initially, the research aimed to perfect automated performance. Over time, it shifted towards enabling these robots to engage musically with human players and conductors. This transition reflects broader goals in robotics and AI to develop machines that can meaningfully and responsively integrate into human activities. Through demonstrations of technical advancements and collaborative performances, the video illustrates how Waseda University is pushing the boundaries of robotics, blending technology and artistry to create interactive robotic musicians.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_58">
             14:15-15:00, Paper ThINT2S.58
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod534" name="modify499" onclick="modify(499,534)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('499'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Real-Time Prediction of Vision, Force, and Motion: Connector Insertion Motion Generation by 4-Channel Bilateral Imitation Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#243493" title="Click to go to the Author Index">
             Ito, Hiroshi
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#398636" title="Click to go to the Author Index">
             Kanai, Yoshiki
            </a>
           </td>
           <td class="r">
            Hitachi
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#410961" title="Click to go to the Author Index">
             Kamigaki, Masahiro
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#285628" title="Click to go to the Author Index">
             Ichiwara, Hideyuki
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd. / Waseda University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab499" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Imitation learning has gained attention as a key technology for enabling robots to autonomously perform tasks in various human living environments. Motion teaching through teleoperation, such as the Leader-follower system, can intuitively teach complex motions. When combined with imitation learning, complex tasks that were previously difficult to achieve can be successfully performed. However, many imitation learning systems are limited to generating hand posture or joint angle commands and struggle with contact-rich or occlusion tasks. Introducing 4-channel bilateral control into imitation learning allows robots to adaptively perform tasks requiring force adjustments, such as cleaning or cooking. Despite these advancements, previous studies faced issues with low position generalization performance due to reliance on color center of gravity or CNN image features for object position recognition. This paper proposes a motion generation model for high-precision simultaneous control of position and force. The model combines position and force information from 4-channel bilateral control, a spatial attention mechanism for explicit object position extraction, and a hierarchical RNN inspired by computational neuroscience. The model's effectiveness was verified using a USB flash drive insertion task requiring visual and force information. Experimental results demonstrated an average success rate of 87.8% for USB connector misalignment (±5mm) after training with only 50 demonstrations.
            </div>
           </td>
          </tr>
          <tr style="line-height: 0.2em">
           <td colspan="2">
           </td>
          </tr>
          <tr class="pHdr">
           <td valign="bottom">
            <a name="thint2s_59">
             14:15-15:00, Paper ThINT2S.59
            </a>
           </td>
           <td class="r">
            Add to My Program
            <input id="mod535" name="modify501" onclick="modify(501,535)" type="checkbox" value="on"/>
           </td>
          </tr>
          <tr>
           <td colspan="2">
            <span class="pTtl">
             <a href="" onclick="viewAbstract('501'); return false" title="Click to show or hide the keywords and abstract (text summary)">
              Achieving Faster and More Accurate Operation of Deep Predictive Learning
             </a>
            </span>
           </td>
          </tr>
          <tr>
           <td colspan="2" style="height: 2px">
            <hr class="thin"/>
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#411259" title="Click to go to the Author Index">
             Yoshikawa, Masaki
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#243493" title="Click to go to the Author Index">
             Ito, Hiroshi
            </a>
           </td>
           <td class="r">
            Hitachi, Ltd
           </td>
          </tr>
          <tr>
           <td>
            <a href="ICRAX24_AuthorIndexWeb.html#101828" title="Click to go to the Author Index">
             Ogata, Tetsuya
            </a>
           </td>
           <td class="r">
            Waseda University
           </td>
          </tr>
          <tr>
           <td colspan="2" style="padding: 0px">
            <div id="Ab501" style="padding-top: 10px; padding-left: 4px; padding-right: 4px; display: none">
             <span style="line-height: 2em">
              <strong>
               Keywords:
              </strong>
              <a href="ICRAX24_KeywordIndexWeb.html#perception_action_coupling" title="Click to go to the Keyword Index">
               Perception-Action Coupling
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#learning_from_experience" title="Click to go to the Keyword Index">
               Learning from Experience
              </a>
              ,
              <a href="ICRAX24_KeywordIndexWeb.html#imitation_learning" title="Click to go to the Keyword Index">
               Imitation Learning
              </a>
             </span>
             <br/>
             <strong>
              Abstract:
             </strong>
             Achieving both high speed and precision in robot operations is a significant challenge for social implementation. While factory robots excel at predefined tasks, they struggle with environment-specific actions like cleaning and cooking. Deep learning research aims to address this by enabling robots to autonomously execute behaviors through end-to-end learning with sensor data. RT-1 and ACT are notable examples that have expanded robots' capabilities. However, issues with model inference speed and hand position accuracy persist. High-quality training data and fast, stable inference mechanisms are essential to overcome these challenges. This paper proposes a motion generation model for high-speed, high-precision tasks, exemplified by the sport stacking task. By teaching motions slowly and inferring at high speeds, the model achieved a 94% success rate in stacking cups with a real robot.
            </div>
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thk3n">
             <b>
              ThK3N
             </b>
             RTM Stage
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod536" name="modifyThK3N" onclick="modsession(81,536)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thk3n" title="Click to go to the Program at a Glance">
             <b>
              Keynote Session 12
              <br/>
              <br/>
              Barbara Mazzolai – Marcia O'Malley – Jessica
              <br/>
              Burgner-Kahrs – Josie Hughes – Kenji Suzuki – Heike Vallery
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thpm_br">
             <b>
              ThPM_BR
             </b>
             Rotterdam + Port
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod537" name="modifyThPM_BR" onclick="modsession(83,537)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thpm_br" title="Click to go to the Program at a Glance">
             <b>
              Coffee Break 8
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thind2p">
             <b>
              ThIND2P
             </b>
             RTM Stage
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod538" name="modifyThIND2P" onclick="modsession(85,538)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thind2p" title="Click to go to the Program at a Glance">
             <b>
              Industry Pitches 4
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thdnp">
             <b>
              ThDNP
             </b>
             RTM Stage
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod539" name="modifyThDNP" onclick="modsession(87,539)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thdnp" title="Click to go to the Program at a Glance">
             <b>
              Debate and Panel 4
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
         <table class="trk">
          <tr>
           <td colspan="2">
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a name="thevev">
             <b>
              ThEVEV
             </b>
             Rotterdam + Port
            </a>
           </td>
           <td class="r" nowrap="">
            Add to My Program
            <input id="mod540" name="modifyThEVEV" onclick="modsession(93,540)" type="checkbox" value="ON"/>
           </td>
          </tr>
          <tr class="sHdr">
           <td nowrap="">
            <a href="ICRAX24_ProgramAtAGlanceWeb.html#thevev" title="Click to go to the Program at a Glance">
             <b>
              Farewell Reception
             </b>
            </a>
           </td>
           <td>
           </td>
          </tr>
          <tr class="sHdr">
           <td colspan="2" style="line-height: 0pt">
           </td>
          </tr>
         </table>
        </div>
        <p>
         <br/>
        </p>
        <p>
         <br/>
        </p>
       </td>
       <td height="100%" style="background-color:#5B2999;" width="5">
       </td>
      </tr>
      <tr>
       <td alt="" border="0" colspan="4" height="8" style="background-color:#5B2999;" valign="center" width="100%">
        <p align="center">
         <span style="font-size:8pt;line-height:10pt;color:#fff;">
          Technical Content ©
IEEE Robotics &amp; Automation Society
         </span>
        </p>
       </td>
      </tr>
      <tr>
       <td colspan="4" width="100%">
        <p align="right">
         <span style="text-decoration:none;">
          <img align="right" border="0" src="/images/pc_logo_small.png" style="margin-left: 10px; margin-right: 10px"/>
          This site is protected
by copyright and trademark laws under US and International law.
          <br/>
          All rights
reserved. © 2002-2024 PaperCept, Inc.
          <br/>
          Page generated 2024-09-30  01:22:01 PST
          <a href="" onclick="window.open('/conferences/scripts/about.pl','tc','width=1000,scrollbars=yes'); return false">
           Terms
of use
          </a>
         </span>
        </p>
       </td>
      </tr>
     </table>
    </body>
   </div>
  </form>
 </body>
</html>
